{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submodule 5 - Deep Learning for Biomedical Applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This submodule will help readers grasping the deep learning core concepts and architectures, exploring diverse applications, and gaining hands-on experience building and evaluating deep learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "At the end of this module, you should be able to:\n",
    "\n",
    "- Gain a fundamental understanding of deep learning concepts and architectures (e.g.,\n",
    "  simple neural networks, convolutional neural networks, recurrent neural networks etc.).\n",
    "- Explore diverse applications of deep learning in various domains of biomedical research. (e.g., drug discovery, protein structure\n",
    "  prediction, medical image analysis).\n",
    "- Develop practical skills in building and training deep learning models using a popular deep learning library. (e.g., TensorFlow, PyTorch).\n",
    "- Apply deep learning techniques to solve a specific biomedical research problem through a hands-on project and evaluate the effectiveness and limitations of deep learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- An AWS account with access to Amazon SageMaker\n",
    "- Basic understanding of Python programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "- Watch the Lecture Videos.\n",
    "- Complete the Quizzes to solidify your understanding.\n",
    "- Enhance your programming skills with Tutorials.\n",
    "- Challenge yourself with the Exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Deep Learning\n",
    "\n",
    "The lecture introduces deep learning (DL), a subfield of machine learning, emphasizing its ability to automatically learn representations of data through layered neural networks. Unlike traditional machine learning that relies heavily on hand-engineered features, DL layers concepts to enable hierarchical understanding, making it adept at handling complex data and performing tasks like feature extraction and classification. DL's effectiveness improves with larger datasets and advanced software frameworks, such as TensorFlow, which facilitate building complex, multi-layered models.\n",
    "\n",
    "Artificial neural networks (ANNs) are core to DL, mimicking the structure of the brain with interconnected layers of neurons, each layer processing information in sequence. This structure includes a forward flow, where data is fed from input to output, and a backward flow (or backpropagation), which adjusts weights to minimize prediction errors. Types of neural networks covered include multi-layer perceptrons (MLPs), which are fully connected networks used for general applications, and more specialized architectures like convolutional neural networks (CNNs) for image processing, and recurrent neural networks (RNNs) for sequential data. Long Short-Term Memory (LSTM) networks, an extension of RNNs, address issues like the vanishing gradient problem, enabling better handling of long sequences.\n",
    "\n",
    "The lecture also explores encoder-decoder architectures, such as transformers, which leverage self-attention to capture dependencies in sequence data. Important network components like linear units, layers, activation functions, and the loss function are discussed, each playing a role in the transformation and evaluation of data as it passes through the network. Optimizers, particularly stochastic gradient descent (SGD) and its adaptive variant Adam, adjust weights to minimize the model’s loss. The learning rate and batch size parameters influence SGD’s effectiveness, impacting the model’s convergence to optimal weights.\n",
    "\n",
    "Regularization techniques like early stopping, dropout layers, and batch normalization prevent overfitting. Early stopping halts training when validation performance declines, while dropout layers randomly deactivate neurons to generalize learning patterns. Batch normalization scales input data within each batch, stabilizing training. Additionally, DL’s signal-noise distinction in training data and learning curves help evaluate model generalization.\n",
    "\n",
    "The lecture concludes with insights on DeepChem, a library supporting DL applications in fields like drug discovery and materials science, showcasing DL's adaptability across scientific domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "# Youtube\n",
    "YouTubeVideo(id=\"introduction_to_deep_learning\", height=200, width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture Slides\n",
    "\n",
    "Download the lecture slides [Introduction to Deep Learning](Submodule_5/Lectures/Submodule_5_Lecture_1_Introduction_to_Deep_Learning.pptx).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quizzes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jupyterquiz\n",
    "\n",
    "from jupyterquiz import display_quiz\n",
    "\n",
    "display_quiz(\n",
    "    \"Submodule_5/Quizzes/Submodule_5_Quiz_1_Introduction_to_Deep_Learning.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tutorials\n",
    "\n",
    "- [Pfam Protein Sequence Classification with Tensorflow and Keras](Submodule_5/Tutorials/Submodule_5_Tutorial_1_Pfam_Protein_Sequence_Classification_with_Tensorflow_Keras.ipynb)\n",
    "- [Predicting the Solubility of Small Molecules](Submodule_5/Tutorials/Submodule_5_Tutorial_2_Predicting_the_Solubility_of_Small_Molecules.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercises\n",
    "\n",
    "- [Predicting Wine Types using Deep Learning](Submodule_5/Exercises/Submodule_5_Exercise_1_Predicting_Wine_Types_Deep_Learing.ipynb) ([Solution](Submodule_5/Exercises/Submodule_5_Exercise_1_Predicting_Wine_Types_Deep_Learing_Solution.ipynb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This submodule introduces deep learning (DL), a machine learning subset that builds layered data representations for tasks like image and sequence processing. Using artificial neural networks (ANNs) such as CNNs and RNNs, DL automates feature extraction and enhances accuracy through forward and backward data flows, loss functions, and optimizers like stochastic gradient descent. Techniques like dropout, batch normalization, and early stopping prevent overfitting. DL’s versatility is highlighted through applications in fields like drug discovery with the DeepChem library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "A reminder to shutdown VM and delete any relevant resources. <br><br>\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
