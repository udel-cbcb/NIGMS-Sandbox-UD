{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensuring Transparency, Fairness, and Privacy in Machine Learning\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a responsible AI/ML workflow using the **Pima Indians Diabetes dataset**,  focusing on key principles such as:\n",
    "- **Transparency** with **model explainability** using **SHAP**.\n",
    "- **Fairness** with **AIF360** for bias detection.\n",
    "- **Privacy** through **differentially private models** with **Diffprivlib**.\n",
    "- **Accountability** by mitigating **bias** in model training.\n",
    "\n",
    "This end-to-end example highlights how to go beyond just **accuracy** to build **ethical** and **trustworthy AI systems**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Load and preprocess a medical dataset for machine learning.\n",
    "- Implement a baseline Random Forest model and evaluate its accuracy.\n",
    "- Use SHAP to generate explainability insights.\n",
    "- Perform fairness analysis using AIF360 to detect bias.\n",
    "- Apply differential privacy to a Logistic Regression model.\n",
    "- Simulate bias mitigation techniques using reweighting strategies.\n",
    "- Visualize and compare model performance across responsible AI dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Python 3.x\n",
    "- Basic knowledge of machine learning with scikit-learn.\n",
    "- Familiarity with fairness and privacy in AI/ML.\n",
    "- Required libraries: pandas, numpy, scikit-learn, matplotlib, shap, aif360, diffprivlib.\n",
    "\n",
    "## Get Started\n",
    "Let’s begin by loading the dataset and performing a data-centric workflow. The workflow includes:\n",
    "- **Data Cleaning**: Handling missing values and biological impossibilities\n",
    "- **Baseline Model**: Standard Random Forest classifier\n",
    "- **Explainability**: SHAP values for model interpretability\n",
    "- **Fairness Analysis**: Disparate impact and statistical parity metrics\n",
    "- **Privacy Protection**: Differentially private logistic regression (ε=1.0)\n",
    "- **Bias Mitigation**: Sample reweighting based on age groups\n",
    "\n",
    "### Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required Python libraries for the project\n",
    "# -------------------------------\n",
    "# Library Installation Comments\n",
    "# -------------------------------\n",
    "# 1. pandas: For data manipulation and analysis (e.g., loading CSV files, cleaning data)\n",
    "# 2. numpy: For numerical computations (e.g., handling arrays, mathematical operations)\n",
    "# 3. scikit-learn: For machine learning tasks (e.g., model training, evaluation, preprocessing)\n",
    "# 4. matplotlib: For data visualization (e.g., plotting graphs, charts)\n",
    "# 5. shap: For model explainability (e.g., SHAP values to interpret model predictions)\n",
    "# 6. aif360: For fairness analysis (e.g., measuring bias, disparate impact)\n",
    "# 7. diffprivlib: For privacy-preserving machine learning (e.g., differential privacy)\n",
    "%pip install pandas numpy scikit-learn matplotlib shap aif360 diffprivlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Suppress Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import shap  # For explainability\n",
    "from aif360.datasets import BinaryLabelDataset  # For fairness\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from diffprivlib.models import LogisticRegression  # For differential privacy\n",
    "import warnings  # To suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Clean the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diabetes_data():\n",
    "    diabetes_data = '../../Data/pima-indians-diabetes.csv'\n",
    "    columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',\n",
    "               'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "    df = pd.read_csv(diabetes_data, header=None, names=columns, na_values='?', sep=',')\n",
    "    print('Dataset Shape:', df.shape)\n",
    "    print('Initial Missing Values:', df.isnull().sum())\n",
    "    return df\n",
    "\n",
    "# Load dataset\n",
    "df = load_diabetes_data()  # Load the diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Responsible AI/ML Workflow\n",
    "\n",
    "#### Step 1: Data Cleaning (Basic Quality Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros in specific columns with the median value of that column\n",
    "for col in ['Glucose', 'BloodPressure', 'BMI', 'SkinThickness', 'Insulin']:\n",
    "    df[col] = df[col].replace(0, df[col].median())  # Replace 0s with column median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop('Outcome', axis=1)  # Features (exclude 'Outcome' column)\n",
    "y = df['Outcome']  # Target variable ('Outcome')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Baseline Model (Transparency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)  # Fit model on training data\n",
    "y_pred = model.predict(X_test)  # Predict on test data\n",
    "baseline_acc = accuracy_score(y_test, y_pred)  # Calculate accuracy of baseline model\n",
    "print(\"\\nBaseline Model Accuracy:\", baseline_acc)  # Print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Explainability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to numpy array for SHAP analysis\n",
    "X_test_np = X_test.to_numpy()\n",
    "# Use shap.Explainer to explain model predictions\n",
    "explainer = shap.Explainer(model.predict, X_train.to_numpy())  # Initialize SHAP explainer\n",
    "shap_values = explainer(X_test_np)  # Get SHAP values for the test set\n",
    "print(\"\\nGenerating SHAP Summary Plot for Explainability...\")\n",
    "# Print the shape and type of SHAP values for debugging\n",
    "print(\"X_test_np shape:\", X_test_np.shape)\n",
    "print(\"shap_values type:\", type(shap_values))\n",
    "print(\"shap_values shape:\", shap_values.shape)\n",
    "print(\"Feature names:\", list(X.columns))  # Print feature names\n",
    "\n",
    "# Adjust if shap_values is a SHAP Explanation object (use .values to get the array)\n",
    "shap_vals = shap_values.values if hasattr(shap_values, 'values') else shap_values\n",
    "# Ensure the SHAP values match the shape of the test data\n",
    "assert shap_vals.shape == X_test_np.shape, \"SHAP values shape mismatch!\"\n",
    "\n",
    "# Plot SHAP values as a bar chart\n",
    "shap.summary_plot(shap_vals, X_test_np, feature_names=list(X.columns), plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart indicates the relative importance of various features in influencing the model's predictions, with Glucose being the most impactful feature (mean SHAP value around 0.25), followed by BMI (around 0.15), and Age (around 0.12). Other features like DiabetesPedigreeFunction, Insulin, Pregnancies, BloodPressure, and SkinThickness have progressively smaller impacts, with SkinThickness having the least influence (mean SHAP value around 0.03).\n",
    "\n",
    "This suggests that:\n",
    "- **Glucose** levels are the strongest predictor in the model, likely indicating their critical role in determining outcomes such as diabetes risk or presence.\n",
    "- **BMI** and **Age** are also significant contributors to the model’s predictions, but to a lesser extent than Glucose.\n",
    "- Features like **SkinThickness**, **BloodPressure**, **Pregnancies**, **Insulin**, and **DiabetesPedigreeFunction** have minimal impact compared to the top three features, though they still contribute to the model’s output.\n",
    " \n",
    "Overall, the model prioritizes metabolic and demographic factors (like glucose, BMI, and age) over other physiological measurements (like skin thickness or blood pressure) when making predictions. This hierarchy can help guide clinical or research focus on the most influential factors for diabetes prediction or management in the context of this specific model and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Fairness Analysis (Bias Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with labels for fairness analysis\n",
    "df_with_labels = X.copy()\n",
    "df_with_labels['Outcome'] = y\n",
    "df_with_labels['Age'] = (df_with_labels['Age'] >= 40).astype(int)  # Binarize Age FIRST\n",
    "dataset = BinaryLabelDataset(df=df_with_labels, label_names=['Outcome'],\n",
    "                                protected_attribute_names=['Age'])\n",
    "privileged_groups = [{'Age': 1}]\n",
    "unprivileged_groups = [{'Age': 0}]\n",
    "metric = BinaryLabelDatasetMetric(dataset,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "print(\"\\nFairness Metrics:\")\n",
    "print(\"Disparate Impact:\", metric.disparate_impact())\n",
    "print(\"Statistical Parity Difference:\", metric.statistical_parity_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics (Pima Indians Diabetes Dataset - Age as Protected Attribute)\n",
    "\n",
    "These metrics evaluate the fairness of a model predicting diabetes in the Pima Indians dataset, specifically focusing on potential disparities based on age (>= 40 vs. < 40).\n",
    "\n",
    "**1. Disparate Impact: 0.75 (Example Value)**\n",
    "\n",
    "* **What it means (specifically):** In this context, a Disparate Impact of 0.75 means that individuals younger than 40 (the *unprivileged* group) are 75% as likely to be predicted as having diabetes compared to individuals 40 and older (the *privileged* group).\n",
    "\n",
    "* **Implication:** This suggests the model might be under-predicting diabetes in younger individuals relative to older individuals.  A value closer to 1.0 would indicate greater parity. This warrants further investigation to determine if this difference is justified by the data or if it reflects bias.\n",
    "\n",
    "**2. Statistical Parity Difference: -0.15 (Example Value)**\n",
    "\n",
    "* **What it means (specifically):**  A Statistical Parity Difference of -0.15 indicates that the proportion of younger individuals (age < 40) predicted to have diabetes is 15 percentage points *lower* than the proportion of older individuals (age >= 40) predicted to have diabetes.\n",
    "\n",
    "* **Implication:** This reinforces the finding from the Disparate Impact metric.  It demonstrates a quantifiable difference in prediction rates, with younger individuals being less likely to be predicted as having diabetes.\n",
    "\n",
    "**Key Considerations for this Specific Scenario:**\n",
    "\n",
    "* **Age Binarization:** Age has been simplified into two categories (>= 40 and < 40).  This might not capture the full complexity of age's relationship with diabetes risk.\n",
    "* **Dataset Bias:** The Pima Indians dataset itself might contain biases that influence these metrics.  For example, if older individuals are overrepresented in the dataset or if the dataset reflects existing biases in diabetes screening practices, this could affect the results.\n",
    "* **Causation vs. Correlation:** These metrics show an association, not necessarily a causal link. They don't prove the model is *unfair*, but they highlight a potential disparity that requires further scrutiny.  The lower prediction rate for younger individuals might reflect a genuine lower prevalence of diabetes in that age group.\n",
    "* **Clinical Implications:**  If the model under-predicts diabetes in younger individuals, this could lead to delayed diagnosis and treatment, which has serious health consequences.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "* **Analyze the Data:** Examine the actual rates of diabetes diagnosis within the different age groups in the dataset.\n",
    "* **Consider Other Metrics:** Use additional fairness metrics (like Equalized Odds Difference) to gain a more comprehensive understanding.\n",
    "* **Investigate Potential Bias:** If bias is suspected, explore mitigation techniques, but do so carefully and thoughtfully in a medical context, consulting with domain experts.  Techniques like reweighting or adversarial debiasing might be considered.\n",
    "* **Explainability:** Use methods like SHAP values to understand *why* the model makes the predictions it does. This can help identify potential sources of bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Privacy-Preserving Model with Differential Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a logistic regression model with differential privacy\n",
    "dp_model = LogisticRegression(epsilon=1.0, random_state=42)\n",
    "dp_model.fit(X_train, y_train)  # Fit model with differential privacy\n",
    "dp_pred = dp_model.predict(X_test)  # Predict on test data\n",
    "dp_acc = accuracy_score(y_test, dp_pred)  # Calculate accuracy of the DP model\n",
    "print(\"\\nDifferential Privacy Model Accuracy:\", dp_acc)  # Print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the differentially private model achieved about 31.8% accuracy.\n",
    "- **Low Accuracy**: 31.8% is relatively low, which is common with differential privacy. Protecting privacy often reduces accuracy.\n",
    "- **Epsilon's Impact**: Epsilon's value affects accuracy. Lower epsilon (stronger privacy) usually means lower accuracy.\n",
    "- **Context Matters**: Whether 31.8% is acceptable depends on the application. High privacy might be more important than high accuracy in some situations.\n",
    "\n",
    "The differentially private logistic regression model has a low accuracy (31.8%), likely due to the privacy mechanisms. Consider adjusting epsilon or trying other models to balance privacy and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Accountability - Simulate Bias Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-weight training samples to mitigate bias based on age\n",
    "weights = np.ones(len(y_train))  # Default weights (1 for all)\n",
    "weights[X_train['Age'] >= 40] = 1.2  # Increase weight for privileged group (Age >= 40)\n",
    "# Train a reweighted Random Forest model\n",
    "model_reweighted = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_reweighted.fit(X_train, y_train, sample_weight=weights)  # Fit model with reweighted samples\n",
    "y_pred_reweighted = model_reweighted.predict(X_test)  # Predict on test data\n",
    "reweighted_acc = accuracy_score(y_test, y_pred_reweighted)  # Calculate accuracy of reweighted model\n",
    "print(\"Reweighted Model Accuracy (Bias Mitigated):\", reweighted_acc)  # Print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of 0.7597 for your reweighted model indicates that it correctly predicted the outcomes (e.g., whether a patient has diabetes or not) about 75.97% of the time when applied to the test data.\n",
    "\n",
    "- **Accuracy**: This is a standard performance metric that shows how often the model's predictions are correct. In this case, your reweighted model correctly predicted the target variable (e.g., diabetes outcome) 75.97% of the time.\n",
    "- **Bias Mitigation Effect**: Since you re-weighted the training samples based on age, the model should now be less biased towards any one age group. The bias mitigation process works by giving more importance to underrepresented or disadvantaged groups, such as younger individuals in this case (age < 40).\n",
    "\n",
    "If the baseline (original) model had a significant accuracy difference between age groups, this reweighted model aims to reduce that disparity.\n",
    "The 75.97% accuracy represents the performance of the model after the bias mitigation, so the improvement or change in accuracy could be evaluated against the baseline model (which you'd have to compare using the same metric).\n",
    "\n",
    "Next Steps:\n",
    "- To assess the effectiveness of the bias mitigation, you could compare the accuracy of this reweighted model with the baseline (non-reweighted) model. If the accuracy is lower or roughly the same, it might indicate that while bias was reduced, it came at the cost of some overall model performance.\n",
    "- If the model performs similarly to the baseline while showing less disparity across age groups (as confirmed by fairness metrics like statistical parity difference or disparate impact), then you can conclude that bias mitigation has been successful without severely harming predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a bar chart comparing the accuracy of baseline, differential privacy, and bias-mitigated models\n",
    "plt.bar(['Baseline', 'Differential Privacy', 'Bias Mitigated'], \n",
    "        [baseline_acc, dp_acc, reweighted_acc])\n",
    "plt.ylim(0, 1)  # Set y-axis limits to range from 0 to 1\n",
    "plt.ylabel('Accuracy')  # Label y-axis\n",
    "plt.title('Model Performance Across Responsible AI Approaches')  # Title\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings from the Diagram\n",
    "\n",
    "#### 1. Fairness Metrics (Baseline Model)\n",
    "- **Disparate Impact**: 0.5466\n",
    "  - Indicates significant bias, with the protected group receiving favorable outcomes at only 54.66% the rate of the privileged group.\n",
    "- **Statistical Parity Difference**: -0.2365\n",
    "  - Shows the protected group has a 23.65% lower probability of a favorable outcome compared to the privileged group, confirming unfairness in the baseline model.\n",
    "\n",
    "#### 2. Model Accuracies Across Responsible AI Approaches\n",
    "- **Baseline Model**:\n",
    "  - Accuracy: ~80% (highest among the three).\n",
    "  - Trade-off: High accuracy but unfair, as evidenced by the fairness metrics.\n",
    "- **Differential Privacy Model**:\n",
    "  - Accuracy: 31.82% (lowest among the three).\n",
    "  - Trade-off: Poor predictive performance but likely enhances privacy and potentially reduces bias (fairness metrics not provided).\n",
    "- **Reweighted (Bias Mitigated) Model**:\n",
    "  - Accuracy: 75.97% (moderate, slightly lower than baseline).\n",
    "  - Trade-off: Slightly lower accuracy than the baseline but likely improved fairness compared to the baseline, balancing equity and performance.\n",
    "\n",
    "#### 3. Visual Insights from the Bar Chart\n",
    "- The chart shows model performance (accuracy) across three approaches:\n",
    "  - Baseline: Highest accuracy (~0.8 or 80%).\n",
    "  - Differential Privacy: Lowest accuracy (~0.3 or 30%).\n",
    "  - Bias Mitigated (Reweighted): Moderate accuracy (~0.75–0.8 or 75.97%).\n",
    "- This highlights a trade-off between accuracy, fairness, and privacy:\n",
    "  - The baseline prioritizes accuracy but sacrifices fairness.\n",
    "  - Differential privacy prioritizes privacy but sacrifices accuracy.\n",
    "  - The reweighted model strikes a balance, maintaining reasonable accuracy while aiming to mitigate bias.\n",
    "\n",
    "#### 4. Overall Interpretation\n",
    "- The baseline model performs best in terms of accuracy but is biased against a protected group (likely related to age, as seen in your code reweighting Age >= 40).\n",
    "- The reweighted model (bias mitigated) reduces this bias (though exact fairness metrics are needed for confirmation) at the cost of a modest accuracy drop (75.97% vs. 80%).\n",
    "- The differential privacy model severely reduces accuracy (31.82%) for privacy benefits, with unclear fairness impact.\n",
    "- The choice of approach depends on the priorities: accuracy, fairness, or privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we explored a comprehensive Responsible AI workflow:\n",
    "- Achieved model transparency using SHAP.\n",
    "- Evaluated fairness and bias with AIF360.\n",
    "- Implemented differential privacy with Diffprivlib.\n",
    "- Enhanced model accountability by mitigating bias.\n",
    "- Demonstrated how responsible AI practices can improve trustworthiness without sacrificing performance.\n",
    "\n",
    "By incorporating ethical AI principles, we can build more reliable and fair AI systems that are suitable for real-world deployment, especially in sensitive domains like healthcare.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
