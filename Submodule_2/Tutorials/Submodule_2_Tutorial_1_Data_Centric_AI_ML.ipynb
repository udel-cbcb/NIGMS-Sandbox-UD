{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Centric AI/ML: Diabetes Dataset Example\n",
    "\n",
    "\n",
    "## Overview\n",
    "In this notebook, we explore the concept of **Data-Centric AI/ML**, where the focus is on improving the quality of the dataset to enhance model performance. Using the **Diabetes Dataset**, we demonstrate how data cleaning, feature engineering, and iterative data improvement can lead to better model accuracy. This approach emphasizes the importance of high-quality data over complex model architectures.\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "* Understand the principles of **Data-Centric AI/ML**.\n",
    "* Learn how to clean and preprocess a biomedical dataset effectively.\n",
    "* Perform **feature engineering** to create meaningful features.\n",
    "* Identify and correct **noisy labels** in the dataset.\n",
    "* Evaluate the impact of data-centric improvements on model performance.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Before starting, ensure you have the following:\n",
    "* Basic knowledge of Python and Pandas.\n",
    "* Familiarity with machine learning concepts (e.g., classification, Random Forests).\n",
    "* Libraries installed: pandas numpy scikit-learn matplotlib\n",
    "\n",
    "## Get Started\n",
    "Let’s begin by loading the dataset and performing a data-centric workflow. The workflow includes:\n",
    "* **Data Cleaning**: Handling missing values and outliers.\n",
    "* **Feature Engineering**: Creating new features like BMI categories.\n",
    "* **Model Training**: Training a baseline Random Forest model.\n",
    "* **Data-Centric Iteration**: Identifying and correcting noisy labels to improve model performance.\n",
    "\n",
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install essential Python libraries for data analysis, machine learning, and visualization\n",
    "# - pandas: For data manipulation and handling the diabetes dataset\n",
    "# - numpy: For numerical operations and array management\n",
    "# - scikit-learn: For machine learning models (e.g., RandomForestClassifier) and metrics (e.g., accuracy_score)\n",
    "# - matplotlib: For plotting SHAP summary and model performance comparisons\n",
    "%pip install pandas numpy scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "import pandas as pd                # For data manipulation and analysis\n",
    "import numpy as np                 # For numerical operations and handling arrays\n",
    "\n",
    "# Importing machine learning libraries\n",
    "from sklearn.model_selection import train_test_split  # To split data into training and testing sets\n",
    "from sklearn.ensemble import RandomForestClassifier   # Random Forest algorithm for classification\n",
    "from sklearn.metrics import accuracy_score            # To evaluate the accuracy of the model\n",
    "\n",
    "# Importing visualization library\n",
    "import matplotlib.pyplot as plt    # For plotting graphs and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diabetes_data():\n",
    "    # Define the path to the diabetes dataset (Pima Indians Diabetes Dataset)\n",
    "    diabetes_data = \"../../Data/pima-indians-diabetes.csv\"\n",
    "\n",
    "    # Define the column names for the dataset\n",
    "    columns = [\n",
    "        'Pregnancies',               # Number of times pregnant\n",
    "        'Glucose',                   # Plasma glucose concentration (mg/dL)\n",
    "        'BloodPressure',             # Diastolic blood pressure (mm Hg)\n",
    "        'SkinThickness',             # Triceps skinfold thickness (mm)\n",
    "        'Insulin',                   # 2-Hour serum insulin (mu U/ml)\n",
    "        'BMI',                       # Body mass index (weight in kg/(height in m)^2)\n",
    "        'DiabetesPedigreeFunction',  # Diabetes pedigree function (genetic risk)\n",
    "        'Age',                       # Age in years\n",
    "        'Outcome'                    # Class variable (0: Non-diabetic, 1: Diabetic)\n",
    "    ]\n",
    "\n",
    "    # Load the dataset into a DataFrame\n",
    "    df = pd.read_csv(\n",
    "        diabetes_data,   # File path to the CSV data\n",
    "        header=None,     # No header row in the original file\n",
    "        names=columns,   # Assign column names defined above\n",
    "        na_values=\"?\",   # Treat \"?\" as NaN (missing values)\n",
    "        sep=','          # CSV file uses commas as the delimiter\n",
    "    )\n",
    "    \n",
    "    # Display the shape of the dataset (rows, columns)\n",
    "    print(\"Dataset Shape:\", df.shape)\n",
    "    \n",
    "    # Show the count of missing values in each column\n",
    "    print(\"Initial Missing Values:\\n\", df.isnull().sum())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Centric Workflow with Synthetic Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data-Centric Workflow with Synthetic Noise\n",
    "def data_centric_workflow():\n",
    "    # Load the cleaned diabetes dataset\n",
    "    df = load_diabetes_data()\n",
    "    \n",
    "    # Step 1: Data Cleaning\n",
    "    # Replace zeros with the median value in columns where zero is not a valid measurement\n",
    "    for col in ['Glucose', 'BloodPressure', 'BMI', 'SkinThickness', 'Insulin']:\n",
    "        zero_count = (df[col] == 0).sum()  # Count zeros in the column\n",
    "        print(f\"Zeros in {col}: {zero_count}\")\n",
    "        df[col] = df[col].replace(0, df[col].median())  # Replace zeros with the median value\n",
    "    \n",
    "    # Remove extreme outliers for 'BMI' and 'BloodPressure' columns\n",
    "    # Filter the DataFrame to keep rows where 'BMI' is less than or equal to 60 \n",
    "    # and 'BloodPressure' is less than or equal to 200\n",
    "    df = df[(df['BMI'] <= 60) & (df['BloodPressure'] <= 200)]\n",
    "    \n",
    "    # Display the new shape of the DataFrame after removing outliers\n",
    "    print(\"\\nAfter Cleaning Shape:\", df.shape)  # This shows the number of rows and columns remaining\n",
    "    \n",
    "    # Step 2: Feature Engineering\n",
    "    # Create a new categorical feature based on BMI ranges\n",
    "    # The `pd.cut` function divides the 'BMI' values into intervals (bins) and assigns a label to each interval.\n",
    "    df['BMI_category'] = pd.cut(\n",
    "        df['BMI'],  # Column containing BMI values\n",
    "        bins=[0, 18.5, 25, 30, 100],  # Define BMI ranges: \n",
    "                                      # 0 - 18.5 (Underweight), \n",
    "                                      # 18.5 - 25 (Normal weight), \n",
    "                                      # 25 - 30 (Overweight), \n",
    "                                      # 30 - 100 (Obese)\n",
    "        labels=['underweight', 'normal', 'overweight', 'obese']  # Assign category labels\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Step 3: Introduce Synthetic Label Noise\n",
    "    np.random.seed(42)  # Set random seed for reproducibility\n",
    "    # Randomly select 20 indices from the dataframe without replacement\n",
    "    noise_idx = np.random.choice(df.index, size=20, replace=False)\n",
    "    # Flip the 'Outcome' values at the selected indices (0 becomes 1, and 1 becomes 0)\n",
    "    df.loc[noise_idx, 'Outcome'] = 1 - df.loc[noise_idx, 'Outcome']\n",
    "    # Print confirmation message\n",
    "    print(\"\\nIntroduced synthetic noise to 20 labels.\")\n",
    "    \n",
    "    # Step 4: Initial Model Training\n",
    "    # Prepare features and labels\n",
    "    X = df.drop(['Outcome', 'BMI_category'], axis=1)  # Drop target and new feature for model input\n",
    "    y = df['Outcome']  # Target variable\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    # X: Feature matrix, y: Target variable\n",
    "    # test_size=0.2 -> 20% of the data will be used for testing, 80% for training\n",
    "    # random_state=42 -> Ensures reproducibility of the split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    \n",
    "    # Train a Random Forest model\n",
    "    # Initialize a Random Forest Classifier\n",
    "    # n_estimators=100 -> The model will use 100 decision trees\n",
    "    # random_state=42 -> Ensures reproducibility of the results\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    # Train the model using the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    # Evaluate initial model performance\n",
    "    # Generate predictions on the test set\n",
    "    initial_pred = model.predict(X_test)\n",
    "    # Calculate accuracy by comparing predictions with actual labels\n",
    "    initial_acc = accuracy_score(y_test, initial_pred)\n",
    "    # Print the accuracy of the model before any modifications\n",
    "    print(\"\\nInitial Model Accuracy (with noise):\", initial_acc)\n",
    "\n",
    "    \n",
    "    # Step 5: Iterative Data Improvement - Detect and Fix Noisy Labels\n",
    "    # Use model predictions to identify potentially mislabeled data\n",
    "    full_pred = model.predict(X)\n",
    "    \n",
    "    # Define a rule to identify potentially mislabeled data\n",
    "    # Find indices where:\n",
    "    # - The model predicts 0 (no diabetes), but the actual label is 1 (diabetes)\n",
    "    # - The glucose level is in the lowest 25% (first quartile), which contradicts the diagnosis\n",
    "    suspicious_idx = df[\n",
    "        (full_pred == 0) &  # Model predicts no diabetes\n",
    "        (df['Outcome'] == 1) &  # Actual label indicates diabetes\n",
    "        (df['Glucose'] < df['Glucose'].quantile(0.25))  # Glucose level is in the lowest quartile\n",
    "    ].index\n",
    "    \n",
    "    # Print the number of suspicious labels detected\n",
    "    print(\"Suspicious Labels Found:\", len(suspicious_idx))\n",
    "\n",
    "    \n",
    "    # Correct potentially mislabeled data based on the defined rule\n",
    "    # Check if any suspicious labels were identified\n",
    "    if len(suspicious_idx) > 0:\n",
    "        # Change the labels of suspicious cases from 1 (diabetes) to 0 (no diabetes)\n",
    "        df.loc[suspicious_idx, 'Outcome'] = 0  \n",
    "        print(f\"Corrected {len(suspicious_idx)} labels from 1 to 0.\")\n",
    "    else:\n",
    "        # If no suspicious labels were found, notify the user\n",
    "        print(\"No suspicious labels found with current rule.\")\n",
    "\n",
    "    \n",
    "    # Retrain the model with the corrected dataset\n",
    "    # Define feature set (excluding the target variable and categorical feature)\n",
    "    X = df.drop(['Outcome', 'BMI_category'], axis=1)  \n",
    "    # Define target variable\n",
    "    y = df['Outcome']\n",
    "    # Split the dataset into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # Train the Random Forest model again with the updated dataset\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    # Evaluate the improved model performance\n",
    "    # Use the trained model to make predictions on the test data (X_test)\n",
    "    improved_pred = model.predict(X_test)\n",
    "    # Calculate accuracy by comparing predictions to actual test labels (y_test)\n",
    "    improved_acc = accuracy_score(y_test, improved_pred)\n",
    "    # Display the calculated accuracy score of the improved model\n",
    "    print(\"Improved Model Accuracy:\", improved_acc)  \n",
    "    \n",
    "    # Visualize the improvement in model accuracy\n",
    "    # Create a bar chart comparing initial and improved accuracy scores\n",
    "    plt.bar(['Initial (Noisy)', 'Improved'], [initial_acc, improved_acc])  \n",
    "    # Set y-axis limits from 0 to 1 to standardize the accuracy scale for better visualization\n",
    "    plt.ylim(0, 1)\n",
    "    # Add a label to the y-axis indicating that it represents accuracy values\n",
    "    plt.ylabel('Accuracy') \n",
    "    # Add a descriptive title to the plot\n",
    "    plt.title('Model Performance Before and After Data-Centric Improvement') \n",
    "    # Display the bar chart to the user\n",
    "    plt.show()  \n",
    "    \n",
    "    # Return the dataframe and trained model for potential further use\n",
    "    return df, model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point of the script\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the data-centric machine learning workflow\n",
    "    # The function returns the cleaned DataFrame and the trained model\n",
    "    cleaned_df, final_model = data_centric_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suspicious Labels**: The rule will typically find 5-15 labels (varies with split), some of which overlap with the synthetic noise.\n",
    "\n",
    "**Accuracy Improvement**: You’ll see a small but noticeable increase (e.g., 0.68 to 0.72), demonstrating the value of data refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we demonstrated how a data-centric approach can improve model performance by focusing on:\n",
    "* **Iterative Data Improvement**: The code now actively detects and corrects noisy labels, retraining the model to show improved performance.\n",
    "* **Label Consistency and Noise Reduction**: Synthetic noise is introduced and then mitigated, mimicking real-world data imperfections.\n",
    "* **Data Quality Over Model Complexity**: The focus remains on fixing the data, not tweaking the model.\n",
    "* **Domain Knowledge Integration**: The rule uses Glucose (a key diabetes indicator) and model predictions, reflecting biomedical intuition.\n",
    "* **Quantifying Improvements**: The accuracy increases and plot clearly shows the impact of data-centric changes.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
