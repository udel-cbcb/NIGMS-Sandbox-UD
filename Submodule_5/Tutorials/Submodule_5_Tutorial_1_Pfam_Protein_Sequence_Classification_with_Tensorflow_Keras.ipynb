{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pfam Protein Sequence Classification with Tensorflow and Keras\n",
    "\n",
    "Adapted from Saleh Alkhalifa. [Machine Learning in Biotechnology and Life Sciences](https://github.com/PacktPublishing/Machine-Learning-in-Biotechnology-and-Life-Sciences).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial provides a step-by-step guide for building a deep learning model to classify protein sequences by their family accessions using the Pfam dataset. Leveraging TensorFlow and Keras, we will:\n",
    "\n",
    "- Process amino acid sequences into a format suitable for neural networks.\n",
    "- Train a deep learning model to predict protein family classifications.\n",
    "- Evaluate performance and interpret results to understand sequence-function relationships.\n",
    "\n",
    "By the end, you will have a working model capable of automating protein family annotation, with applications in bioinformatics, drug discovery, and evolutionary biology.\n",
    "\n",
    "### Pfam\n",
    "\n",
    "The [Pfam](http://pfam.xfam.org/) dataset consists of several columns, as follows:\n",
    "\n",
    "- *Family_id*: The name of the family that the seqeunce belongs to (for example, filamin).\n",
    "- *Family Accession*: The class or output that our model will aim to predict.\n",
    "- *Sequence*: The amino acid sequence we will use as input for our model\n",
    "\n",
    "Pfam: The protein families database in 2021: J. Mistry, S. Chuguransky, L. Williams, M. Qureshi, G.A. Salazar, E.L.L. Sonnhammer, S.C.E. Tosatto, L. Paladin, S. Raj, L.J. Richardson, R.D. Finn, A. Bateman\n",
    "Nucleic Acids Research (2020) doi: 10.1093/nar/gkaa913\n",
    "\n",
    "### TensorFlow and Keras\n",
    "\n",
    "- **[TensorFlow](https://www.tensorflow.org/)** is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community. \n",
    "- **[Keras](https://keras.io/)** is a deep learning API written in Python, running on top of the machine learning platform TensorFlow.\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn how to preprocess protein sequence data for deep learning\n",
    "    - Encode amino acid sequences\n",
    "    - Pad sequences to uniform length\n",
    "    - Convert labels to categorical format\n",
    "- Build and train a deep learning model for sequence classification using TensorFlow/Keras\n",
    "    - Implement embedding layers\n",
    "    - Use bidirectional LSTM architecture\n",
    "    - Apply dropout for regularization\n",
    "- Evaluate model performance using standard metrics\n",
    "    - Analyze accuracy and loss curves\n",
    "    - Interpret classification reports\n",
    "    - Visualize confusion matrices\n",
    "\n",
    "### Tasks to be completed\n",
    "\n",
    "- Download and prepare Pfam dataset\n",
    "- Preprocess protein sequences and labels\n",
    "- Build and train deep learning model\n",
    "- Evaluate model performance\n",
    "- Generate predictions and visualize results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "- Please select kernel \"conda_tensorflow2_p310\" from SageMaker notebook instance.\n",
    "- Import the necessary libraries and download the needed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyplot module from matplotlib for plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the numpy library for numerical operations, often used for array manipulations.\n",
    "import numpy as np\n",
    "\n",
    "# Import the pandas library for data manipulation and analysis, especially for DataFrames.\n",
    "import pandas as pd\n",
    "\n",
    "# Import the seaborn library for statistical data visualization, built on top of matplotlib.\n",
    "import seaborn as sns\n",
    "\n",
    "# Import specific layers from keras.layers for building neural networks.\n",
    "from keras.layers import (\n",
    "    LSTM, # Import LSTM layer for Long Short-Term Memory networks.\n",
    "    Bidirectional, # Import Bidirectional layer for bidirectional processing in RNNs.\n",
    "    Conv1D, # Import Conv1D layer for 1D convolutional neural networks.\n",
    "    Dense, # Import Dense layer for fully connected neural networks.\n",
    "    Dropout, # Import Dropout layer for regularization to prevent overfitting.\n",
    "    Embedding, # Import Embedding layer for creating word embeddings.\n",
    "    Flatten, # Import Flatten layer to flatten the input tensor.\n",
    "    Input, # Import Input layer to instantiate a Keras tensor.\n",
    "    MaxPooling1D, # Import MaxPooling1D layer for 1D max pooling.\n",
    ")\n",
    "\n",
    "# Import the Model class and Sequential class from keras.models to define neural network models.\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "# Import the pad_sequences function from keras.preprocessing.sequence for padding sequences to the same length.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Import the l2 regularizer from keras.regularizers for applying L2 regularization to layers.\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Import classification_report and confusion_matrix from sklearn.metrics for model evaluation.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Import train_test_split from sklearn.model_selection for splitting data into training and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import LabelEncoder from sklearn.preprocessing for encoding categorical labels into numerical form.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import the tensorflow library, the main deep learning framework.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import EarlyStopping callback from tensorflow.keras.callbacks to stop training early when validation loss stops improving.\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Import to_categorical from tensorflow.keras.utils for one-hot encoding of categorical variables.\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Import Sequential model from tensorflow.keras.models for linear stack of layers.\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import Embedding, Bidirectional, LSTM, Dropout, and Dense layers from tensorflow.keras.layers for building neural networks.\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "\n",
    "# Set the default style for seaborn plots to \"darkgrid\" for better visualization.\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Import the os module to interact with the operating system, used here for environment variables.\n",
    "import os\n",
    "# Force CPU usage if no GPU detected\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Set environment variable to force TensorFlow to use CPU only by disabling GPU visibility.\n",
    "\n",
    "# Import the tensorflow library again (it's already imported above, this line might be redundant).\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the number of GPUs available to TensorFlow, useful for checking GPU setup.\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "import warnings\n",
    "# Filter out and ignore all warning messages that might be generated during the execution of the code.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Note: The first time run these imports may take some time, because Matplotlib is building the font cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset\n",
    "\n",
    "We load the dataset directly into a pandas DataFrame for analysis, streamlining the data ingestion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL for the dataset files hosted on GitHub.\n",
    "URL = \"https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-in-Biotechnology-and-Life-Sciences/main/datasets/dataset_pfam\"\n",
    "\n",
    "# Initialize an empty list called 'files' to store DataFrames.\n",
    "files = []\n",
    "\n",
    "# Loop 8 times to read in 8 different CSV files.\n",
    "for i in range(8):\n",
    "    # Read each CSV file from the specified URL pattern into a pandas DataFrame.\n",
    "    # The filename is constructed by appending 'dataset_pfam_seq_sd' and the loop index (i+1), followed by '.csv'.\n",
    "    # 'index_col=None' prevents pandas from using the first column as index.\n",
    "    # 'header=0' sets the first row as the header of the DataFrame.\n",
    "    df = pd.read_csv(f\"{URL}/dataset_pfam_seq_sd{i+1}.csv\", index_col=None, header=0)\n",
    "    \n",
    "    # Append the DataFrame read from the CSV file to the 'files' list.\n",
    "    files.append(df)\n",
    "\n",
    "# Concatenate all DataFrames stored in the 'files' list into a single DataFrame 'df'.\n",
    "# 'axis=0' concatenates along rows (vertically).\n",
    "# 'ignore_index=True' resets the index of the resulting DataFrame to a new sequential index.\n",
    "df = pd.concat(files, axis=0, ignore_index=True)\n",
    "\n",
    "# Print the shape (number of rows and columns) of the concatenated DataFrame 'df'.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly inspect your dataset's structure and first few rows, use df.head(). This displays the initial 5 entries by default, giving you a snapshot of column names, data types, and sample values—helpful for early data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify missing values in your dataset, use df.isna().sum(). This command checks for NaN values across each column in the DataFrame df and returns the total count of missing entries per column, helping you quickly assess data completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the top 10 most frequent family IDs from a DataFrame column by grouping the data by family_id, counts occurrences of each ID, and returns the 10 most abundant families in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select the 'family_id' column from DataFrame df\n",
    "# 2. Group the data by the same 'family_id' values\n",
    "# 3. For each group, count the occurrences of each family_id (value_counts)\n",
    "# 4. From these counts, select the top 10 most frequent family_ids (nlargest)\n",
    "df[\"family_id\"].groupby(df[\"family_id\"]).value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the top 10 most frequent family accessions, group the DataFrame column df['family_accession'] by itself, count occurrences of each value, and extract the 10 largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown of operations:\n",
    "# 1. Select the 'family_accession' column from DataFrame df\n",
    "# 2. Group the data by the same 'family_accession' values (creates groups of identical accessions)\n",
    "# 3. Count occurrences of each accession within its own group (redundant in this case)\n",
    "# 4. Select the top 10 most frequent accessions\n",
    "df[\"family_accession\"].groupby(df[\"family_accession\"]).value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a histogram of sequence length frequencies to analyze their distribution in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distribution plot of protein sequence lengths using seaborn's displot\n",
    "sns.displot(\n",
    "    df[\"sequence\"].apply(lambda x: len(x)),  # Calculate length of each sequence in 'sequence' column\n",
    "    bins=75,            # Use 75 bins to show distribution granularity\n",
    "    height=4,           # Set plot height to 4 inches  \n",
    "    aspect=2            # Set aspect ratio (width = height * aspect = 8 inches)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean sequence length can be obtained by first determining the character count of each entry in the 'sequence' column using str.len(), then calculating the average of these lengths with .mean()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average length of protein sequences in the DataFrame\n",
    "# 1. df[\"sequence\"] - Selects the 'sequence' column containing protein sequences\n",
    "# 2. .str.len() - Computes the length of each string in the sequence column\n",
    "# 3. .mean() - Calculates the arithmetic mean of all sequence lengths\n",
    "\n",
    "df[\"sequence\"].str.len().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum sequence length can be obtained by first determining the character count of each entry in the 'sequence' column using str.len(), then calculating the average of these lengths with .min()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the minimum length of protein sequences in the DataFrame\n",
    "# 1. df[\"sequence\"] - Accesses the 'sequence' column containing protein sequences\n",
    "# 2. .str.len() - Computes the length of each protein sequence (number of amino acids)\n",
    "# 3. .min() - Returns the smallest sequence length found in the column\n",
    "\n",
    "df[\"sequence\"].str.len().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum sequence length can be obtained by first determining the character count of each entry in the 'sequence' column using str.len(), then calculating the average of these lengths with .max()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the maximum length of protein sequences in the DataFrame\n",
    "# 1. df[\"sequence\"] - Selects the 'sequence' column containing protein sequences (amino acid strings)\n",
    "# 2. .str.len() - Computes the length (number of amino acids) for each sequence\n",
    "# 3. .max() - Identifies and returns the largest sequence length in the dataset\n",
    "\n",
    "df[\"sequence\"].str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mediam sequence length can be obtained by first determining the character count of each entry in the 'sequence' column using str.len(), then calculating the average of these lengths with .median()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median length of protein sequences in the DataFrame\n",
    "# 1. df[\"sequence\"] - Selects the column containing protein sequence strings\n",
    "# 2. .str.len() - Computes the length (number of amino acids) for each sequence\n",
    "# 3. .median() - Calculates the median value of all sequence lengths\n",
    "\n",
    "df[\"sequence\"].str.len().median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all family accessions where the count exceeds 1200, filtering for high-frequency records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to only include protein families with more than 1200 sequences\n",
    "# 1. df.groupby(\"family_accession\") - Groups the DataFrame by protein family accession IDs\n",
    "# 2. .filter(lambda x: len(x) > 1200) - Keeps only groups with more than 1200 sequences\n",
    "#    - lambda x: len(x) counts the number of sequences in each family group\n",
    "#    - > 1200 specifies the minimum count threshold\n",
    "# 3. Stores the filtered result in df_filt\n",
    "\n",
    "df_filt = df.groupby(\"family_accession\").filter(lambda x: len(x) > 1200)\n",
    "\n",
    "# Display the filtered DataFrame showing only families meeting the criteria\n",
    "df_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a balanced dataset by sampling 1200 sequences from each protein family\n",
    "# 1. df_filt.groupby(\"family_accession\") - Groups the filtered DataFrame by protein family accession\n",
    "# 2. .apply(lambda x: x.sample(1200)) - For each family group:\n",
    "#    - Takes a random sample of exactly 1200 sequences (using pandas sample())\n",
    "#    - Ensures equal representation from each family\n",
    "# 3. Stores the balanced dataset in df_bal\n",
    "\n",
    "df_bal = df_filt.groupby(\"family_accession\").apply(lambda x: x.sample(1200))\n",
    "\n",
    "# Verify the balancing by counting sequences per family\n",
    "# Shows the number of samples for each family (should all be 1200)\n",
    "df_bal.family_accession.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek into the balanced dataset\n",
    "df_bal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input dataframe for modeling\n",
    "\n",
    "The **reset_index**() function in pandas is used to restore a DataFrame's index to the default integer-based sequence (0 to n-1, where n represents the total number of rows) or to flatten a multi-level hierarchical index. When applied, this operation converts the existing index into a regular column within the DataFrame, unless explicitly instructed otherwise using the drop=True parameter. This function is particularly valuable for restructuring DataFrames after operations that modify the index, such as grouping, pivoting, or sorting, as it returns the data to a simpler, more conventional format. By transforming indices into columns, reset_index() enables easier data manipulation and analysis, especially when working with aggregated results or preparing data for visualization. The function also provides control over index preservation through its parameters, allowing users to either retain the original index as a new column or discard it entirely, making it a versatile tool for data cleaning and reorganization in pandas workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reduced DataFrame containing only the essential columns for analysis:\n",
    "# 1. Selects only two columns from the balanced DataFrame (df_bal):\n",
    "#    - \"family_accession\": The protein family identifier\n",
    "#    - \"sequence\": The amino acid sequence data\n",
    "# 2. Resets the index to default integer-based indexing:\n",
    "#    - drop=True ensures the old index is discarded (not added as a column)\n",
    "# 3. Stores the result in df_red (short for \"reduced DataFrame\")\n",
    "\n",
    "df_red = df_bal[[\"family_accession\", \"sequence\"]].reset_index(drop=True)\n",
    "\n",
    "# Display the first 5 rows of the reduced DataFrame to verify the structure\n",
    "df_red.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of distinct classes present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique protein family classes in the reduced dataset:\n",
    "# 1. df_red.family_accession.value_counts() - Counts occurrences of each family accession\n",
    "#    (returns a Series with family_accessions as index and counts as values)\n",
    "# 2. len() - Counts the number of unique family accessions (length of the Series index)\n",
    "# 3. Stores the result in num_classes (total number of distinct protein families)\n",
    "\n",
    "num_classes = len(df_red.family_accession.value_counts())\n",
    "\n",
    "# Display/return the number of classes (for verification or use in subsequent code)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total number of distinct Pfam family accession IDs present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'family_accession' column of the dataframe 'df_red'\n",
    "df_red.family_accession.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train and test datasets\n",
    "\n",
    "Split the data into 75% for training (X_train) and 25% for testing (X_test). Further divide the test set equally: 50% for validation (X_val) and 50% for final testing (X_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the DataFrame 'df_red' into training set 'X_train' and a temporary test set 'X_Test', allocating 25% of the data to the test set.\n",
    "X_train, X_Test = train_test_split(df_red, test_size=0.25)\n",
    "\n",
    "# Splits the temporary test set 'X_Test' further into validation set 'X_val' and final test set 'X_test', allocating 50% of 'X_Test' to the final test set.\n",
    "X_val, X_test = train_test_split(X_Test, test_size=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of samples in the training, test, and validation splits to confirm dataset distribution aligns with expected ratios (e.g., 70-15-15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create amino acid sequence dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_seq_dict = {\n",
    "    \"A\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"E\": 4,\n",
    "    \"F\": 5,\n",
    "    \"G\": 6,\n",
    "    \"H\": 7,\n",
    "    \"I\": 8,\n",
    "    \"K\": 9,\n",
    "    \"L\": 10,\n",
    "    \"M\": 11,\n",
    "    \"N\": 12,\n",
    "    \"P\": 13,\n",
    "    \"Q\": 14,\n",
    "    \"R\": 15,\n",
    "    \"S\": 16,\n",
    "    \"T\": 17,\n",
    "    \"V\": 18,\n",
    "    \"W\": 19,\n",
    "    \"Y\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode amino acid sequence using the dictionary above\n",
    "# Define a function named 'aa_seq_encoder' that takes 'data' as input.\n",
    "def aa_seq_encoder(data):\n",
    "    # Initialize an empty list 'full_sequence_list' to store encoded sequences.\n",
    "    full_sequence_list = []\n",
    "    \n",
    "    # Iterate over each sequence in the 'sequence' column of the input 'data' DataFrame.\n",
    "    for i in data[\"sequence\"].values:\n",
    "        # Initialize an empty list 'row_sequence_list' for each individual sequence.\n",
    "        row_sequence_list = []\n",
    "        \n",
    "        # Iterate over each amino acid 'j' in the current protein sequence 'i'.\n",
    "        for j in i:\n",
    "            # Look up the numerical encoding for amino acid 'j' in 'aa_seq_dict' and append it to 'row_sequence_list'. If not found, default to 0.\n",
    "            row_sequence_list.append(aa_seq_dict.get(j, 0))\n",
    "            \n",
    "        # After processing all amino acids in a sequence, append the 'row_sequence_list' (now a NumPy array) to 'full_sequence_list'.\n",
    "        full_sequence_list.append(np.array(row_sequence_list))\n",
    "    # Return the 'full_sequence_list' containing encoded amino acid sequences as NumPy arrays.\n",
    "    return full_sequence_list\n",
    "\n",
    "# Encode the 'sequence' column in X_train DataFrame using the 'aa_seq_encoder' function and assign the result to 'X_train_encode'.\n",
    "X_train_encode = aa_seq_encoder(X_train)\n",
    "\n",
    "# Encode the 'sequence' column in X_val DataFrame using the 'aa_seq_encoder' function and assign the result to 'X_val_encode'.\n",
    "X_val_encode = aa_seq_encoder(X_val)\n",
    "\n",
    "# Encode the 'sequence' column in X_test DataFrame using the 'aa_seq_encoder' function and assign the result to 'X_test_encode'.\n",
    "X_test_encode = aa_seq_encoder(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example encoded amino acid sequence\n",
    "X_train_encode[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequences are padded to a fixed length of 100 tokens/values, ensuring uniform dimensions for downstream processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum length for padding/truncating protein sequences\n",
    "# Sequences longer than this will be truncated, shorter ones will be padded\n",
    "max_length = 100\n",
    "\n",
    "# Pad/truncate the encoded training sequences:\n",
    "# - X_train_encode: Pre-encoded training sequences (likely integer-encoded amino acids)\n",
    "# - maxlen=max_length: Standardize all sequences to this length\n",
    "# - padding=\"post\": Add padding zeros AFTER the sequence content\n",
    "# - truncating=\"post\": Remove excess elements from the END if sequence exceeds max_length\n",
    "X_train_padded = pad_sequences(\n",
    "    X_train_encode, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "\n",
    "# Apply identical padding/truncation to validation sequences\n",
    "# Ensures consistent dimensions between training and validation data\n",
    "X_val_padded = pad_sequences(\n",
    "    X_val_encode, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "\n",
    "# Apply identical padding/truncation to test sequences\n",
    "# Maintains same preprocessing across all datasets\n",
    "X_test_padded = pad_sequences(\n",
    "    X_test_encode, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View original raw sequence string from training set (position 1)\n",
    "X_train.sequence.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View encoded version (before padding) of the same sequence\n",
    "# Shows integer mapping of amino acids\n",
    "X_train_encode[1][:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View final padded/truncated version of the same sequence\n",
    "# Shows how padding (zeros) were added to reach max_length\n",
    "X_train_padded[1][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert target labels into numerical values ranging from `0` to `n_classes-1`, where n_classes represents the total number of unique categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a LabelEncoder object to convert protein family labels (strings) to numerical indices\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode training labels:\n",
    "# 1. Fit the encoder on training family accessions (learns the label-to-index mapping)\n",
    "# 2. Transform the labels to numerical indices (0 to n_classes-1)\n",
    "y_train_enc = le.fit_transform(X_train[\"family_accession\"])\n",
    "\n",
    "# Encode validation labels using the SAME encoder (maintains consistent mapping)\n",
    "y_val_enc = le.transform(X_val[\"family_accession\"])\n",
    "\n",
    "# Encode test labels using the SAME encoder (maintains consistent mapping)\n",
    "y_test_enc = le.transform(X_test[\"family_accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original family accession labels from training set\n",
    "X_train[\"family_accession\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show corresponding encoded numerical labels\n",
    "y_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of unique protein families (classes)\n",
    "num_classes = len(le.classes_)\n",
    "num_classes  # Display the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numerical labels to one-hot encoded vectors for neural network training:\n",
    "# Each label becomes a binary vector of length num_classes with 1 at the class index\n",
    "y_train = to_categorical(y_train_enc)  # Training set one-hot\n",
    "y_val = to_categorical(y_val_enc)     # Validation set one-hot\n",
    "y_test = to_categorical(y_test_enc)    # Test set one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample one-hot encoded training labels for verification\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential groups a linear stack of layers into a tf.keras.Model.\n",
    "# Sequential provides training and inference features on this model.\n",
    "model = Sequential()\n",
    "\n",
    "# EmbeddingLayer: Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "#  input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "#  output_dim: Integer. Dimension of the dense embedding.\n",
    "#  input_length: Length of input sequences, when it is constant.\n",
    "model.add(Embedding(21, 16, name=\"EmbeddingLayer\"))  # max_length not needed for Bidirectional LSTM\n",
    "\n",
    "# Bidirectional wrapper for RNNs with 16 units of LSTM\n",
    "model.add(Bidirectional(LSTM(16), name=\"BidirectionalLayer\"))\n",
    "\n",
    "# Applies Dropout to the input with 20% of the input units to drop.\n",
    "model.add(Dropout(0.2, name=\"DropoutLayer\"))\n",
    "\n",
    "# densely-connected NN layer of 28 units\n",
    "model.add(Dense(28, activation=\"softmax\", name=\"DenseLayer\"))\n",
    "\n",
    "# Optimizer that implements the Adam algorithm\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)  # For TF 2.10+\n",
    "\n",
    "# Configure the neural network model for training:\n",
    "# - optimizer=opt: Uses the specified optimizer (e.g., Adam, SGD) for gradient descent\n",
    "# - loss=\"categorical_crossentropy\": Appropriate loss function for multi-class classification\n",
    "# - metrics=[\"accuracy\"]: Tracks accuracy during training/validation\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Display the model architecture summary:\n",
    "# - Shows layer-by-layer structure\n",
    "# - Displays output shapes and parameter counts\n",
    "# - Provides total trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early stopping** is a technique that halts model training when a monitored performance metric (like validation loss) stops improving, indicating that further training may lead to overfitting. This approach automatically finds the optimal stopping point by tracking the metric over epochs and terminating training when no improvement is observed for a specified number of consecutive iterations (patience). It serves as both an effective regularization method and computational efficiency measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor: Quantity to be monitored.\n",
    "# Specifies the metric to monitor for early stopping (in this case, 'val_loss' - validation loss).\n",
    "es = EarlyStopping(\n",
    "    # patience: Number of epochs with no improvement after which training will be stopped.\n",
    "    # Sets the number of epochs to wait after last time validation loss improved before stopping (patience=3).\n",
    "    patience=3,\n",
    "    \n",
    "    # restore_best_weights: Whether to restore model weights from the epoch with the best value of the monitored quantity.\n",
    "    # When set to True, restores model weights to the best epoch's weights when stopping (restore_best_weights=True).\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The following cell may take a few minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model for a fixed number of epochs (iterations on a dataset)\n",
    "# Training with fixed callbacks\n",
    "# Calls the 'fit' method of the 'model' to train the neural network.\n",
    "history = model.fit(\n",
    "    # Provides the input training data (features) as 'X_train_padded'.\n",
    "    X_train_padded,\n",
    "    \n",
    "    # Provides the target training data (labels) as 'y_train'.\n",
    "    y_train,\n",
    "    \n",
    "    # Specifies the number of training epochs (full passes through the training data) as 10.\n",
    "    epochs=10,\n",
    "    \n",
    "    # Sets the batch size for training, processing 256 samples at a time.\n",
    "    batch_size=256,\n",
    "    \n",
    "    # Provides validation data (features and labels) as a tuple for monitoring performance on a separate dataset during training.\n",
    "    validation_data=(X_val_padded, y_val),\n",
    "    \n",
    "    # Includes a list of callbacks, here using 'es' (likely an EarlyStopping callback) to control training process.\n",
    "    callbacks=[es],  # Now using properly imported callback\n",
    "    \n",
    "    # Sets verbosity to 1 to display progress bars and training information during each epoch.\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure for plotting with a size of 10x10 inches.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Define the first subplot in a 2x2 grid (top-left subplot).\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "# Set the title of the subplot to \"Accuracy\" with a fontsize of 15.\n",
    "plt.title(\"Accuracy\", fontsize=15)\n",
    "\n",
    "# Set the label for the x-axis to \"Epochs\" with a fontsize of 15.\n",
    "plt.xlabel(\"Epochs\", fontsize=15)\n",
    "\n",
    "# Set the label for the y-axis to \"Accuracy (%)\" with a fontsize of 15.\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=15)\n",
    "\n",
    "# Plot the validation accuracy from the training history, label it \"Validation Accuracy\", and use a dashed line style.\n",
    "plt.plot(\n",
    "    history.history[\"val_accuracy\"], label=\"Validation Accuracy\", linestyle=\"dashed\"\n",
    ")\n",
    "\n",
    "# Plot the training accuracy from the training history and label it \"Training Accuracy\".\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "\n",
    "# Display a legend in the lower right corner to distinguish between validation and training accuracy lines.\n",
    "plt.legend([\"Validation\", \"Training\"], loc=\"lower right\")\n",
    "\n",
    "# Define the second subplot in a 2x2 grid (top-right subplot).\n",
    "plt.subplot(2, 2, 2)\n",
    "\n",
    "# Set the title of the subplot to \"Loss\" with a fontsize of 15.\n",
    "plt.title(\"Loss\", fontsize=15)\n",
    "\n",
    "# Set the label for the x-axis to \"Epochs\" with a fontsize of 15.\n",
    "plt.xlabel(\"Epochs\", fontsize=15)\n",
    "\n",
    "# Set the label for the y-axis to \"Loss\" with a fontsize of 15.\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "\n",
    "# Plot the validation loss from the training history, label it \"Validation loss\", and use a dashed line style.\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation loss\", linestyle=\"dashed\")\n",
    "\n",
    "# Plot the training loss from the training history and label it \"Training loss\".\n",
    "plt.plot(history.history[\"loss\"], label=\"Training loss\")\n",
    "\n",
    "# Display a legend in the upper right corner to distinguish between validation and training loss lines.\n",
    "plt.legend([\"Validation\", \"Training\"], loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates output predictions for the input samples using the trained model.\n",
    "y_pred = model.predict(X_test_padded)\n",
    "\n",
    "# Build a text report showing the main classification metrics using sklearn's classification_report.\n",
    "print(\n",
    "    classification_report(\n",
    "        # Converts one-hot encoded true labels (y_test) back to class indices using argmax.\n",
    "        np.argmax(y_test, axis=1),\n",
    "        \n",
    "        # Converts probability predictions (y_pred) to class indices using argmax.\n",
    "        np.argmax(y_pred, axis=1),\n",
    "        \n",
    "        # Uses class names from the label encoder (le) to label the classes in the report.\n",
    "        target_names=le.classes_,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Support** is the number of actual occurrences of the class in the specified dataset. \n",
    "- **Macro avg** takes the arithmetic mean (aka unweighted mean). \n",
    "- **Weighted avg** takes the mean of all per-class while considering each class’s support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "A confusion matrix is a fundamental tool for evaluating the performance of a classification model by comparing its predictions against the true labels. The following code computes this matrix by first converting the one-hot encoded true labels (y_test) and predicted probability distributions (y_pred) into their respective class indices using np.argmax(axis=1). This step is necessary because the raw outputs of neural networks are typically probability vectors, while the confusion matrix requires discrete class labels. The confusion_matrix function then tabulates the predictions, with rows representing the actual classes and columns representing the predicted classes. The diagonal entries of the matrix indicate correct predictions (true positives), while the off-diagonal entries reveal misclassifications (false positives and negatives). This matrix provides a granular view of model performance, highlighting which classes are well-classified and where the model struggles, making it invaluable for diagnosing issues like class imbalance or biased predictions. Beyond mere accuracy, the confusion matrix enables the calculation of metrics like precision, recall, and F1-score, offering a comprehensive assessment of the model's strengths and weaknesses across all classes. Visualizing this matrix as a heatmap further aids interpretation, allowing quick identification of problematic class confusions and guiding targeted improvements to the model or dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the confusion matrix using true labels (y_test) and predicted labels (y_pred).\n",
    "#   - y_true: np.argmax(y_test, axis=1) -  True class labels.\n",
    "#     - y_test:  Represents the true labels, likely in one-hot encoded format (e.g., from test data).\n",
    "#     - np.argmax(y_test, axis=1): Converts one-hot encoded y_test to categorical labels by finding the index of the maximum value along axis 1 (rows).\n",
    "#   - y_pred: np.argmax(y_pred, axis=1) - Predicted class labels.\n",
    "#     - y_pred: Represents the predicted probabilities or one-hot encoded predictions from the model.\n",
    "#     - np.argmax(y_pred, axis=1): Converts probability predictions or one-hot encoded predictions to categorical labels by finding the index of the maximum probability along axis 1 (rows).\n",
    "cf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates a visual representation of a confusion matrix using a heatmap plot to facilitate model performance analysis. First, it establishes the dimensions of the plot by setting the figure size to 15 inches wide and 10 inches tall through plt.figure(figsize=(15, 10)), ensuring adequate space for clear visualization of the matrix elements. Using Seaborn's heatmap function, it then creates the heatmap visualization with several key parameters: the cf_matrix variable supplies the confusion matrix data to be plotted; annot=True enables the display of numerical values within each cell of the heatmap, allowing for precise interpretation of the counts in each category; fmt=\"\" applies default formatting to these annotations; and cmap=\"Blues\" selects a gradient of blue shades to represent the magnitude of values in the matrix, with darker blues typically indicating higher values. This visualization effectively highlights the model's classification patterns, making it easy to identify correct predictions along the diagonal and misclassifications in the off-diagonal cells, thereby providing an intuitive and informative summary of the model's performance across different classes. The combination of numerical annotations and color gradients allows for quick assessment of both the distribution and relative frequency of correct and incorrect classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size for the heatmap plot to 15 inches wide and 10 inches tall.\n",
    "plt.figure(figsize=(15, 10))\n",
    "# Create a heatmap using seaborn's heatmap function.\n",
    "#   - cf_matrix: The confusion matrix data to be visualized as a heatmap.\n",
    "#   - annot=True: Display numerical values (annotations) in each cell of the heatmap.\n",
    "#   - fmt=\"\":  Format string for the annotations (empty string means default formatting).\n",
    "#   - cmap=\"Blues\": Use the \"Blues\" colormap for the heatmap, representing values with shades of blue.\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=\"\", cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this tutorial, we learned how to:\n",
    "- Process and prepare protein sequence data for deep learning\n",
    "- Implement a deep learning model for protein family classification\n",
    "- Train and evaluate the model's performance\n",
    "- Visualize and interpret the results using various metrics\n",
    "\n",
    "The model achieved good classification performance across multiple protein families, demonstrating the effectiveness of deep learning approaches for protein sequence analysis.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (S2_T2)",
   "language": "python",
   "name": "s2_t2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
