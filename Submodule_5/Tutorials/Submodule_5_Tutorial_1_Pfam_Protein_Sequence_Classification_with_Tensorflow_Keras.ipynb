{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pfam Protein Sequence Classification with Tensorflow and Keras\n",
    "\n",
    "Adapted from Saleh Alkhalifa. [Machine Learning in Biotechnology and Life Sciences](https://github.com/PacktPublishing/Machine-Learning-in-Biotechnology-and-Life-Sciences).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to develop a protein sequence classification model using deep learning. We will classify protein sequences based on their known family accession using the Pfam dataset. The model will use TensorFlow and Keras to process amino acid sequences and predict their protein family classifications.\n",
    "\n",
    "### Pfam\n",
    "\n",
    "The Pfam dataset consists of several columns, as follows:\n",
    "\n",
    "- *Family_id*: The name of the family that the seqeunce belongs to (for example, filamin).\n",
    "- *Family Accession*: The class or output that our model will aim to predict.\n",
    "- *Sequence*: The amino acid sequence we will use as input for our model\n",
    "\n",
    "Pfam: The protein families database in 2021: J. Mistry, S. Chuguransky, L. Williams, M. Qureshi, G.A. Salazar, E.L.L. Sonnhammer, S.C.E. Tosatto, L. Paladin, S. Raj, L.J. Richardson, R.D. Finn, A. Bateman\n",
    "Nucleic Acids Research (2020) doi: 10.1093/nar/gkaa913\n",
    "\n",
    "### TensorFlow and Keras\n",
    "\n",
    "- **TensorFlow** is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community. \n",
    "- **Keras** is a deep learning API written in Python, running on top of the machine learning platform TensorFlow.\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn how to preprocess protein sequence data for deep learning\n",
    "    - Encode amino acid sequences\n",
    "    - Pad sequences to uniform length\n",
    "    - Convert labels to categorical format\n",
    "- Build and train a deep learning model for sequence classification using TensorFlow/Keras\n",
    "    - Implement embedding layers\n",
    "    - Use bidirectional LSTM architecture\n",
    "    - Apply dropout for regularization\n",
    "- Evaluate model performance using standard metrics\n",
    "    - Analyze accuracy and loss curves\n",
    "    - Interpret classification reports\n",
    "    - Visualize confusion matrices\n",
    "\n",
    "### Tasks to be completed\n",
    "\n",
    "- Download and prepare Pfam dataset\n",
    "- Preprocess protein sequences and labels\n",
    "- Build and train deep learning model\n",
    "- Evaluate model performance\n",
    "- Generate predictions and visualize results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "- Please select kernel \"conda_tensorflow2_p310\" from SageMaker notebook instance.\n",
    "- Import the necessary libraries and download the needed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyplot module from matplotlib for plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "# Import the numpy library for numerical operations, often used for array manipulations.\n",
    "import numpy as np\n",
    "# Import the pandas library for data manipulation and analysis, especially for DataFrames.\n",
    "import pandas as pd\n",
    "# Import the seaborn library for statistical data visualization, built on top of matplotlib.\n",
    "import seaborn as sns\n",
    "# Import specific layers from keras.layers for building neural networks.\n",
    "from keras.layers import (\n",
    "    LSTM, # Import LSTM layer for Long Short-Term Memory networks.\n",
    "    Bidirectional, # Import Bidirectional layer for bidirectional processing in RNNs.\n",
    "    Conv1D, # Import Conv1D layer for 1D convolutional neural networks.\n",
    "    Dense, # Import Dense layer for fully connected neural networks.\n",
    "    Dropout, # Import Dropout layer for regularization to prevent overfitting.\n",
    "    Embedding, # Import Embedding layer for creating word embeddings.\n",
    "    Flatten, # Import Flatten layer to flatten the input tensor.\n",
    "    Input, # Import Input layer to instantiate a Keras tensor.\n",
    "    MaxPooling1D, # Import MaxPooling1D layer for 1D max pooling.\n",
    ")\n",
    "# Import the Model class and Sequential class from keras.models to define neural network models.\n",
    "from keras.models import Model, Sequential\n",
    "# Import the pad_sequences function from keras.preprocessing.sequence for padding sequences to the same length.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Import the l2 regularizer from keras.regularizers for applying L2 regularization to layers.\n",
    "from keras.regularizers import l2\n",
    "# Import classification_report and confusion_matrix from sklearn.metrics for model evaluation.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Import train_test_split from sklearn.model_selection for splitting data into training and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import LabelEncoder from sklearn.preprocessing for encoding categorical labels into numerical form.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import the tensorflow library, the main deep learning framework.\n",
    "import tensorflow as tf\n",
    "# Import EarlyStopping callback from tensorflow.keras.callbacks to stop training early when validation loss stops improving.\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Import to_categorical from tensorflow.keras.utils for one-hot encoding of categorical variables.\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Import Sequential model from tensorflow.keras.models for linear stack of layers.\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Import Embedding, Bidirectional, LSTM, Dropout, and Dense layers from tensorflow.keras.layers for building neural networks.\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "\n",
    "# Set the default style for seaborn plots to \"darkgrid\" for better visualization.\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Import the os module to interact with the operating system, used here for environment variables.\n",
    "import os\n",
    "# Force CPU usage if no GPU detected\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Set environment variable to force TensorFlow to use CPU only by disabling GPU visibility.\n",
    "\n",
    "# Import the tensorflow library again (it's already imported above, this line might be redundant).\n",
    "import tensorflow as tf\n",
    "# Print the number of GPUs available to TensorFlow, useful for checking GPU setup.\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset\n",
    "\n",
    "Here, we download the data directly into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL for the dataset files hosted on GitHub.\n",
    "URL = \"https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-in-Biotechnology-and-Life-Sciences/main/datasets/dataset_pfam\"\n",
    "\n",
    "# Initialize an empty list called 'files' to store DataFrames.\n",
    "files = []\n",
    "# Loop 8 times to read in 8 different CSV files.\n",
    "for i in range(8):\n",
    "    # Read each CSV file from the specified URL pattern into a pandas DataFrame.\n",
    "    # The filename is constructed by appending 'dataset_pfam_seq_sd' and the loop index (i+1), followed by '.csv'.\n",
    "    # 'index_col=None' prevents pandas from using the first column as index.\n",
    "    # 'header=0' sets the first row as the header of the DataFrame.\n",
    "    df = pd.read_csv(f\"{URL}/dataset_pfam_seq_sd{i+1}.csv\", index_col=None, header=0)\n",
    "    # Append the DataFrame read from the CSV file to the 'files' list.\n",
    "    files.append(df)\n",
    "\n",
    "# Concatenate all DataFrames stored in the 'files' list into a single DataFrame 'df'.\n",
    "# 'axis=0' concatenates along rows (vertically).\n",
    "# 'ignore_index=True' resets the index of the resulting DataFrame to a new sequential index.\n",
    "df = pd.concat(files, axis=0, ignore_index=True)\n",
    "# Print the shape (number of rows and columns) of the concatenated DataFrame 'df'.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peek into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top 10 abundant family ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"family_id\"].groupby(df[\"family_id\"]).value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top 10 abundant family accessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"family_accession\"].groupby(df[\"family_accession\"]).value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the sequence length frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df[\"sequence\"].apply(lambda x: len(x)), bins=75, height=4, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get mean sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sequence\"].str.len().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get min sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sequence\"].str.len().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get max sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sequence\"].str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get median sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sequence\"].str.len().median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get family accessions with counts more than 1200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt = df.groupby(\"family_accession\").filter(lambda x: len(x) > 1200)\n",
    "df_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bal = df_filt.groupby(\"family_accession\").apply(lambda x: x.sample(1200))\n",
    "df_bal.family_accession.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek into the balanced dataset\n",
    "df_bal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input dataframe for modeling\n",
    "\n",
    "`reset_index` in pandas is used to reset index of the dataframe object to default indexing (0 to number of rows minus 1) or to reset multi level index. By doing so, the original index gets converted to a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red = df_bal[[\"family_accession\", \"sequence\"]].reset_index(drop=True)\n",
    "df_red.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute num of unique classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(df_red.family_accession.value_counts())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Pfam family accession unique number counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red.family_accession.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train and test datasets\n",
    "\n",
    "Split data into 75% X_train and 25% X_Test, among `X_Test`, 50% for validation (`X_val`) and 50% for test (`X_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the DataFrame 'df_red' into training set 'X_train' and a temporary test set 'X_Test', allocating 25% of the data to the test set.\n",
    "X_train, X_Test = train_test_split(df_red, test_size=0.25)\n",
    "# Splits the temporary test set 'X_Test' further into validation set 'X_val' and final test set 'X_test', allocating 50% of 'X_Test' to the final test set.\n",
    "X_val, X_test = train_test_split(X_Test, test_size=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the train, test, and validation dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create amino acid sequence dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_seq_dict = {\n",
    "    \"A\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"E\": 4,\n",
    "    \"F\": 5,\n",
    "    \"G\": 6,\n",
    "    \"H\": 7,\n",
    "    \"I\": 8,\n",
    "    \"K\": 9,\n",
    "    \"L\": 10,\n",
    "    \"M\": 11,\n",
    "    \"N\": 12,\n",
    "    \"P\": 13,\n",
    "    \"Q\": 14,\n",
    "    \"R\": 15,\n",
    "    \"S\": 16,\n",
    "    \"T\": 17,\n",
    "    \"V\": 18,\n",
    "    \"W\": 19,\n",
    "    \"Y\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode amino acid sequence using the dictionary above\n",
    "# Define a function named 'aa_seq_encoder' that takes 'data' as input.\n",
    "def aa_seq_encoder(data):\n",
    "    # Initialize an empty list 'full_sequence_list' to store encoded sequences.\n",
    "    full_sequence_list = []\n",
    "    # Iterate over each sequence in the 'sequence' column of the input 'data' DataFrame.\n",
    "    for i in data[\"sequence\"].values:\n",
    "        # Initialize an empty list 'row_sequence_list' for each individual sequence.\n",
    "        row_sequence_list = []\n",
    "        # Iterate over each amino acid 'j' in the current protein sequence 'i'.\n",
    "        for j in i:\n",
    "            # Look up the numerical encoding for amino acid 'j' in 'aa_seq_dict' and append it to 'row_sequence_list'. If not found, default to 0.\n",
    "            row_sequence_list.append(aa_seq_dict.get(j, 0))\n",
    "        # After processing all amino acids in a sequence, append the 'row_sequence_list' (now a NumPy array) to 'full_sequence_list'.\n",
    "        full_sequence_list.append(np.array(row_sequence_list))\n",
    "    # Return the 'full_sequence_list' containing encoded amino acid sequences as NumPy arrays.\n",
    "    return full_sequence_list\n",
    "\n",
    "# Encode the 'sequence' column in X_train DataFrame using the 'aa_seq_encoder' function and assign the result to 'X_train_encode'.\n",
    "X_train_encode = aa_seq_encoder(X_train)\n",
    "# Encode the 'sequence' column in X_val DataFrame using the 'aa_seq_encoder' function and assign the result to 'X_val_encode'.\n",
    "X_val_encode = aa_seq_encoder(X_val)\n",
    "# Encode the 'sequence' column in X_test DataFrame using the 'aa_seq_encoder' function and assign the result to 'X_test_encode'.\n",
    "X_test_encode = aa_seq_encoder(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example encoded amino acid sequence\n",
    "X_train_encode[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad sequence to the same length of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum length for padding sequences.\n",
    "max_length = 100\n",
    "\n",
    "# Pad the training sequences 'X_train_encode' to 'max_length', using 'post' padding and 'post' truncation.\n",
    "X_train_padded = pad_sequences(\n",
    "    X_train_encode, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "# Pad the validation sequences 'X_val_encode' to 'max_length', using 'post' padding and 'post' truncation.\n",
    "X_val_padded = pad_sequences(\n",
    "    X_val_encode, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "# Pad the test sequences 'X_test_encode' to 'max_length', using 'post' padding and 'post' truncation.\n",
    "X_test_padded = pad_sequences(\n",
    "    X_test_encode, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.sequence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encode[1][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded[1][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode target labels with value between `0` and `n_classes-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a LabelEncoder object to convert categorical labels to numerical values.\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder on the 'family_accession' column of the training data (X_train) and transform it.\n",
    "y_train_enc = le.fit_transform(X_train[\"family_accession\"])\n",
    "# Transform the 'family_accession' column of the validation data (X_val) using the fitted LabelEncoder.\n",
    "y_val_enc = le.transform(X_val[\"family_accession\"])\n",
    "# Transform the 'family_accession' column of the test data (X_test) using the fitted LabelEncoder.\n",
    "y_test_enc = le.transform(X_test[\"family_accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"family_accession\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(le.classes_)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the training class labels (y_train_enc) into a binary class matrix using one-hot encoding.\n",
    "y_train = to_categorical(y_train_enc)\n",
    "# Converts the validation class labels (y_val_enc) into a binary class matrix using one-hot encoding.\n",
    "y_val = to_categorical(y_val_enc)\n",
    "# Converts the test class labels (y_test_enc) into a binary class matrix using one-hot encoding.\n",
    "y_test = to_categorical(y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential groups a linear stack of layers into a tf.keras.Model.\n",
    "# Sequential provides training and inference features on this model.\n",
    "model = Sequential()\n",
    "\n",
    "# EmbeddingLayer: Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "#  input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "#  output_dim: Integer. Dimension of the dense embedding.\n",
    "#  input_length: Length of input sequences, when it is constant.\n",
    "model.add(Embedding(21, 16, name=\"EmbeddingLayer\"))  # max_length not needed for Bidirectional LSTM\n",
    "\n",
    "# Bidirectional wrapper for RNNs with 16 units of LSTM\n",
    "model.add(Bidirectional(LSTM(16), name=\"BidirectionalLayer\"))\n",
    "\n",
    "# Applies Dropout to the input with 20% of the input units to drop.\n",
    "model.add(Dropout(0.2, name=\"DropoutLayer\"))\n",
    "\n",
    "# densely-connected NN layer of 28 units\n",
    "model.add(Dense(28, activation=\"softmax\", name=\"DenseLayer\"))\n",
    "\n",
    "# Optimizer that implements the Adam algorithm\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)  # For TF 2.10+\n",
    "\n",
    "# Configures the model for training use 'Adam' as optimizer, 'categorical_crossentropy'\n",
    "# as loss funciton, and 'accuracy' as evaluation metrics\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop training when a monitored metric has stopped improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor: Quantity to be monitored.\n",
    "# Specifies the metric to monitor for early stopping (in this case, 'val_loss' - validation loss).\n",
    "es = EarlyStopping(\n",
    "    # patience: Number of epochs with no improvement after which training will be stopped.\n",
    "    # Sets the number of epochs to wait after last time validation loss improved before stopping (patience=3).\n",
    "    patience=3,\n",
    "    # restore_best_weights: Whether to restore model weights from the epoch with the best value of the monitored quantity.\n",
    "    # When set to True, restores model weights to the best epoch's weights when stopping (restore_best_weights=True).\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The following cell may takeu a few minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model for a fixed number of epochs (iterations on a dataset)\n",
    "# Training with fixed callbacks\n",
    "# Calls the 'fit' method of the 'model' to train the neural network.\n",
    "history = model.fit(\n",
    "    # Provides the input training data (features) as 'X_train_padded'.\n",
    "    X_train_padded,\n",
    "    # Provides the target training data (labels) as 'y_train'.\n",
    "    y_train,\n",
    "    # Specifies the number of training epochs (full passes through the training data) as 10.\n",
    "    epochs=10,\n",
    "    # Sets the batch size for training, processing 256 samples at a time.\n",
    "    batch_size=256,\n",
    "    # Provides validation data (features and labels) as a tuple for monitoring performance on a separate dataset during training.\n",
    "    validation_data=(X_val_padded, y_val),\n",
    "    # Includes a list of callbacks, here using 'es' (likely an EarlyStopping callback) to control training process.\n",
    "    callbacks=[es],  # Now using properly imported callback\n",
    "    # Sets verbosity to 1 to display progress bars and training information during each epoch.\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure for plotting with a size of 10x10 inches.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Define the first subplot in a 2x2 grid (top-left subplot).\n",
    "plt.subplot(2, 2, 1)\n",
    "# Set the title of the subplot to \"Accuracy\" with a fontsize of 15.\n",
    "plt.title(\"Accuracy\", fontsize=15)\n",
    "# Set the label for the x-axis to \"Epochs\" with a fontsize of 15.\n",
    "plt.xlabel(\"Epochs\", fontsize=15)\n",
    "# Set the label for the y-axis to \"Accuracy (%)\" with a fontsize of 15.\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=15)\n",
    "# Plot the validation accuracy from the training history, label it \"Validation Accuracy\", and use a dashed line style.\n",
    "plt.plot(\n",
    "    history.history[\"val_accuracy\"], label=\"Validation Accuracy\", linestyle=\"dashed\"\n",
    ")\n",
    "# Plot the training accuracy from the training history and label it \"Training Accuracy\".\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "# Display a legend in the lower right corner to distinguish between validation and training accuracy lines.\n",
    "plt.legend([\"Validation\", \"Training\"], loc=\"lower right\")\n",
    "\n",
    "# Define the second subplot in a 2x2 grid (top-right subplot).\n",
    "plt.subplot(2, 2, 2)\n",
    "# Set the title of the subplot to \"Loss\" with a fontsize of 15.\n",
    "plt.title(\"Loss\", fontsize=15)\n",
    "# Set the label for the x-axis to \"Epochs\" with a fontsize of 15.\n",
    "plt.xlabel(\"Epochs\", fontsize=15)\n",
    "# Set the label for the y-axis to \"Loss\" with a fontsize of 15.\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "# Plot the validation loss from the training history, label it \"Validation loss\", and use a dashed line style.\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation loss\", linestyle=\"dashed\")\n",
    "# Plot the training loss from the training history and label it \"Training loss\".\n",
    "plt.plot(history.history[\"loss\"], label=\"Training loss\")\n",
    "# Display a legend in the upper right corner to distinguish between validation and training loss lines.\n",
    "plt.legend([\"Validation\", \"Training\"], loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates output predictions for the input samples using the trained model.\n",
    "y_pred = model.predict(X_test_padded)\n",
    "\n",
    "# Build a text report showing the main classification metrics using sklearn's classification_report.\n",
    "print(\n",
    "    classification_report(\n",
    "        # Converts one-hot encoded true labels (y_test) back to class indices using argmax.\n",
    "        np.argmax(y_test, axis=1),\n",
    "        # Converts probability predictions (y_pred) to class indices using argmax.\n",
    "        np.argmax(y_pred, axis=1),\n",
    "        # Uses class names from the label encoder (le) to label the classes in the report.\n",
    "        target_names=le.classes_,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Support** is the number of actual occurrences of the class in the specified dataset. \n",
    "- **Macro avg** takes the arithmetic mean (aka unweighted mean). \n",
    "- **Weighted avg** takes the mean of all per-class while considering each class’s support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "Compute confusion matrix to evaluate the accuracy of a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the confusion matrix using true labels (y_test) and predicted labels (y_pred).\n",
    "#   - y_true: np.argmax(y_test, axis=1) -  True class labels.\n",
    "#     - y_test:  Represents the true labels, likely in one-hot encoded format (e.g., from test data).\n",
    "#     - np.argmax(y_test, axis=1): Converts one-hot encoded y_test to categorical labels by finding the index of the maximum value along axis 1 (rows).\n",
    "#   - y_pred: np.argmax(y_pred, axis=1) - Predicted class labels.\n",
    "#     - y_pred: Represents the predicted probabilities or one-hot encoded predictions from the model.\n",
    "#     - np.argmax(y_pred, axis=1): Converts probability predictions or one-hot encoded predictions to categorical labels by finding the index of the maximum probability along axis 1 (rows).\n",
    "cf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size for the heatmap plot to 15 inches wide and 10 inches tall.\n",
    "plt.figure(figsize=(15, 10))\n",
    "# Create a heatmap using seaborn's heatmap function.\n",
    "#   - cf_matrix: The confusion matrix data to be visualized as a heatmap.\n",
    "#   - annot=True: Display numerical values (annotations) in each cell of the heatmap.\n",
    "#   - fmt=\"\":  Format string for the annotations (empty string means default formatting).\n",
    "#   - cmap=\"Blues\": Use the \"Blues\" colormap for the heatmap, representing values with shades of blue.\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=\"\", cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this tutorial, we learned how to:\n",
    "- Process and prepare protein sequence data for deep learning\n",
    "- Implement a deep learning model for protein family classification\n",
    "- Train and evaluate the model's performance\n",
    "- Visualize and interpret the results using various metrics\n",
    "\n",
    "The model achieved good classification performance across multiple protein families, demonstrating the effectiveness of deep learning approaches for protein sequence analysis.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
