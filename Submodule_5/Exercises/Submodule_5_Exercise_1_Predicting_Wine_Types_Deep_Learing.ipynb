{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Types Analysis Using Deep Learning Exercise Solution\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module focuses on building predictive models to predict wine types (red or white wine) based on various features using Deep Learning techniques. The analysis uses a dataset containing wine quality measurements and characteristics.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Build and train a Deep Neural Network model for wine type classification\n",
    "- Understand feature scaling and preprocessing techniques\n",
    "- Evaluate model performance using various metrics\n",
    "- Visualize model training progress and results\n",
    "- Apply binary classification techniques using deep learning\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Build and compile a Deep Neural Network model\n",
    "- Train the model using scaled features\n",
    "- Evaluate model performance on test data\n",
    "- Generate performance visualizations and metrics\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python programming environment\n",
    "- Basic understanding of statistical and machine learning concepts\n",
    "- Familiarity with common ML libraries\n",
    "\n",
    "## Get Started\n",
    "\n",
    "- Please select kernel \"conda_tensorflow2_p310\" from SageMaker notebook instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from numpy import interp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and merge datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "red_wine = pd.read_csv(\"../../Data/winequality-red.csv\", sep=\";\")\n",
    "white_wine = pd.read_csv(\"../../Data/winequality-white.csv\", sep=\";\")\n",
    "\n",
    "# Add wine type and quality labels\n",
    "red_wine[\"wine_type\"] = \"red\"\n",
    "white_wine[\"wine_type\"] = \"white\"\n",
    "\n",
    "def categorize_quality(value):\n",
    "    \"\"\"Categorize wine quality into low, medium, or high.\"\"\"\n",
    "    if value <= 5:\n",
    "        return \"low\"\n",
    "    elif value <= 7:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "red_wine[\"quality_label\"] = red_wine[\"quality\"].apply(categorize_quality)\n",
    "white_wine[\"quality_label\"] = white_wine[\"quality\"].apply(categorize_quality)\n",
    "\n",
    "# Merge datasets and shuffle\n",
    "wines = pd.concat([red_wine, white_wine]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display dataset info\n",
    "print(\"Dataset Info:\")\n",
    "print(wines.info())\n",
    "print(\"\\nSample Data:\")\n",
    "print(wines.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand dataset features and values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(white_wine.shape, red_wine.shape)\n",
    "print(wines.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4898 white wine data points and 1599 red wine data points. The\n",
    "merged dataset contains a total of 6497 data points and we also get an idea of numeric and categorical\n",
    "attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letâ€™s take a peek at our dataset to see some sample data points.\n",
    "wines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilty functions for model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for model evaluation\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculate and print performance metrics.\"\"\"\n",
    "    print(\"Accuracy:\", np.round(metrics.accuracy_score(true_labels, predicted_labels), 4))\n",
    "    print(\"Precision:\", np.round(metrics.precision_score(true_labels, predicted_labels, average=\"weighted\"), 4))\n",
    "    print(\"Recall:\", np.round(metrics.recall_score(true_labels, predicted_labels, average=\"weighted\"), 4))\n",
    "    print(\"F1 Score:\", np.round(metrics.f1_score(true_labels, predicted_labels, average=\"weighted\"), 4))\n",
    "\n",
    "def display_classification_report(true_labels, predicted_labels, classes):\n",
    "    \"\"\"Display classification report.\"\"\"\n",
    "    print(metrics.classification_report(true_labels, predicted_labels, labels=classes))\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes):\n",
    "    \"\"\"Display confusion matrix.\"\"\"\n",
    "    cm = metrics.confusion_matrix(true_labels, predicted_labels, labels=classes)\n",
    "    cm_frame = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_frame)\n",
    "\n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes):\n",
    "    \"\"\"Display model performance metrics.\"\"\"\n",
    "    print(\"Model Performance Metrics:\")\n",
    "    print(\"-\" * 30)\n",
    "    get_metrics(true_labels, predicted_labels)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"-\" * 30)\n",
    "    display_classification_report(true_labels, predicted_labels, classes)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"-\" * 30)\n",
    "    display_confusion_matrix(true_labels, predicted_labels, classes)\n",
    "\n",
    "def plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n",
    "    \"\"\"Plot ROC curve for the model.\"\"\"\n",
    "    if hasattr(clf, \"classes_\"):\n",
    "        class_labels = clf.classes_\n",
    "    elif label_encoder:\n",
    "        class_labels = label_encoder.classes_\n",
    "    elif class_names:\n",
    "        class_labels = class_names\n",
    "    else:\n",
    "        raise ValueError(\"Unable to derive prediction classes!\")\n",
    "\n",
    "    n_classes = len(class_labels)\n",
    "    y_test = label_binarize(true_labels, classes=class_labels)\n",
    "\n",
    "    if n_classes == 2:\n",
    "        y_score = clf.predict_proba(features)[:, 1] if hasattr(clf, \"predict_proba\") else clf.decision_function(features)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.2f})\", linewidth=2.5)\n",
    "    else:\n",
    "        y_score = clf.predict_proba(features) if hasattr(clf, \"predict_proba\") else clf.decision_function(features)\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f\"ROC curve of class {class_labels[i]} (area = {roc_auc:.2f})\", linestyle=\":\", linewidth=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Wine Types\n",
    "\n",
    "We will predict the wine type based on other features. To start with, we\n",
    "will first select our necessary features and separate out the prediction class labels and prepare train and test\n",
    "datasets. We use the prefix **wtp\\_** in our variables to easily identify them as needed, where **wtp** depicts wine\n",
    "type prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp_features = wines.iloc[:, :-3]\n",
    "wtp_feature_names = wtp_features.columns\n",
    "wtp_class_labels = np.array(wines[\"wine_type\"])\n",
    "\n",
    "# prepare train and test datasets\n",
    "wtp_train_X, wtp_test_X, wtp_train_y, wtp_test_y = train_test_split(\n",
    "    wtp_features, wtp_class_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(Counter(wtp_train_y), Counter(wtp_test_y))\n",
    "print(\"Features:\", list(wtp_feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers show us the wine samples for each class and we can also see the feature names which will\n",
    "be used in our feature set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "We will be using a standard scaler in this scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler().fit(wtp_train_X)\n",
    "wtp_train_SX = scaler.transform(wtp_train_X)\n",
    "wtp_test_SX = scaler.transform(wtp_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model using Deep Learning (MLP)\n",
    "\n",
    "Letâ€™s try\n",
    "modeling the data using a fully connected deep neural network (DNN) with three hidden layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encodes wine type class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "wtp_train_ey = label_encoder.fit_transform(wtp_train_y)\n",
    "wtp_test_ey = label_encoder.transform(wtp_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wtp_train_SX.shape)\n",
    "print(wtp_train_ey.shape)\n",
    "print(wtp_train_SX.nbytes / 1024**2)  # Size in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build & Compile DNN Model Architecture\n",
    "\n",
    "Letâ€™s build the architecture for our three-hidden layer DNN where each hidden layer has 16 units (the\n",
    "input layer has 11 units for the 11 features) and the output layer has 1 unit to predict a 0 or 1, which maps\n",
    "back to red or white wine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sequential model of three-hidden layer DNN where each hidden layer has 16 units and 'relu' activation\n",
    "# (the input layer has 11 units for the 11 features) and the output layer has 1 unit and 'sigmoid' activation\n",
    "# Build and train the DNN model\n",
    "\n",
    "# Your code goes here\n",
    "\n",
    "# Configures the model for training use 'Adam' as optimizer, 'binary_crossentropy' \n",
    "# as loss funciton, and 'accuracy' as evaluation metrics\n",
    "\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model\n",
    "\n",
    "We use 10% of the training data for a validation set while training the model to see how it performs at\n",
    "each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = wtp_dnn_model.fit(wtp_train_SX, wtp_train_ey, epochs=10, batch_size=5, shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on test data\n",
    "\n",
    "Letâ€™s now predict and evaluate our model on the actual test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp_dnn_ypred = wtp_dnn_model.predict(wtp_test_SX)\n",
    "wtp_dnn_ypred = (wtp_dnn_ypred > 0.5).astype(int)  # More efficient thresholding\n",
    "wtp_dnn_predictions = le.inverse_transform(np.ravel(wtp_dnn_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_model_performance_metrics(\n",
    "    true_labels=wtp_test_y,\n",
    "    predicted_labels=wtp_dnn_predictions,\n",
    "    classes=[\"red\", \"white\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an overall F1 Score and model accuracy of 99.5%, which is even\n",
    "better than our previous model! This goes to prove you donâ€™t always need big data but good quality data\n",
    "and features even for Deep Learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accurcy measures at each epoch\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "t = f.suptitle(\"Deep Neural Net Performance\", fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "epochs = list(range(1, 11))\n",
    "ax1.plot(epochs, history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "ax1.plot(epochs, history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "ax1.set_xticks(epochs)\n",
    "ax1.set_ylabel(\"Accuracy Value\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_title(\"Accuracy\")\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epochs, history.history[\"loss\"], label=\"Train Loss\")\n",
    "ax2.plot(epochs, history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "ax2.set_xticks(epochs)\n",
    "ax2.set_ylabel(\"Loss Value\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_title(\"Loss\")\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this module, we successfully implemented a complete deep learning workflow including:\n",
    "\n",
    "- Data preprocessing and feature scaling techniques\n",
    "- Building a three-layer Deep Neural Network architecture\n",
    "- Training the model with validation monitoring\n",
    "- Implementing comprehensive model evaluation metrics\n",
    "- Creating performance visualizations to track model training\n",
    "- Applying binary classification techniques to real-world data\n",
    "\n",
    "These practical skills demonstrate the end-to-end process of developing and deploying deep learning models for classification tasks.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
