{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65fc0a41-058d-481a-99c9-ae0ad2eea34b",
   "metadata": {},
   "source": [
    "# **Diabetes Prediction using Deep Neural Network (DNN)** (Solution)\n",
    "\n",
    "Inspired by Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## **Overview**\n",
    "This notebook demonstrates how to build a **deep neural network (DNN)** model using **TensorFlow and Keras** to predict whether a patient has diabetes. We will use the **Pima Indians Diabetes Dataset** and go through the process of **data loading, preprocessing, model creation, training, and evaluation**.  This notebook provides a practical example of applying deep learning techniques to a binary classification problem in healthcare.\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "By the end of this notebook, you will be able to:\n",
    "- Load and explore the **Pima Indians Diabetes Dataset**.\n",
    "- Preprocess data for neural network training, including **feature scaling**.\n",
    "- Build a **sequential deep neural network model** using **TensorFlow/Keras**.\n",
    "- Train the DNN model to predict diabetes using the dataset.\n",
    "- Evaluate the model's performance using **accuracy, precision, recall, F1-score, and confusion matrix**.\n",
    "- Visualize model training progress using **accuracy and loss curves**.\n",
    "- Make **predictions on new sample data** using the trained DNN model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Tasks to Complete**\n",
    "1. **Load and Explore the Dataset:**\n",
    "    - Read the Pima Indians Diabetes Dataset from a CSV file.\n",
    "    - Display dataset information and sample data to understand its structure and features.\n",
    "\n",
    "2. **Preprocess the Data:**\n",
    "    - Prepare features and labels for model training.\n",
    "    - Split the dataset into **training and testing sets**.\n",
    "    - **Standardize numerical features** using `StandardScaler`.\n",
    "\n",
    "3. **Build and Train a Deep Neural Network (DNN) Model:**\n",
    "    - Create a **Sequential model** using TensorFlow/Keras with multiple dense layers and ReLU activation, and a sigmoid output layer for binary classification.\n",
    "    - **Compile the model** with binary cross-entropy loss, Adam optimizer, and accuracy metric.\n",
    "    - **Train the DNN model** using the training data, including validation during training.\n",
    "\n",
    "4. **Evaluate Model Performance:**\n",
    "    - Make predictions on the test set.\n",
    "    - Evaluate and display model performance metrics: **accuracy, precision, recall, F1-score, classification report, and confusion matrix**.\n",
    "    - Plot **training accuracy and loss curves** to visualize training progress.\n",
    "\n",
    "5. **Make Predictions on New Samples:**\n",
    "    - Create new sample patient data.\n",
    "    - Preprocess the new sample data using the **same scaler** fitted on the training data.\n",
    "    - Use the trained DNN model to **predict diabetes risk probabilities** for the new samples.\n",
    "    - Display the predicted outcomes and probabilities for each sample.\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "Before running this notebook, ensure you have the following:\n",
    "- **Python 3.6+**\n",
    "- **Required Python libraries installed:**\n",
    "    ```bash\n",
    "    pip install numpy pandas matplotlib scikit-learn tensorflow\n",
    "    ```\n",
    "    *(Specifically, ensure you have TensorFlow and Keras installed)*\n",
    "\n",
    "---\n",
    "\n",
    "## **Get Started**\n",
    "- **Launch an AWS SageMaker Notebook Instance:** Follow the instructions in the \"Setup an AWS SageMaker Notebook Instance\" section to create and configure your instance, ensuring you select a GPU-enabled instance type. (e.g., g4dn.xlarge for CUDA support, 50GB storage).\n",
    "- Please select kernel \"conda_tensorflow2_p310\" from SageMaker notebook instance.\n",
    "- Execute the code cells in the notebook sequentially to follow the steps of building, training, and evaluating the diabetes prediction DNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc03ea7b-5245-4323-9443-7e1872f15fb0",
   "metadata": {},
   "source": [
    "### Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31872426-87ed-4f35-95b2-808be6d65f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NumPy library for numerical operations in Python.\n",
    "import numpy as np\n",
    "\n",
    "# Import the Pandas library for data manipulation and analysis using DataFrames.\n",
    "import pandas as pd\n",
    "\n",
    "# Import the Pyplot module from Matplotlib library for creating static, interactive, and animated visualizations in Python.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the 'read_csv' function from the Pandas library to read comma-separated values (CSV) files into DataFrame.\n",
    "from pandas import read_csv\n",
    "\n",
    "# Import the 'Counter' class from the 'collections' module for counting hashable objects.\n",
    "from collections import Counter\n",
    "\n",
    "# Import the 'metrics' module from scikit-learn library for model evaluation metrics.\n",
    "from sklearn import metrics\n",
    "\n",
    "# Import the 'train_test_split' function from scikit-learn to split data into training and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import specific modules from scikit-learn for preprocessing data: LabelEncoder, StandardScaler, and label_binarize.\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
    "\n",
    "# Import the 'Sequential' model from TensorFlow Keras to build neural network models layer by layer.\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import the 'Dense' layer from TensorFlow Keras to create densely connected neural network layers.\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Import the 'logging' module for configuring logging levels (e.g., suppressing TensorFlow warnings)\n",
    "import logging\n",
    "\n",
    "# Import TensorFlow, a deep learning framework\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import the 'random' module for generating random numbers (used for reproducibility)\n",
    "import random\n",
    "\n",
    "# Set TensorFlow's logger to only display ERROR-level messages (suppresses INFO/WARNING logs)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# Import the 'warnings' module to handle Python warnings\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings (e.g., deprecation warnings, NumPy/TensorFlow alerts)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable matplotlib to display plots inline within the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d485582-1fc4-4ec9-8c79-0552cbe49f49",
   "metadata": {},
   "source": [
    "# Pima Indians Diabetes Dataset\n",
    "\n",
    "## Overview\n",
    "The **Pima Indians Diabetes Dataset** is a well-known dataset used for binary classification tasks in machine learning, specifically for predicting whether a patient has diabetes based on various medical attributes. The dataset originates from the **National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK)** and focuses on female patients of **Pima Indian heritage**.\n",
    "\n",
    "## Source\n",
    "- **Dataset Repository:** [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/12/pima+indians+diabetes)\n",
    "- **Original Source:** National Institute of Diabetes and Digestive and Kidney Diseases\n",
    "- **Purpose:** Predicting the onset of diabetes based on diagnostic measurements.\n",
    "\n",
    "## Dataset Description\n",
    "The dataset contains **768 samples** with **8 numerical features** and **1 binary target variable** (diabetes outcome).\n",
    "\n",
    "### **Features:**\n",
    "1. **Pregnancies** – Number of times pregnant  \n",
    "2. **Glucose** – Plasma glucose concentration over 2 hours in an oral glucose tolerance test  \n",
    "3. **BloodPressure** – Diastolic blood pressure (mm Hg)  \n",
    "4. **SkinThickness** – Triceps skinfold thickness (mm)  \n",
    "5. **Insulin** – 2-Hour serum insulin (mu U/ml)  \n",
    "6. **BMI** – Body mass index (weight in kg/(height in m²))  \n",
    "7. **DiabetesPedigreeFunction** – Diabetes pedigree function (genetic influence)  \n",
    "8. **Age** – Age of the patient (years)  \n",
    "9. **Outcome** – Binary classification (1 = Diabetic, 0 = Non-Diabetic)  \n",
    "\n",
    "## Summary Statistics\n",
    "- **Total samples:** 768  \n",
    "- **Diabetes positive cases (Outcome = 1):** ~35%  \n",
    "- **Diabetes negative cases (Outcome = 0):** ~65%  \n",
    "- **Missing values:** Some attributes contain zero values which may indicate missing data (e.g., Glucose, BloodPressure).\n",
    "\n",
    "## Example Usage\n",
    "This dataset is frequently used in **machine learning** and **statistical modeling** for:\n",
    "- Logistic Regression\n",
    "- Decision Trees & Random Forests\n",
    "- Support Vector Machines (SVM)\n",
    "- Deep Learning\n",
    "- Feature Engineering and Imputation Techniques\n",
    "\n",
    "## References\n",
    "- UCI Machine Learning Repository: [Pima Indians Diabetes Dataset](https://archive.ics.uci.edu/dataset/12/pima+indians+diabetes)\n",
    "- Smith, J. W., et al. \"Using the ADAP learning algorithm to forecast the onset of diabetes mellitus.\" In Proceedings of the Annual Symposium on Computer Application in Medical Care. American Medical Informatics Association, 1988."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de2540c-df33-4c3a-8605-48908eaeae3b",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2932a8c-d11f-4535-8c52-ebca643569b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the pima-indians-diabetes dataset CSV file.\n",
    "diabetes_data = \"../../Data/pima-indians-diabetes.csv\"\n",
    "\n",
    "# Define a list of column names for the dataset, based on the dataset description.\n",
    "columns = [\n",
    "    'Pregnancies', # Column name for number of pregnancies.\n",
    "    'Glucose', # Column name for plasma glucose concentration.\n",
    "    'BloodPressure', # Column name for diastolic blood pressure.\n",
    "    'SkinThickness', # Column name for triceps skin fold thickness.\n",
    "    'Insulin', # Column name for 2-hour serum insulin.\n",
    "    'BMI', # Column name for body mass index.\n",
    "    'DiabetesPedigreeFunction', # Column name for diabetes pedigree function.\n",
    "    'Age', # Column name for age.\n",
    "    'Outcome' # Column name for class variable (diabetes outcome).\n",
    "]\n",
    "\n",
    "# Load the dataset from the CSV file using pandas read_csv function.\n",
    "df = read_csv(\n",
    "    diabetes_data, # Path to the CSV file.\n",
    "    header=None, # Indicate that the CSV file has no header row.\n",
    "    names=columns, # Assign the defined column names to the DataFrame.\n",
    "    na_values=\"?\", # Treat question marks '?' in the data as Not a Number (NaN) values.\n",
    "    sep=',' # Specify that the data is separated by commas.\n",
    ")\n",
    "\n",
    "# Print information about the loaded dataset.\n",
    "print(\"Dataset Info:\")\n",
    "\n",
    "# Print a concise summary of the DataFrame, including data types and non-null values.\n",
    "print(df.info())\n",
    "\n",
    "# Print a newline for better readability.\n",
    "print(\"\\nSample Data:\")\n",
    "\n",
    "# Display the first few rows of the DataFrame to get a glimpse of the data.\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83b77b-36f5-4c20-93e9-c661398204de",
   "metadata": {},
   "source": [
    "## Domain Knowledge\n",
    "\n",
    "### Key Health Indicators\n",
    "\n",
    "- **Pregnancies:** Number of times pregnant\n",
    "- **Glucose:** Plasma glucose concentration (mg/dL)\n",
    "- **BloodPressure:** Diastolic blood pressure (mm Hg)\n",
    "- **SkinThickness:** Triceps skin fold thickness (mm)\n",
    "- **Insulin:** 2-Hour serum insulin (mu U/ml)\n",
    "- **BMI:** Body mass index (kg/m²)\n",
    "- **DiabetesPedigreeFunction:** Diabetes risk genetic score\n",
    "- **Age:** Years\n",
    "- **Outcome:** Diabetes diagnosis (0 = Negative, 1 = Positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3935acc8-763e-412a-891c-67a462674946",
   "metadata": {},
   "source": [
    "### Define helper functions for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e8ca8-cd79-4b26-80c2-84d8d2ec5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variability in accuracy, F1-score, and ROC-AUC scores when running a Deep Neural Network (DNN) \n",
    "# multiple times can be attributed to several factors related to the randomness and initialization \n",
    "# in the training process\n",
    "\n",
    "# Fix seeds for reproducibility \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Disable GPU Non-Determinism\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# Define a function called 'get_metrics' that takes true labels and predicted labels as input.\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculate and print performance metrics.\"\"\"\n",
    "    # Print the Accuracy score, rounded to 4 decimal places, by comparing true labels and predicted labels.\n",
    "    print(\"Accuracy:\", np.round(metrics.accuracy_score(true_labels, predicted_labels), 4))\n",
    "    \n",
    "    # Print the Precision score, rounded to 4 decimal places, using weighted averaging for multi-class if needed.\n",
    "    print(\"Precision:\", np.round(metrics.precision_score(true_labels, predicted_labels, average=\"weighted\"), 4))\n",
    "    \n",
    "    # Print the Recall score, rounded to 4 decimal places, using weighted averaging for multi-class if needed.\n",
    "    print(\"Recall:\", np.round(metrics.recall_score(true_labels, predicted_labels, average=\"weighted\"), 4))\n",
    "    \n",
    "    # Print the F1 Score, rounded to 4 decimal places, using weighted averaging for multi-class if needed.\n",
    "    print(\"F1 Score:\", np.round(metrics.f1_score(true_labels, predicted_labels, average=\"weighted\"), 4))\n",
    "\n",
    "# Define a function called 'display_classification_report' that takes true labels, predicted labels, and class names as input.\n",
    "def display_classification_report(true_labels, predicted_labels, classes):\n",
    "    \"\"\"Display classification report.\"\"\" # Docstring for the function explaining its purpose.\n",
    "    # Prints the classification report using scikit-learn's metrics.classification_report function, using provided true labels, predicted labels, and class labels.\n",
    "    print(metrics.classification_report(true_labels, predicted_labels, labels=classes)) \n",
    "\n",
    "# Defines a function called 'display_confusion_matrix' that takes true labels, predicted labels, and class names as input.\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes):\n",
    "    \"\"\"Display confusion matrix.\"\"\"\n",
    "    # Calculates the confusion matrix using scikit-learn's metrics.confusion_matrix function.\n",
    "    cm = metrics.confusion_matrix(true_labels, predicted_labels, labels=classes)\n",
    "    \n",
    "    # Creates a Pandas DataFrame from the confusion matrix 'cm', using class names for index and columns.\n",
    "    cm_frame = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    \n",
    "    # Prints the title \"Confusion Matrix:\" to the console.\n",
    "    print(\"Confusion Matrix:\")\n",
    "    \n",
    "    # Prints the Pandas DataFrame 'cm_frame' which represents the confusion matrix.\n",
    "    print(cm_frame)\n",
    "\n",
    "# Define function to display comprehensive model performance metrics\n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes):\n",
    "    \"\"\"Display model performance metrics.\"\"\"\n",
    "    # Prints a header indicating the start of model performance metrics display.\n",
    "    print(\"Model Performance Metrics:\")\n",
    "    \n",
    "    # Prints a separator line for visual clarity.\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Calls the 'get_metrics' function to calculate and print accuracy, precision, recall, and F1 score.\n",
    "    get_metrics(true_labels, predicted_labels)\n",
    "    \n",
    "    # Prints a header indicating the start of the classification report.\n",
    "    print(\"\\nClassification Report:\")\n",
    "    \n",
    "    # Prints a separator line for visual clarity.\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Calls the 'display_classification_report' function to print the classification report.\n",
    "    display_classification_report(true_labels, predicted_labels, classes)\n",
    "    \n",
    "    # Prints a header indicating the start of the confusion matrix display.\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    \n",
    "    # Prints a separator line for visual clarity.\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Calls the 'display_confusion_matrix' function to print the confusion matrix.\n",
    "    display_confusion_matrix(true_labels, predicted_labels, classes)\n",
    "\n",
    "# Define a function to plot the Receiver Operating Characteristic (ROC) curve for a classification model.\n",
    "def plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n",
    "    \"\"\"Plot ROC curve for the model.\"\"\"\n",
    "    # Determine class labels from classifier, label encoder, or provided class names.\n",
    "    # Check if the classifier object 'clf' has attribute 'classes_' (scikit-learn classifiers).\n",
    "    if hasattr(clf, \"classes_\"): \n",
    "        # Get class labels from the classifier object.\n",
    "        class_labels = clf.classes_ \n",
    "        \n",
    "    # If 'label_encoder' is provided.    \n",
    "    elif label_encoder: \n",
    "        # Get class labels from the label encoder.\n",
    "        class_labels = label_encoder.classes_ \n",
    "\n",
    "    # If 'class_names' are directly provided.\n",
    "    elif class_names: \n",
    "        # Use the provided class names.\n",
    "        class_labels = class_names \n",
    "\n",
    "    # If class labels cannot be determined from any source.\n",
    "    else: \n",
    "        # Raise a ValueError exception indicating inability to get class labels.\n",
    "        raise ValueError(\"Unable to derive prediction classes!\") \n",
    "\n",
    "    # Get the number of classes.\n",
    "    n_classes = len(class_labels) \n",
    "    \n",
    "    # Binarize true labels for ROC curve calculation, handling multi-class if necessary.\n",
    "    y_test = label_binarize(true_labels, classes=class_labels) \n",
    "\n",
    "    # For binary classification (2 classes).\n",
    "    if n_classes == 2:\n",
    "        # Get probability scores for the positive class (index 1) if 'predict_proba' is available, otherwise use 'decision_function'.\n",
    "        y_score = clf.predict_proba(features)[:, 1] if hasattr(clf, \"predict_proba\") else clf.decision_function(features)\n",
    "\n",
    "        # Calculate False Positive Rate, True Positive Rate, and thresholds for ROC curve.\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score) \n",
    "\n",
    "        # Calculate Area Under the ROC Curve (AUC).\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Plot ROC curve for binary class with AUC value in label.\n",
    "        plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.2f})\", linewidth=2.5) \n",
    "    \n",
    "    # For multi-class classification (more than 2 classes).\n",
    "    else:\n",
    "        # Get probability scores for all classes.\n",
    "        y_score = clf.predict_proba(features) if hasattr(clf, \"predict_proba\") else clf.decision_function(features) \n",
    "\n",
    "        # Iterate through each class.\n",
    "        for i in range(n_classes): \n",
    "            # Calculate FPR, TPR for each class vs. rest.\n",
    "            fpr, tpr, _ = roc_curve(y_test[:, i], y_score[:, i]) \n",
    "\n",
    "            # Calculate AUC for each class.\n",
    "            roc_auc = auc(fpr, tpr) \n",
    "\n",
    "            # Plot ROC curve for each class with AUC value and linestyle.\n",
    "            plt.plot(fpr, tpr, label=f\"ROC curve of class {class_labels[i]} (area = {roc_auc:.2f})\", linestyle=\":\", linewidth=2) \n",
    "\n",
    "    # Plot the diagonal line representing random guessing.\n",
    "    plt.plot([0, 1], [0, 1], \"k--\") \n",
    "\n",
    "    # Set x-axis limits from 0 to 1.\n",
    "    plt.xlim([0.0, 1.0]) \n",
    "\n",
    "    # Set y-axis limits from 0 to 1.05.\n",
    "    plt.ylim([0.0, 1.05]) \n",
    "\n",
    "    # Set x-axis label.\n",
    "    plt.xlabel(\"False Positive Rate\") \n",
    "\n",
    "    # Set y-axis label.\n",
    "    plt.ylabel(\"True Positive Rate\") \n",
    "\n",
    "    # Set plot title.\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\") \n",
    "\n",
    "    # Display legend in the lower right corner.\n",
    "    plt.legend(loc=\"lower right\") \n",
    "\n",
    "    # Show the plot.\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd760c-6794-42df-90bd-e19c5e2295e2",
   "metadata": {},
   "source": [
    "### Prepare features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50acfee-c56a-4ba6-bc17-2c101c3e9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: Select all columns except 'Outcome' as features (independent variables).\n",
    "X = df.drop(columns=['Outcome']).values\n",
    "\n",
    "# Target: Select the 'Outcome' column as the target variable (dependent variable), representing diabetes status (1=Diabetic, 0=Non-Diabetic).\n",
    "y = df['Outcome'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e60bde-bb21-48ec-a9f3-6db1ea0a03bf",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01e676-13bf-4302-bb62-3721ea884e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print a header to indicate class distribution information is being displayed.\n",
    "print(\"\\nClass Distribution:\")\n",
    "\n",
    "# Print the class distribution (counts of each class) in the training labels (train_y).\n",
    "print(\"Train:\", Counter(train_y))\n",
    "\n",
    "# Print the class distribution (counts of each class) in the test labels (test_y).\n",
    "print(\"Test:\", Counter(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef90994a-dff6-4f36-a3c2-2c1227a988d6",
   "metadata": {},
   "source": [
    "### Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518256a-b41b-415d-9b1e-0da6b62d799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using StandardScaler to standardize the training features.\n",
    "scaler = StandardScaler().fit(train_X)\n",
    "\n",
    "# Transform the training features 'train_X' using the fitted scaler and store the result in 'train_SX'.\n",
    "train_SX = scaler.transform(train_X)\n",
    "\n",
    "# Transform the test features 'test_X' using the fitted scaler and store the result in 'test_SX'.\n",
    "test_SX = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a652c2-ede8-4e32-ac47-41237477ea67",
   "metadata": {},
   "source": [
    "### Build and train the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882b7d7-4e84-4014-9e85-11cc180087e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Sequential Deep Neural Network model using Keras.\n",
    "model = Sequential([\n",
    "    # Add a Dense layer with 16 neurons, ReLU activation, and input shape matching the number of features.\n",
    "    Dense(16, activation=\"relu\", input_shape=(train_SX.shape[1],)),\n",
    "    \n",
    "    # Add another Dense hidden layer with 16 neurons and ReLU activation.\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    \n",
    "    # Add one more Dense hidden layer with 16 neurons and ReLU activation.\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    \n",
    "    # Add the output Dense layer with 1 neuron and sigmoid activation for binary classification.\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Configures the model for training use 'Adam' as optimizer, 'binary_crossentropy' \n",
    "# as loss funciton, and 'accuracy' as evaluation metrics\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the compiled DNN model using the training data.\n",
    "history = model.fit(\n",
    "    train_SX, train_y,\n",
    "    epochs=10, # Train for 10 epochs.\n",
    "    batch_size=5, # Use a batch size of 5 during training.\n",
    "    validation_split=0.1, # Use 10% of the training data for validation.\n",
    "    verbose=1 # Display training progress for each epoch.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d693824-66f4-4dd9-84cf-ea939420635e",
   "metadata": {},
   "source": [
    "### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279a92d-88c6-44fb-8327-3bd302eaa6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance on test set\n",
    "# Predict class labels (0 or 1) for the test set based on probability threshold of 0.5.\n",
    "y_pred = (model.predict(test_SX) > 0.5).astype(int)\n",
    "\n",
    "# Display model performance metrics (accuracy, precision, recall, F1-score, confusion matrix, classification report).\n",
    "display_model_performance_metrics(test_y, y_pred, classes=[0, 1])\n",
    "\n",
    "# Plot training history\n",
    "# Create a new figure with a size of 12x4 inches for the plots.\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Create the first subplot in a 1x2 grid (1 row, 2 columns), which is for accuracy.\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Plot the training accuracy from the training history.\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "\n",
    "# Plot the validation accuracy from the training history.\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "# Set the title of the accuracy subplot.\n",
    "plt.title('Accuracy')\n",
    "\n",
    "# Display the legend for the accuracy subplot.\n",
    "plt.legend()\n",
    "\n",
    "# Create the second subplot in a 1x2 grid (1 row, 2 columns), which is for loss.\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Plot the training loss from the training history.\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "\n",
    "# Plot the validation loss from the training history.\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "# Set the title of the loss subplot.\n",
    "plt.title('Loss')\n",
    "\n",
    "# Display the legend for the loss subplot.\n",
    "plt.legend()\n",
    "\n",
    "# Show the complete figure with both subplots.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925f4e6-8c2c-4884-a7ea-4838458d31ba",
   "metadata": {},
   "source": [
    "## Model Training and Validation Performance Analysis\n",
    "\n",
    "The plot provides a comprehensive view of the deep learning model's performance, both during training and on the unseen test dataset. Let's break down each component to understand the model's strengths and areas for potential improvement.\n",
    "\n",
    "### 1. Model Performance Metrics (Test Set)\n",
    "\n",
    "These metrics evaluate the model's ability to generalize to new, unseen data (the test set).\n",
    "\n",
    "*   **Accuracy: 0.7403**\n",
    "    -   **Interpretation:** The model correctly classified approximately 74.03% of the samples in the test dataset. This provides an overall measure of correctness.\n",
    "*   **Precision: 0.7425**\n",
    "    -   **Interpretation:**  When the model predicts a class (considering both classes in a weighted manner), it is correct about 74.25% of the time.  This indicates the model's ability to avoid false positives, weighted by class support.\n",
    "*   **Recall: 0.7403**\n",
    "    -   **Interpretation:** The model correctly identified approximately 74.03% of the actual positive instances (again, weighted across both classes). This reflects the model's ability to avoid false negatives, weighted by class support.\n",
    "*   **F1 Score: 0.7413**\n",
    "    -   **Interpretation:** The F1-score, being the harmonic mean of precision and recall, provides a balanced measure of the model's performance. A score of 0.7413 suggests a good balance between precision and recall on the test set.\n",
    "\n",
    "**Overall Test Set Performance:** The model demonstrates a reasonable performance on the test set, with all metrics around 75%. This suggests a moderate level of generalization to unseen data.\n",
    "\n",
    "### 2. Classification Report\n",
    "\n",
    "This report offers a class-wise breakdown of the model's performance, providing insights into how well it performs for each class (0 and 1).\n",
    "\n",
    "| Metric        | Class 0 | Class 1 |\n",
    "|---------------|---------|---------|\n",
    "| **Precision** | 0.80    | 0.63    |\n",
    "| **Recall**    | 0.79    | 0.65    |\n",
    "| **F1-Score**  | 0.80    | 0.64    |\n",
    "| **Support**   | 99      | 55      |\n",
    "\n",
    "*   **Class 0 Performance:** The model shows strong performance for Class 0, with precision, recall, and F1-score all at 0.80, 0.79 and 0.80. This indicates the model is effective at identifying and classifying instances of Class 0.\n",
    "*   **Class 1 Performance:** Performance is lower for Class 1, with precision, recall, and F1-score at 0.63, 0.65, and 0.64. This suggests the model struggles more with accurately classifying instances of Class 1 compared to Class 0.\n",
    "*   **Support:** The 'support' indicates the number of actual instances of each class in the test set. Class 0 (99 instances) is more prevalent than Class 1 (55 instances), suggesting a potential class imbalance.\n",
    "*   **Accuracy (Report): 0.74** -  The overall accuracy reported in the classification report is 0.74, slightly lower than the 0.7403 reported separately, likely due to rounding or calculation differences.\n",
    "*   **Macro Avg F1-Score: 0.72** - The unweighted average F1-score across both classes.\n",
    "*   **Weighted Avg F1-Score: 0.74** - The F1-score weighted by the support (number of instances) for each class, aligning with the F1-score reported in the initial metrics.\n",
    "\n",
    "**Class-Specific Performance:** The model is better at predicting Class 0 than Class 1. The class imbalance (more instances of Class 0) might contribute to this difference in performance.\n",
    "\n",
    "### 3. Confusion Matrix\n",
    "\n",
    "The confusion matrix visually summarizes the performance by showing counts of True Positives, True Negatives, False Positives, and False Negatives.\n",
    "\n",
    "| Predicted Class | Class 0 | Class 1 |\n",
    "|-----------------|---------|---------|\n",
    "| **Actual Class 0** | 78 (TN) | 21 (FP) |\n",
    "| **Actual Class 1** | 19 (FN) | 36 (TP) |\n",
    "\n",
    "*   **True Negatives (TN) = 78:**  Correctly predicted Class 0 when the actual class was 0.\n",
    "*   **False Positives (FP) = 21:** Incorrectly predicted Class 1 when the actual class was 0 (Type I Error).\n",
    "*   **False Negatives (FN) = 19:** Incorrectly predicted Class 0 when the actual class was 1 (Type II Error).\n",
    "*   **True Positives (TP) = 36:** Correctly predicted Class 1 when the actual class was 1.\n",
    "\n",
    "**Confusion Analysis:** The confusion matrix reinforces the observation that the model performs better for Class 0 (higher TN) compared to Class 1 (lower TP). The equal number of False Positives and False Negatives (19 each) suggests a relatively balanced type of error, though further investigation into minimizing False Negatives might be important depending on the application.\n",
    "\n",
    "### 4. Accuracy and Loss Curves (Training History)\n",
    "\n",
    "These plots illustrate the model's learning process over training epochs, showing trends in accuracy and loss for both the training and validation datasets.\n",
    "\n",
    "*   **Accuracy Curve:**\n",
    "    -   **Train Accuracy (Blue):**  Increases steadily and plateaus around 0.80, indicating good learning on the training data.\n",
    "    -   **Validation Accuracy (Orange):**  Also increases but plateaus slightly lower than training accuracy, around 0.73. The validation accuracy generally follows the trend of the training accuracy, suggesting the model is learning effectively without severe overfitting.\n",
    "*   **Loss Curve:**\n",
    "    -   **Train Loss (Blue):** Decreases consistently, indicating the model is minimizing the error on the training data.\n",
    "    -   **Validation Loss (Orange):** Decreases initially and then plateaus at a slightly higher value than the training loss. The validation loss is slightly above the training loss, which is expected, and it doesn't show a significant increase, further suggesting no major overfitting.\n",
    "\n",
    "**Training Dynamics:** The training curves show that the model is learning and improving over epochs. The validation curves follow a similar trend to the training curves, indicating that the model is generalizing reasonably well to unseen data during training. The slight gap between training and validation performance is normal and might be reduced with further optimization.\n",
    "\n",
    "### **Summary and Potential Improvements**\n",
    "\n",
    "The model demonstrates a **moderate level of performance** in classifying the dataset, achieving around 75% accuracy and F1-score on the test set. It performs better on Class 0 compared to Class 1, potentially due to class imbalance. The training curves suggest the model is learning without severe overfitting.\n",
    "\n",
    "**Potential areas for improvement include:**\n",
    "\n",
    "*   **Addressing Class Imbalance:** Techniques to handle class imbalance (e.g., oversampling, undersampling, class weights) could improve performance, especially for Class 1.\n",
    "*   **Hyperparameter Tuning:** Experimenting with different model architectures (layers, neurons), optimizers, learning rates, and regularization techniques might lead to better results.\n",
    "*   **Feature Engineering/Selection:** Exploring feature engineering or selection to enhance the model's ability to discriminate between classes could be beneficial.\n",
    "*   **Increasing Data:**  More training data, if available, could help the model generalize better and improve overall performance.\n",
    "\n",
    "By addressing these areas, you could potentially optimize the model to achieve higher accuracy and a more balanced performance across both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c34aa-6573-45cd-8daa-d81ecac0337f",
   "metadata": {},
   "source": [
    "### Prediction using new patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72d906-da49-4300-add1-74b8535babb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create new sample data (replace with your desired sample data)\n",
    "# Define a dictionary 'new_samples_data' to hold data for new samples.\n",
    "new_samples_data = {\n",
    "    # Define data for 'Sample1' as a dictionary of features and their values.\n",
    "    'Sample1': {'Pregnancies': 2, 'Glucose': 100, 'BloodPressure': 70, 'SkinThickness': 25, 'Insulin': 50, 'BMI': 30.0, 'DiabetesPedigreeFunction': 0.5, 'Age': 35},\n",
    "    \n",
    "    # Define data for 'Sample2' as a dictionary of features and their values.\n",
    "    'Sample2': {'Pregnancies': 8, 'Glucose': 180, 'BloodPressure': 90, 'SkinThickness': 35, 'Insulin': 100, 'BMI': 40.0, 'DiabetesPedigreeFunction': 1.0, 'Age': 50}\n",
    "}\n",
    "# Create a Pandas DataFrame 'new_samples_df' from the 'new_samples_data' dictionary, orienting it by index.\n",
    "new_samples_df = pd.DataFrame.from_dict(new_samples_data, orient='index')\n",
    "\n",
    "# 2. Preprocess the new samples using the SAME scaler fitted on training data\n",
    "# Transform the new sample data using the 'scaler' object (fitted on training data) to standardize features.\n",
    "new_samples_scaled = scaler.transform(new_samples_df)\n",
    "\n",
    "# 3. Make predictions\n",
    "# Get probability predictions from the 'model' for the scaled new samples.\n",
    "new_probabilities = model.predict(new_samples_scaled) # Get probabilities\n",
    "\n",
    "# 4. Display the predictions\n",
    "# Print a header to indicate predictions for new samples.\n",
    "print(\"\\n--- Predictions for New Samples ---\")\n",
    "# Iterate through the new samples using their index and sample name.\n",
    "for i, sample_name in enumerate(new_samples_data.keys()):\n",
    "    # Predict the class label (0 or 1) by thresholding the probability at 0.5.\n",
    "    prediction = (new_probabilities[i] > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate the probability of being 'non-diabetic' (class 0) in percentage.\n",
    "    # Probability for 'non-diabetic' class\n",
    "    probability_non_diabetic = (1 - new_probabilities[i][0]) * 100 \n",
    "    \n",
    "    # Calculate the probability of being 'diabetic' (class 1) in percentage.\n",
    "    # Probability for 'diabetic' class\n",
    "    probability_diabetic = new_probabilities[i][0] * 100   \n",
    "    \n",
    "    # Print the sample name.\n",
    "    print(f\"Sample: {sample_name}\")\n",
    "    \n",
    "    # Print the predicted outcome (class label).\n",
    "    print(f\"  Predicted Outcome: {prediction}\")\n",
    "    \n",
    "    # Print the probability of being 'non-diabetic'.\n",
    "    print(f\"  Probability (non-diabetic): {probability_non_diabetic:.2f}%\")\n",
    "    \n",
    "    # Print the probability of being 'diabetic'.\n",
    "    print(f\"  Probability (diabetic): {probability_diabetic:.2f}%\")\n",
    "    \n",
    "    # Print a separator line for better readability.\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358618fa-761b-42d7-bb5f-9afa0e64b6b6",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "This notebook successfully demonstrated the development and evaluation of a Deep Neural Network model for diabetes prediction using the Pima Indians Diabetes Dataset. \n",
    "- We covered the essential steps of a machine learning workflow, including data loading, preprocessing, model building with TensorFlow/Keras, training, and performance evaluation. \n",
    "- The model achieved a reasonable level of accuracy in predicting diabetes based on the provided health metrics.\n",
    "- This example provides a foundation for understanding how deep learning can be applied to healthcare classification problems and can be further extended by exploring different model architectures, hyperparameter tuning, and feature engineering techniques to potentially improve prediction performance.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
