{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning\n",
    "\n",
    "Adpated from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers basic data cleaning techniques using Python and pandas. We'll explore common data quality issues and learn how to address them effectively.\n",
    "\n",
    "Data cleaning is a crucial step in the data science workflow, ensuring that your datasets are accurate, consistent, and ready for analysis. In this tutorial, you'll learn to identify and address common data quality issues, such as missing values, duplicates, and inconsistent formats. Through hands-on exercises using pandas, you'll gain practical experience in essential data cleaning tasks. By the end of this tutorial, you'll have developed the skills necessary to prepare datasets for further analysis and modeling, setting a strong foundation for your data science projects.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the importance of data cleaning in the data science workflow\n",
    "- Learn to identify and handle common data quality issues\n",
    "- Gain practical experience in using pandas for data cleaning tasks\n",
    "- Develop skills to prepare datasets for further analysis and modeling\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic knowledge of Python programming\n",
    "- Familiarity with pandas library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages, import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs the pandas library using pip, a package installer for Python.\n",
    "# Pandas is a powerful data manipulation and analysis library.\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the pandas library and assigns it the alias 'pd' for easier use throughout the code.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messy Dataset\n",
    "\n",
    "The breast cancer dataset classifies breast cancer patient as either a recurrence or no recurrence of cancer. \n",
    "\n",
    "```\n",
    "Number of Instances: 289\n",
    "Number of Attributes: 9 + the class attribute\n",
    "Attribute Information:\n",
    "   1. Class: no-recurrence-events, recurrence-events\n",
    "   2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "   3. menopause: lt40, ge40, premeno.\n",
    "   4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
    "   5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
    "   6. node-caps: yes, no.\n",
    "   7. deg-malig: 1, 2, 3.\n",
    "   8. breast: left, right.\n",
    "   9. breast-quad: left-up, left-low, right-up,\tright-low, central.\n",
    "  10. irradiat:\tyes, no.\n",
    "Missing Attribute Values: (denoted by \"?\")\n",
    "   Attribute #:  Number of instances with missing values:\n",
    "   6.             8\n",
    "   9.             1.\n",
    "Class Distribution:\n",
    "    1. no-recurrence-events: 201 instances\n",
    "    2. recurrence-events: 85 instances \n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Breast Cancer Dataset ([breast-cancer.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv))\n",
    "* Breast Cancer Dataset Description ([breast-cancer.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names))\n",
    "\n",
    "\n",
    "The messy dataset was modified from Breast Cancer Dataset so that various data cleaning techniques may be demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Columns That Contain a Single Value\n",
    "\n",
    "First, summarize the number of unique values for each column using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to the messy_data.csv dataset.\n",
    "messy_data = \"../../Data/messy_data.csv\"\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv(messy_data, header=None)\n",
    "\n",
    "# Display the first few rows of the DataFrame to inspect the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, summarize the number of unique values in each column using `nunique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the DataFrame 'df', which represents the number of rows and columns.\n",
    "print(\"Shape of messy data: \", df.shape)\n",
    "# Print a header to indicate that the following output shows the number of unique values per column.\n",
    "print(\"Column\\t#Unique values \")\n",
    "# Print the number of unique values for each column in the DataFrame 'df'.\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that column index 5 only has a single value and should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete columns that contain a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from a csv file named 'messy_data' into a pandas DataFrame\n",
    "df = pd.read_csv(messy_data, header=None)\n",
    "# print the shape (number of rows and columns) of the DataFrame to understand its dimensions\n",
    "print(df.shape)\n",
    "\n",
    "# calculate and store the number of unique values in each column of the DataFrame\n",
    "counts = df.nunique()\n",
    "\n",
    "# create a list called 'to_del' containing the indices of columns where the number of unique values is equal to 1\n",
    "to_del = [i for i, v in enumerate(counts) if v == 1]\n",
    "# print the list of column indices that are marked for deletion because they have only one unique value\n",
    "print(to_del)\n",
    "\n",
    "# drop the columns identified in 'to_del' from the DataFrame in place (modifying the DataFrame directly)\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "# print the shape of the DataFrame again after dropping the columns to show the updated dimensions\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify columns that have very few values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file into a pandas DataFrame. The file path is specified by the variable 'messy_data' and there is no header row in the CSV.\n",
    "df = pd.read_csv(messy_data, header=None)\n",
    "\n",
    "# Print a header to the output summarizing the columns that will be displayed.\n",
    "print(\"Column, Count, <1%\")\n",
    "# Iterate through each column index and the count of unique values in each column of the DataFrame.\n",
    "for i, v in enumerate(df.nunique()):\n",
    "    # Calculate the percentage of unique values in the current column relative to the total number of rows in the DataFrame.\n",
    "    percentage = float(v) / df.shape[0] * 100\n",
    "    # Check if the calculated percentage is less than 1%.\n",
    "    if percentage < 1:\n",
    "        # If the percentage is less than 1%, print the column index (i), the count of unique values (v), and the calculated percentage (formatted to one decimal place).\n",
    "        print(\"%d, %d, %.1f%%\" % (i, v, percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns with unique values less than 1 percent of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from the CSV file 'messy_data' into a pandas DataFrame, assuming no header row.\n",
    "df = pd.read_csv(messy_data, header=None)\n",
    "# print the shape of the DataFrame (number of rows and columns) to get an overview of its dimensions.\n",
    "print(df.shape)\n",
    "\n",
    "# calculate the number of unique values in each column of the DataFrame.\n",
    "counts = df.nunique()\n",
    "\n",
    "# identify columns to be deleted based on a threshold: if the percentage of unique values in a column is less than 1% of the total number of rows, mark it for deletion.\n",
    "to_del = [i for i, v in enumerate(counts) if (float(v) / df.shape[0] * 100) < 1]\n",
    "# print the indices of the columns identified for deletion.\n",
    "print(\"Columns to delete: \", to_del)\n",
    "\n",
    "# drop the columns identified in 'to_del' from the DataFrame. axis=1 specifies columns, and inplace=True modifies the DataFrame directly.\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "# print the shape of the DataFrame again after dropping the columns to show the reduced dimensions.\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify rows that contain duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file named 'messy_data' into a pandas DataFrame, assuming no header row in the file.\n",
    "df = pd.read_csv(messy_data, header=None)\n",
    "\n",
    "# Identify and create a boolean Series 'dups' indicating duplicate rows in the DataFrame 'df'.\n",
    "dups = df.duplicated()\n",
    "\n",
    "# Print a message to the console indicating whether any duplicate rows were found in the DataFrame 'df'.\n",
    "print(\"Any duplicates? \", dups.any())\n",
    "\n",
    "# Print a header message indicating the display of duplicated rows.\n",
    "print(\"Duplicated rows:\")\n",
    "# Print all rows from the DataFrame 'df' that are marked as duplicates in the boolean Series 'dups'.\n",
    "print(df[dups])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete rows that contain duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file named 'messy_data' into a pandas DataFrame.\n",
    "# 'header=None' argument indicates that the CSV file does not have a header row.\n",
    "df = pd.read_csv(messy_data, header=None)\n",
    "# Print the shape of the DataFrame after loading the data.\n",
    "# This will output the number of rows and columns in the DataFrame before removing duplicates.\n",
    "print(df.shape)\n",
    "\n",
    "# Remove duplicate rows from the DataFrame in place.\n",
    "# 'inplace=True' modifies the DataFrame directly instead of returning a new DataFrame.\n",
    "df.drop_duplicates(inplace=True)\n",
    "# Print the shape of the DataFrame after removing duplicate rows.\n",
    "# This will output the number of rows and columns in the DataFrame after duplicate rows are removed.\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've learned essential data cleaning techniques using Python and pandas. We've covered how to handle missing values, remove duplicates, correct data types, and address inconsistent data. These skills are crucial for preparing datasets for further analysis and modeling in data science projects.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
