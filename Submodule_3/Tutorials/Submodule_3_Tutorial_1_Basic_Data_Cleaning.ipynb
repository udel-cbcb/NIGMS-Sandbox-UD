{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Basic Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn:\n",
    "\n",
    "* How to identify and remove column variables that only have a single value.\n",
    "* How to identify and consider column variables with very few unique values.\n",
    "* How to identify and remove rows that contain duplicate observations.\n",
    "\n",
    "Adpated from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Messy Dataset\n",
    "\n",
    "\n",
    "Breast cancer dataset classifies breast cancer\n",
    "patient as either a recurrence or no recurrence of cancer. \n",
    "\n",
    "```\n",
    "Number of Instances: 289\n",
    "Number of Attributes: 9 + the class attribute\n",
    "Attribute Information:\n",
    "   1. Class: no-recurrence-events, recurrence-events\n",
    "   2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "   3. menopause: lt40, ge40, premeno.\n",
    "   4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
    "   5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
    "   6. node-caps: yes, no.\n",
    "   7. deg-malig: 1, 2, 3.\n",
    "   8. breast: left, right.\n",
    "   9. breast-quad: left-up, left-low, right-up,\tright-low, central.\n",
    "  10. irradiat:\tyes, no.\n",
    "Missing Attribute Values: (denoted by \"?\")\n",
    "   Attribute #:  Number of instances with missing values:\n",
    "   6.             8\n",
    "   9.             1.\n",
    "Class Distribution:\n",
    "    1. no-recurrence-events: 201 instances\n",
    "    2. recurrence-events: 85 instances \n",
    "```\n",
    "You can learn more about the dataset here:\n",
    "* Breast Cancer Dataset ([breast-cancer.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv))\n",
    "* Breast Cancer Dataset Description ([breast-cancer.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names))\n",
    "\n",
    "\n",
    "The messy dataset was modified from Breast Cancer Dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Download messy data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wget\n",
    "! python -m wget \"https://raw.githubusercontent.com/udel-cbcb/al_ml_workshop/main/data/messy_data.csv\" -o messy_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Identify Columns That Contain a Single Value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the number of unique values for each column using pandas\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv('messy_data.csv', header=None)\n",
    "# Peek into the top five rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the number of unique values in each column using nunique()\n",
    "print(\"Shape of messy data: \", df.shape)\n",
    "print(\"Column\\t#Unique values \")\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that column index 5 only has a single value and should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Delete columns that contain a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete columns with a single unique value\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv('messy_data.csv', header=None)\n",
    "print(df.shape)\n",
    "# get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
    "print(to_del)\n",
    "# drop useless columns\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Identify columns that have very few values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv('messy_data.csv', header=None)\n",
    "# summarize the number of unique values in each column\n",
    "print(\"Column, Count, <1%\")\n",
    "for i, v in enumerate(df.nunique()):\n",
    "  # Percent of number of unique values across rows\n",
    "  percentage = float(v) / df.shape[0] * 100\n",
    "  if percentage < 1:\n",
    "    print('%d, %d, %.1f%%' % (i, v, percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Drop columns with unique values less than 1 percent of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete columns where number of unique values is less than 1% of the rows\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv('messy_data.csv', header=None)\n",
    "print(df.shape)\n",
    "# get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0] * 100) < 1]\n",
    "print(\"Columns to delete: \", to_del)\n",
    "# drop useless columns\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Identify rows that contain duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate rows of duplicate data\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv('messy_data.csv', header=None)\n",
    "# calculate duplicates\n",
    "dups = df.duplicated()\n",
    "# report if there are any duplicates\n",
    "print(\"Any duplicates? \", dups.any())\n",
    "# list all duplicate rows\n",
    "print(\"Duplicated rows:\")\n",
    "print(df[dups])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Delete rows that contain duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows of duplicate data from the dataset\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv('messy_data.csv', header=None)\n",
    "print(df.shape)\n",
    "# delete duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
