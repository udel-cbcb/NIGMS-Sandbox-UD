{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning\n",
    "\n",
    "Adpated from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers basic data cleaning techniques using Python and pandas. We'll explore common data quality issues and learn how to address them effectively.\n",
    "\n",
    "Data cleaning is a crucial step in the data science workflow, ensuring that your datasets are accurate, consistent, and ready for analysis. In this tutorial, you'll learn to identify and address common data quality issues, such as missing values, duplicates, and inconsistent formats. Through hands-on exercises using pandas, you'll gain practical experience in essential data cleaning tasks. By the end of this tutorial, you'll have developed the skills necessary to prepare datasets for further analysis and modeling, setting a strong foundation for your data science projects.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the importance of data cleaning in the data science workflow\n",
    "- Learn to identify and handle common data quality issues\n",
    "- Gain practical experience in using pandas for data cleaning tasks\n",
    "- Develop skills to prepare datasets for further analysis and modeling\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic knowledge of Python programming\n",
    "- Familiarity with pandas library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages, import the necessary libraries, and define a helper function to download data using the `requests` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility functions\n",
    "\n",
    "Define a helper function for downloading example datasets.  \n",
    "\n",
    "*Note!* It is not essential that you understand the following code.  It is just for getting the example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, to_file):\n",
    "    \"\"\"Download content from the given URL and save it to a file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to download the content from.\n",
    "        to_file (str): The name of the file to save the downloaded content to.\n",
    "\n",
    "    \"\"\"\n",
    "    response = requests.get(url, timeout=10)\n",
    "    Path(to_file).write_bytes(response.content)\n",
    "    print(f\"downloaded file '{to_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messy Dataset\n",
    "\n",
    "The breast cancer dataset classifies breast cancer patient as either a recurrence or no recurrence of cancer. \n",
    "\n",
    "```\n",
    "Number of Instances: 289\n",
    "Number of Attributes: 9 + the class attribute\n",
    "Attribute Information:\n",
    "   1. Class: no-recurrence-events, recurrence-events\n",
    "   2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "   3. menopause: lt40, ge40, premeno.\n",
    "   4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
    "   5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
    "   6. node-caps: yes, no.\n",
    "   7. deg-malig: 1, 2, 3.\n",
    "   8. breast: left, right.\n",
    "   9. breast-quad: left-up, left-low, right-up,\tright-low, central.\n",
    "  10. irradiat:\tyes, no.\n",
    "Missing Attribute Values: (denoted by \"?\")\n",
    "   Attribute #:  Number of instances with missing values:\n",
    "   6.             8\n",
    "   9.             1.\n",
    "Class Distribution:\n",
    "    1. no-recurrence-events: 201 instances\n",
    "    2. recurrence-events: 85 instances \n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Breast Cancer Dataset ([breast-cancer.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv))\n",
    "* Breast Cancer Dataset Description ([breast-cancer.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names))\n",
    "\n",
    "\n",
    "The messy dataset was modified from Breast Cancer Dataset so that various data cleaning techniques may be demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download messy data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\n",
    "  url=\"https://raw.githubusercontent.com/udel-cbcb/al_ml_workshop/main/data/messy_data.csv\",\n",
    "  to_file=\"messy_data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Columns That Contain a Single Value\n",
    "\n",
    "First, summarize the number of unique values for each column using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"messy_data.csv\", header=None)\n",
    "\n",
    "# Peek into the top five rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, summarize the number of unique values in each column using `nunique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of messy data: \", df.shape)\n",
    "print(\"Column\\t#Unique values \")\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that column index 5 only has a single value and should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete columns that contain a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv(\"messy_data.csv\", header=None)\n",
    "print(df.shape)\n",
    "\n",
    "# get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "\n",
    "# record columns to delete\n",
    "to_del = [i for i, v in enumerate(counts) if v == 1]\n",
    "print(to_del)\n",
    "\n",
    "# drop useless columns\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify columns that have very few values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv(\"messy_data.csv\", header=None)\n",
    "\n",
    "# summarize the number of unique values in each column\n",
    "print(\"Column, Count, <1%\")\n",
    "for i, v in enumerate(df.nunique()):\n",
    "    # Percent of number of unique values across rows\n",
    "    percentage = float(v) / df.shape[0] * 100\n",
    "    if percentage < 1:\n",
    "        print(\"%d, %d, %.1f%%\" % (i, v, percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns with unique values less than 1 percent of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv(\"messy_data.csv\", header=None)\n",
    "print(df.shape)\n",
    "\n",
    "# get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "\n",
    "# record columns to delete\n",
    "to_del = [i for i, v in enumerate(counts) if (float(v) / df.shape[0] * 100) < 1]\n",
    "print(\"Columns to delete: \", to_del)\n",
    "\n",
    "# drop useless columns\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify rows that contain duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv(\"messy_data.csv\", header=None)\n",
    "\n",
    "# calculate duplicates\n",
    "dups = df.duplicated()\n",
    "\n",
    "# report if there are any duplicates\n",
    "print(\"Any duplicates? \", dups.any())\n",
    "\n",
    "# list all duplicate rows\n",
    "print(\"Duplicated rows:\")\n",
    "print(df[dups])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete rows that contain duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv(\"messy_data.csv\", header=None)\n",
    "print(df.shape)\n",
    "\n",
    "# delete duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've learned essential data cleaning techniques using Python and pandas. We've covered how to handle missing values, remove duplicates, correct data types, and address inconsistent data. These skills are crucial for preparing datasets for further analysis and modeling in data science projects.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
