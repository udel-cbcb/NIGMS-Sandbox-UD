{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Data with Outliers\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Many machine learning algorithms perform better when numerical input variables are scaled to\n",
    "a standard range. This includes algorithms that use a weighted sum of the input, like linear\n",
    "regression, and algorithms that use distance measures, like k-nearest neighbors. Standardizing\n",
    "is a popular scaling technique that subtracts the mean from values and divides by the standard\n",
    "deviation, transforming the probability distribution for an input variable to a standard Gaussian\n",
    "(zero mean and unit variance). Standardization can become skewed or biased if the input\n",
    "variable contains outlier values.\n",
    "To overcome this, the median and interquartile range can be used when standardizing\n",
    "numerical input variables, generally referred to as robust scaling.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand why many machine learning algorithms prefer scaled numerical input variables\n",
    "- Learn robust scaling techniques that use percentiles to scale numerical input variables containing outliers\n",
    "- Master using `RobustScaler` to scale numerical input variables using median and interquartile range\n",
    "- Evaluate the impact of different scaling ranges on model performance\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Examine raw dataset distributions\n",
    "- Apply robust scaling transformation\n",
    "- Evaluate model performance with different scaling ranges\n",
    "- Compare results between scaled and unscaled data\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment\n",
    "- Basic understanding of Python programming concepts\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages and import necessary libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python libraries using pip\n",
    "%pip install matplotlib numpy pandas scikit-learn  \n",
    "\n",
    "# matplotlib - A library for creating static, animated, and interactive visualizations in Python.\n",
    "# numpy - A fundamental package for numerical computing, providing support for arrays and mathematical operations.\n",
    "# pandas - A powerful data analysis and manipulation library, useful for working with structured data.\n",
    "# scikit-learn - A machine learning library that provides simple and efficient tools for data mining and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from matplotlib import pyplot  # For plotting graphs\n",
    "from numpy import mean, std  # For calculating mean and standard deviation\n",
    "from pandas import DataFrame, read_csv  # For handling data as DataFrames and reading CSV files\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score  # For cross-validation\n",
    "from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors classifier\n",
    "from sklearn.pipeline import Pipeline  # To create machine learning pipelines\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler  # For data preprocessing\n",
    "\n",
    "# Define the path to the dataset\n",
    "pima_indians_diabetes_csv = \"../../Data/pima-indians-diabetes.csv\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaling Data\n",
    "\n",
    "If there are input\n",
    "variables that have very large values relative to the other input variables, these large values\n",
    "can dominate or skew some machine learning algorithms. The result is that the algorithms pay\n",
    "most of their attention to the large values and ignore the variables with smaller values.\n",
    "\n",
    "Sometimes an input variable may have outlier values. Outliers can skew a probability distribution and make data scaling using standardization\n",
    "difficult as the calculated mean and standard deviation will be skewed by the presence of\n",
    "the outliers. \n",
    "\n",
    "Robust standardization or robust data\n",
    "scaling that calculates the median (50th percentile) and the 25th and\n",
    "75th percentiles. The values of each variable then have their median subtracted and are divided\n",
    "by the interquartile range (IQR) which is the difference between the 75th and 25th percentiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient data as\n",
    "either an onset of diabetes within five years or not.\n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class \n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and summarize the diabetes dataset\n",
    "\n",
    "# Define the dataset file path (Ensure 'pima_indians_diabetes_csv' is correctly defined)\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)  # Load dataset without headers\n",
    "print(dataset.head())  # Display the first five rows of the dataset\n",
    "\n",
    "# Summarize the shape of the dataset\n",
    "print(dataset.shape)  # Print the number of rows and columns in the dataset\n",
    "\n",
    "# Summarize each variable (statistical summary)\n",
    "print(dataset.describe())  # Display summary statistics (count, mean, std, min, max, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the 8\n",
    "input variables, one output variable, and 768 rows of data. A statistical summary of the input\n",
    "variables is provided show that each variable has a very different scale. This makes it a good\n",
    "dataset for exploring data scaling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a histogram for each input variable. The plots confirm the differing scale\n",
    "for each input variable and show that the variables have differing scales. Importantly, we can see\n",
    "some of the distributions show the presence of outliers. The dataset provides a good candidate for using a robust scaler transform to standardize the data in the presence of diâ€€ering input\n",
    "variable scales and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate histograms for all variables in the dataset\n",
    "# xlabelsize and ylabelsize control the font size of axis labels\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "\n",
    "# Reduce the title size of each subplot for better readability\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# Display the histogram plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit and evaluate a machine learning model on the raw dataset. We will use\n",
    "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
    "stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate k-Nearest Neighbors (KNN) on the raw Pima Indians Diabetes dataset\n",
    "\n",
    "# Load dataset from CSV file (Assuming 'pima_indians_diabetes_csv' is defined elsewhere)\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Convert the dataset into a NumPy array\n",
    "data = dataset.values\n",
    "\n",
    "# Separate the dataset into input features (X) and output labels (y)\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# Ensure input features are of type float (for consistency in calculations)\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "# Encode the output labels as integers (necessary for classification models)\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# Define the KNN model with default parameters\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Set up cross-validation with 10 splits, repeated 3 times for robust evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Perform cross-validation and compute accuracy scores across different folds\n",
    "n_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report the model's mean accuracy and standard deviation across all cross-validation runs\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "# Show any plots (though no plots are explicitly generated in this code)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the model achieved a mean classiâ€€cationfiaccuracy of about\n",
    "71.7 percent.\n",
    "\n",
    "Next, let's explore a robust scaling transform of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQR Robust Scaler Transform\n",
    "\n",
    "We can apply the robust scaler to the diabetes dataset directly. We will use the default\n",
    "configuration and scale values to the IQR. First, a **RobustScaler** instance is defined with\n",
    "default hyperparameters. Once defined, we can call the **fit.transform()** function and pass it\n",
    "to our dataset to create a robust scale transformed version of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pima Indians Diabetes dataset from a CSV file\n",
    "# `header=None` ensures that no row is treated as column names\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Extract only the numeric input features (excluding the target variable in the last column)\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# Initialize the RobustScaler, which scales features using statistics \n",
    "# that are robust to outliers (i.e., median and interquartile range)\n",
    "trans = RobustScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it to remove the influence of outliers\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# Convert the transformed NumPy array back into a Pandas DataFrame for better readability\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# Display summary statistics of the transformed dataset\n",
    "# This includes count, mean, standard deviation, min, and percentile values\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the\n",
    "distributions have been adjusted. The median values are now zero and the standard deviation\n",
    "values are now close to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate histograms for all variables in the dataset\n",
    "# xlabelsize and ylabelsize adjust the font size of axis labels for readability\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "\n",
    "# Reduce the title font size for each subplot in the figure\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# Display the histogram plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram plots of the variables are created, although the distributions don't look much\n",
    "different from their original distributions seen in the previous section. We can see that the\n",
    "center of mass for each distribution is now close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
    "robust scaler transform of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-Nearest Neighbors (KNN) on the Pima Indians Diabetes dataset \n",
    "# using RobustScaler for data preprocessing\n",
    "\n",
    "# Load the dataset from CSV file\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)  # Replace with actual file path\n",
    "data = dataset.values  # Convert dataframe to NumPy array for processing\n",
    "\n",
    "# Separate the dataset into input features (X) and target labels (y)\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# Convert input features to float type and encode target labels as integers\n",
    "X = X.astype(\"float32\")  \n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))  # Ensures target labels are numerically encoded\n",
    "\n",
    "# Define a preprocessing and modeling pipeline\n",
    "trans = RobustScaler()  # Use RobustScaler to handle outliers and normalize features\n",
    "model = KNeighborsClassifier()  # Initialize KNN classifier\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])  # Create a pipeline with scaling and classification\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)  \n",
    "# This splits data into 10 folds and repeats the process 3 times to ensure robust evaluation\n",
    "\n",
    "# Evaluate the pipeline using cross-validation and compute accuracy scores\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report mean accuracy and standard deviation of scores\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "# Show any plots (though no plot is explicitly created in this code)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the robust scaler transform results in a lift in\n",
    "performance from 71.7 percent accuracy without the transform to about 73.4 percent with the\n",
    "transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Robust Scaler Range\n",
    "\n",
    "The range used to scale each variable is chosen by default as the IQR is bounded by the 25th\n",
    "and 75th percentiles. This is specified by the quantile range argument as a tuple. Other\n",
    "values can be specified and might improve the performance of the model, such as a wider range,\n",
    "allowing fewer values to be considered outliers, or a more narrow range, allowing more values\n",
    "to be considered outliers. The example below explores the effect of different definitions of the\n",
    "range from 1st to the 99th percentiles to 30th to 70th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the scaling range of the robust scaler transform\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    # Load the dataset from the specified CSV file without a header row\n",
    "    dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "    \n",
    "    # Convert the dataset into a numpy array\n",
    "    data = dataset.values\n",
    "    \n",
    "    # Separate the dataset into input features (X) and output labels (y)\n",
    "    X, y = data[:, :-1], data[:, -1]  # Inputs (all columns except the last), Outputs (last column)\n",
    "    \n",
    "    # Ensure that the input features are of type float32 (for better performance)\n",
    "    X = X.astype(\"float32\")\n",
    "    \n",
    "    # Convert the output labels to integers using LabelEncoder (for classification tasks)\n",
    "    y = LabelEncoder().fit_transform(y.astype(\"str\"))  # Convert to string and then encode to integers\n",
    "    \n",
    "    # Return the prepared input and output data\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    # Create an empty dictionary to store the models\n",
    "    models = dict()\n",
    "    \n",
    "    # Iterate over a list of values to create different models\n",
    "    for value in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        # Define the transformation step using RobustScaler with quantile_range based on 'value'\n",
    "        trans = RobustScaler(quantile_range=(value, 100 - value))\n",
    "        \n",
    "        # Define the classifier model using KNeighborsClassifier\n",
    "        model = KNeighborsClassifier()\n",
    "        \n",
    "        # Store the model in the dictionary, where the key is the string representation of 'value'\n",
    "        # The pipeline includes both the transformation step and the model step\n",
    "        models[str(value)] = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "    \n",
    "    # Return the dictionary of models\n",
    "    return models\n",
    "\n",
    "\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # Create a Repeated Stratified K-Fold cross-validation object\n",
    "    # n_splits=10: 10-fold cross-validation (split data into 10 parts)\n",
    "    # n_repeats=3: repeat the process 3 times to get more robust results\n",
    "    # random_state=1: ensures reproducibility of results by fixing the random seed\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    \n",
    "    # Perform cross-validation and calculate accuracy scores for each fold\n",
    "    # scoring=\"accuracy\" specifies that the evaluation metric is accuracy\n",
    "    # n_jobs=-1: use all available CPUs to speed up the computation\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # Return the array of accuracy scores from each fold\n",
    "    return scores\n",
    "\n",
    "# Define the dataset by calling the function to load or generate it\n",
    "X, y = get_dataset()\n",
    "\n",
    "# Get the models to evaluate by calling a function that returns a dictionary of model names and their respective model objects\n",
    "models = get_models()\n",
    "\n",
    "# Initialize empty lists to store the evaluation results and model names\n",
    "results, names = list(), list()\n",
    "\n",
    "# Loop through each model and evaluate it\n",
    "for name, model in models.items():\n",
    "    # Evaluate the current model using the dataset X and y, and store the scores\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    \n",
    "    # Append the scores to the results list\n",
    "    results.append(scores)\n",
    "    \n",
    "    # Append the model's name to the names list\n",
    "    names.append(name)\n",
    "    \n",
    "    # Print the model's name along with its mean and standard deviation of the evaluation scores\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ranges such as 10-90 and 15-85 perform better than the default\n",
    "of 25-75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a boxplot to compare the performance of different models\n",
    "\n",
    "# 'results' contains the performance data for each model (e.g., accuracy scores, etc.)\n",
    "# 'names' is a list of the model names corresponding to the performance results\n",
    "# The 'boxplot' function is used to create the boxplot for comparing results visually\n",
    "pyplot.boxplot(results, showmeans=True)  # Plot the boxplot with showing means\n",
    "\n",
    "# Set the x-axis labels using 'xticklabels'\n",
    "pyplot.xticks(ticks=range(1, len(names) + 1), labels=names)  # Set the model names on the x-axis\n",
    "\n",
    "# Display the plot\n",
    "pyplot.show()  # Show the generated plot to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box and whisker plots are created to summarize the classification accuracy scores for each\n",
    "IQR range. We can see a subtle difference in the distribution and mean accuracy with the larger\n",
    "ranges of 15-85 vs 25-75. percentiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Robust scaling provides an effective way to standardize numerical variables when outliers are present. Different scaling ranges (like 10-90 or 15-85 percentiles) can outperform the default 25-75 percentile range, showing the importance of testing different scaling parameters.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
