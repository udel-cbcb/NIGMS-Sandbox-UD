{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Data with Outliers\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Many machine learning algorithms perform better when numerical input variables are scaled to a standard range. This is particularly important for:\n",
    "\n",
    "- Algorithms that use **a weighted sum of inputs**, such as linear regression.\n",
    "- Algorithms that rely on **distance measures**, such as k-nearest neighbors (k-NN).\n",
    "\n",
    "#### Standardization: A Common Scaling Technique\n",
    "\n",
    "Standardization is a widely used scaling technique that transforms the probability distribution of an input variable into a standard Gaussian distribution (zero mean and unit variance). It involves:\n",
    "\n",
    "1. Subtracting the mean from the values.\n",
    "2. Dividing by the standard deviation.\n",
    "\n",
    "Mathematically, standardization is represented as:\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "where:\n",
    "- $x$ is the original value,\n",
    "- $\\mu$ is the mean of the input variable,\n",
    "- $\\sigma$ is the standard deviation of the input variable,\n",
    "- $z$ is the standardized value.\n",
    "\n",
    "#### Limitations of Standardization\n",
    "\n",
    "Standardization can become skewed or biased if the input variable contains **outlier values**. Outliers can significantly affect the mean and standard deviation, leading to suboptimal scaling.\n",
    "\n",
    "#### Robust Scaling: An Alternative Approach\n",
    "\n",
    "To address the limitations of standardization, **robust scaling** can be used. This technique is less sensitive to outliers and involves:\n",
    "\n",
    "1. Subtracting the median from the values.\n",
    "2. Dividing by the interquartile range (IQR).\n",
    "\n",
    "We will describe the **robust scaling** in details.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Standardization** is a common scaling technique that transforms data to have zero mean and unit variance but can be skewed by outliers.\n",
    "- **Robust scaling** uses the median and interquartile range, making it more suitable for datasets with outliers.\n",
    "- Choosing the right scaling technique depends on the characteristics of your data and the machine learning algorithm being used.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand why many machine learning algorithms prefer scaled numerical input variables\n",
    "- Learn robust scaling techniques that use percentiles to scale numerical input variables containing outliers\n",
    "- Master using `RobustScaler` to scale numerical input variables using median and interquartile range\n",
    "- Evaluate the impact of different scaling ranges on model performance\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Examine raw dataset distributions\n",
    "- Apply robust scaling transformation\n",
    "- Evaluate model performance with different scaling ranges\n",
    "- Compare results between scaled and unscaled data\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment\n",
    "- Basic understanding of Python programming concepts\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages and import necessary libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python libraries using pip\n",
    "%pip install matplotlib numpy pandas scikit-learn  \n",
    "\n",
    "# matplotlib - A library for creating static, animated, and interactive visualizations in Python.\n",
    "# numpy - A fundamental package for numerical computing, providing support for arrays and mathematical operations.\n",
    "# pandas - A powerful data analysis and manipulation library, useful for working with structured data.\n",
    "# scikit-learn - A machine learning library that provides simple and efficient tools for data mining and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from matplotlib import pyplot  # For plotting graphs\n",
    "from numpy import mean, std  # For calculating mean and standard deviation\n",
    "from pandas import DataFrame, read_csv  # For handling data as DataFrames and reading CSV files\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score  # For cross-validation\n",
    "from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors classifier\n",
    "from sklearn.pipeline import Pipeline  # To create machine learning pipelines\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler  # For data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaling Data\n",
    "\n",
    "When working with machine learning algorithms, input variables with **very large values** relative to other variables can dominate or skew the model's behavior. This happens because algorithms may focus excessively on the variables with larger values, effectively ignoring those with smaller values. This imbalance can lead to suboptimal model performance.\n",
    "\n",
    "Additionally, **outliers** in the data can further complicate the scaling process. Outliers can distort the probability distribution of the data, making traditional scaling techniques like standardization less effective. Standardization relies on the mean and standard deviation, both of which can be heavily influenced by outliers, resulting in skewed scaling.\n",
    "\n",
    "### Robust Scaling: A Solution for Outliers and Large Values\n",
    "\n",
    "To address these issues, **robust scaling** is a preferred alternative. Robust scaling uses the **median (50th percentile)** and the **interquartile range (IQR)** to scale the data. The IQR is the difference between the 75th percentile (Q3) and the 25th percentile (Q1). \n",
    "\n",
    "The steps for robust scaling are as follows:\n",
    "1. Subtract the **median** from each value.\n",
    "2. Divide the result by the **IQR**.\n",
    "\n",
    "Mathematically, robust scaling can be expressed as:\n",
    "\n",
    "$\n",
    "z = \\frac{x - \\text{median}}{\\text{IQR}}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $x$ is the original value,\n",
    "- $\\text{median}$ is the median of the input variable,\n",
    "- $\\text{IQR}$ is the interquartile range (Q3 - Q1),\n",
    "- $z$ is the robustly scaled value.\n",
    "\n",
    "### Advantages of Robust Scaling\n",
    "- **Resilience to Outliers**: Since the median and IQR are less sensitive to extreme values, robust scaling is more effective for datasets containing outliers.\n",
    "- **Balanced Scaling**: It ensures that no single variable dominates the model due to its scale, leading to better performance for algorithms sensitive to input variable ranges.\n",
    "\n",
    "By using robust scaling, you can ensure that your data is appropriately normalized, even in the presence of outliers or variables with large value ranges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient data based on whether they experienced the onset of diabetes within five years or not.\n",
    "\n",
    "### Dataset Overview\n",
    "- **Number of Instances**: 768\n",
    "- **Number of Attributes**: 8 (all numeric-valued) plus a class variable.\n",
    "\n",
    "### Attributes Description\n",
    "1. **Number of times pregnant**\n",
    "2. **Plasma glucose concentration** (measured 2 hours after an oral glucose tolerance test)\n",
    "3. **Diastolic blood pressure** (mm Hg)\n",
    "4. **Triceps skinfold thickness** (mm)\n",
    "5. **2-Hour serum insulin** (mu U/ml)\n",
    "6. **Body mass index** (weight in kg/(height in m)^2)\n",
    "7. **Diabetes pedigree function** (a function that scores the likelihood of diabetes based on family history)\n",
    "8. **Age** (years)\n",
    "9. **Class variable** (0 or 1, where 1 indicates a positive test for diabetes)\n",
    "\n",
    "### Missing Attribute Values\n",
    "- The dataset contains missing attribute values.\n",
    "\n",
    "### Class Distribution\n",
    "- **Class Value 0**: 500 instances (interpreted as \"tested negative for diabetes\")\n",
    "- **Class Value 1**: 268 instances (interpreted as \"tested positive for diabetes\")\n",
    "\n",
    "### Additional Resources\n",
    "- **Dataset File**: [pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv)\n",
    "- **Dataset Details**: [pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and summarize the diabetes dataset\n",
    "# Define the path to the dataset\n",
    "pima_indians_diabetes_csv = \"../../Data/pima-indians-diabetes.csv\"  \n",
    "\n",
    "# Define the dataset file path (Ensure 'pima_indians_diabetes_csv' is correctly defined)\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)  # Load dataset without headers\n",
    "print(dataset.head())  # Display the first five rows of the dataset\n",
    "\n",
    "# Summarize the shape of the dataset\n",
    "print(dataset.shape)  # Print the number of rows and columns in the dataset\n",
    "\n",
    "# Summarize each variable (statistical summary)\n",
    "print(dataset.describe())  # Display summary statistics (count, mean, std, min, max, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Summary\n",
    "\n",
    "The dataset consists of:\n",
    "- **8** input variables\n",
    "- **1** output variable\n",
    "- **768** rows of data\n",
    "\n",
    "A statistical summary of the input variables reveals that each variable operates on a **very different scale**. This characteristic makes the dataset an excellent candidate for exploring **data scaling methods**.\n",
    "\n",
    "### Visualizing Input Variables\n",
    "\n",
    "To better understand the dataset, we can create a **histogram for each input variable**. These plots highlight two key observations:\n",
    "1. **Differing Scales**: Each input variable has a distinct range and scale.\n",
    "2. **Presence of Outliers**: Some distributions exhibit outliers, which can skew the data.\n",
    "\n",
    "### Implications for Data Scaling\n",
    "\n",
    "Given the **varying scales** and the **presence of outliers**, the dataset is well-suited for applying a **robust scaler transform**. This method is particularly effective for standardizing data when:\n",
    "- Input variables have significantly different scales.\n",
    "- Outliers are present, as robust scaling is less sensitive to extreme values compared to traditional standardization.\n",
    "\n",
    "\n",
    "By using a robust scaler, we can ensure that the data is appropriately normalized, leading to better performance in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate histograms for all variables in the dataset\n",
    "# xlabelsize and ylabelsize control the font size of axis labels\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "\n",
    "# Reduce the title size of each subplot for better readability\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# Display the histogram plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit and evaluate a machine learning model on the raw dataset. We will use\n",
    "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
    "stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate k-Nearest Neighbors (KNN) on the raw Pima Indians Diabetes dataset\n",
    "\n",
    "# Load dataset from CSV file (Assuming 'pima_indians_diabetes_csv' is defined elsewhere)\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Convert the dataset into a NumPy array\n",
    "data = dataset.values\n",
    "\n",
    "# Separate the dataset into input features (X) and output labels (y)\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# Ensure input features are of type float (for consistency in calculations)\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "# Encode the output labels as integers (necessary for classification models)\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# Define the KNN model with default parameters\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Set up cross-validation with 10 splits, repeated 3 times for robust evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Perform cross-validation and compute accuracy scores across different folds\n",
    "n_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report the model's mean accuracy and standard deviation across all cross-validation runs\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "# Show any plots (though no plots are explicitly generated in this code)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the model achieved a mean classi cationfiaccuracy of about\n",
    "71.7 percent.\n",
    "\n",
    "Next, let's explore a robust scaling transform of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQR Robust Scaler Transform\n",
    "\n",
    "We can apply the **Robust Scaler** to the diabetes dataset directly. This method scales the data using the **Interquartile Range (IQR)**, making it robust to outliers. Here's how it works:\n",
    "\n",
    "1. **Define the RobustScaler Instance**:\n",
    "   - A `RobustScaler` object is created with default hyperparameters.\n",
    "\n",
    "2. **Apply the Transform**:\n",
    "   - The `fit_transform()` function is called on the dataset. This function:\n",
    "     - Computes the median and IQR for each feature.\n",
    "     - Scales the data based on these values.\n",
    "\n",
    "3. **Output**:\n",
    "   - The result is a transformed version of the dataset where the values are scaled to the IQR, ensuring robustness to outliers and varying scales.\n",
    "\n",
    "This approach is particularly useful for datasets like the diabetes dataset, where input variables have differing scales and may contain outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pima Indians Diabetes dataset from a CSV file\n",
    "# `header=None` ensures that no row is treated as column names\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Extract only the numeric input features (excluding the target variable in the last column)\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# Initialize the RobustScaler, which scales features using statistics \n",
    "# that are robust to outliers (i.e., median and interquartile range)\n",
    "trans = RobustScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it to remove the influence of outliers\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# Convert the transformed NumPy array back into a Pandas DataFrame for better readability\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# Display summary statistics of the transformed dataset\n",
    "# This includes count, mean, standard deviation, min, and percentile values\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the\n",
    "distributions have been adjusted. The median values are now zero and the standard deviation\n",
    "values are now close to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate histograms for all variables in the dataset\n",
    "# xlabelsize and ylabelsize adjust the font size of axis labels for readability\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "\n",
    "# Reduce the title font size for each subplot in the figure\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# Display the histogram plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram plots of the variables are created, although the distributions don't look much\n",
    "different from their original distributions seen in the previous section. We can see that the\n",
    "center of mass for each distribution is now close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
    "robust scaler transform of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-Nearest Neighbors (KNN) on the Pima Indians Diabetes dataset \n",
    "# using RobustScaler for data preprocessing\n",
    "\n",
    "# Load the dataset from CSV file\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)  # Replace with actual file path\n",
    "data = dataset.values  # Convert dataframe to NumPy array for processing\n",
    "\n",
    "# Separate the dataset into input features (X) and target labels (y)\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# Convert input features to float type and encode target labels as integers\n",
    "X = X.astype(\"float32\")  \n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))  # Ensures target labels are numerically encoded\n",
    "\n",
    "# Define a preprocessing and modeling pipeline\n",
    "trans = RobustScaler()  # Use RobustScaler to handle outliers and normalize features\n",
    "model = KNeighborsClassifier()  # Initialize KNN classifier\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])  # Create a pipeline with scaling and classification\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)  \n",
    "# This splits data into 10 folds and repeats the process 3 times to ensure robust evaluation\n",
    "\n",
    "# Evaluate the pipeline using cross-validation and compute accuracy scores\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report mean accuracy and standard deviation of scores\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "# Show any plots (though no plot is explicitly created in this code)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the robust scaler transform results in a lift in\n",
    "performance from 71.7 percent accuracy without the transform to about 73.4 percent with the\n",
    "transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Robust Scaler Range\n",
    "\n",
    "By default, the **Robust Scaler** uses the **Interquartile Range (IQR)** to scale each variable. The IQR is bounded by the **25th percentile (Q1)** and the **75th percentile (Q3)**. This range is specified by the `quantile_range` argument as a tuple (e.g., `(0.25, 0.75)`).\n",
    "\n",
    "### Customizing the Range\n",
    "The range can be adjusted to potentially improve model performance:\n",
    "- **Wider Range**: Includes more data points, reducing the number of values considered outliers.\n",
    "- **Narrower Range**: Excludes more data points, increasing the number of values considered outliers.\n",
    "\n",
    "### Example: Exploring Different Ranges\n",
    "The example below demonstrates the effect of varying the range from:\n",
    "- **1st to 99th** percentiles (very wide range, minimizing outliers)\n",
    "- **30th to 70th** percentiles (narrow range, maximizing outliers)\n",
    "\n",
    "By experimenting with different ranges, you can fine-tune the scaling process to better suit your dataset and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the scaling range of the robust scaler transform\n",
    "\n",
    "# Get the dataset\n",
    "def get_dataset():\n",
    "    # Load the dataset from the specified CSV file without a header row\n",
    "    dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "    \n",
    "    # Convert the dataset into a numpy array\n",
    "    data = dataset.values\n",
    "    \n",
    "    # Separate the dataset into input features (X) and output labels (y)\n",
    "    X, y = data[:, :-1], data[:, -1]  # Inputs (all columns except the last), Outputs (last column)\n",
    "    \n",
    "    # Ensure that the input features are of type float32 (for better performance)\n",
    "    X = X.astype(\"float32\")\n",
    "    \n",
    "    # Convert the output labels to integers using LabelEncoder (for classification tasks)\n",
    "    y = LabelEncoder().fit_transform(y.astype(\"str\"))  # Convert to string and then encode to integers\n",
    "    \n",
    "    # Return the prepared input and output data\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "# Get a list of models to evaluate\n",
    "def get_models():\n",
    "    # Create an empty dictionary to store the models\n",
    "    models = dict()\n",
    "    \n",
    "    # Iterate over a list of values to create different models\n",
    "    for value in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        # Define the transformation step using RobustScaler with quantile_range based on 'value'\n",
    "        trans = RobustScaler(quantile_range=(value, 100 - value))\n",
    "        \n",
    "        # Define the classifier model using KNeighborsClassifier\n",
    "        model = KNeighborsClassifier()\n",
    "        \n",
    "        # Store the model in the dictionary, where the key is the string representation of 'value'\n",
    "        # The pipeline includes both the transformation step and the model step\n",
    "        models[str(value)] = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "    \n",
    "    # Return the dictionary of models\n",
    "    return models\n",
    "\n",
    "# Evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # Create a Repeated Stratified K-Fold cross-validation object\n",
    "    # n_splits=10: 10-fold cross-validation (split data into 10 parts)\n",
    "    # n_repeats=3: repeat the process 3 times to get more robust results\n",
    "    # random_state=1: ensures reproducibility of results by fixing the random seed\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    \n",
    "    # Perform cross-validation and calculate accuracy scores for each fold\n",
    "    # scoring=\"accuracy\" specifies that the evaluation metric is accuracy\n",
    "    # n_jobs=-1: use all available CPUs to speed up the computation\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # Return the array of accuracy scores from each fold\n",
    "    return scores\n",
    "\n",
    "# Define the dataset by calling the function to load or generate it\n",
    "X, y = get_dataset()\n",
    "\n",
    "# Get the models to evaluate by calling a function that returns a dictionary of model names and their respective model objects\n",
    "models = get_models()\n",
    "\n",
    "# Initialize empty lists to store the evaluation results and model names\n",
    "results, names = list(), list()\n",
    "\n",
    "# Loop through each model and evaluate it\n",
    "for name, model in models.items():\n",
    "    # Evaluate the current model using the dataset X and y, and store the scores\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    \n",
    "    # Append the scores to the results list\n",
    "    results.append(scores)\n",
    "    \n",
    "    # Append the model's name to the names list\n",
    "    names.append(name+\"-\"+str(100-int(name)))\n",
    "    \n",
    "    # Print the model's name along with its mean and standard deviation of the evaluation scores\n",
    "    print(\">%s-%s %.3f (%.3f)\" % (name, 100-int(name), mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ranges such as 10-90 and 15-85 perform better than the default\n",
    "of 25-75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a boxplot to compare the performance of different models\n",
    "\n",
    "# 'results' contains the performance data for each model (e.g., accuracy scores, etc.)\n",
    "# 'names' is a list of the model names corresponding to the performance results\n",
    "# The 'boxplot' function is used to create the boxplot for comparing results visually\n",
    "pyplot.boxplot(results, showmeans=True)  # Plot the boxplot with showing means\n",
    "\n",
    "# Add x-axis and y-axis labels\n",
    "pyplot.xlabel(\"Model\")\n",
    "pyplot.ylabel(\"Mean Accuracy\")\n",
    "\n",
    "# Set the x-axis labels using 'xticklabels'\n",
    "pyplot.xticks(ticks=range(1, len(names) + 1), labels=names)  # Set the model names on the x-axis\n",
    "\n",
    "# Display the plot\n",
    "pyplot.show()  # Show the generated plot to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating IQR Range Impact\n",
    "\n",
    "To assess the effect of different IQR ranges on model performance, **box and whisker plots** were created to summarize the classification accuracy scores for each range. The plots reveal:\n",
    "\n",
    "- **Subtle Differences**: There is a slight variation in the **distribution** and **mean accuracy** between different IQR ranges.\n",
    "- **Comparison of Ranges**:\n",
    "  - **15th to 85th** Percentiles: A wider range that includes more data points, potentially reducing the impact of outliers.\n",
    "  - **25th to 75th** Percentiles: The default IQR range, which is narrower and may classify more values as outliers.\n",
    "\n",
    "These results suggest that adjusting the IQR range can influence model performance, though the differences may be subtle. Further experimentation with additional ranges could help identify the optimal configuration for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Robust scaling provides an effective way to standardize numerical variables when outliers are present. Different scaling ranges (like 10-90 or 15-85 percentiles) can outperform the default 25-75 percentile range, showing the importance of testing different scaling parameters.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
