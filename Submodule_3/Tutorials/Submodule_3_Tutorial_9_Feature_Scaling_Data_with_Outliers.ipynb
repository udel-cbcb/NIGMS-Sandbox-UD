{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Data with Outliers\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Many machine learning algorithms perform better when numerical input variables are scaled to\n",
    "a standard range. This includes algorithms that use a weighted sum of the input, like linear\n",
    "regression, and algorithms that use distance measures, like k-nearest neighbors. Standardizing\n",
    "is a popular scaling technique that subtracts the mean from values and divides by the standard\n",
    "deviation, transforming the probability distribution for an input variable to a standard Gaussian\n",
    "(zero mean and unit variance). Standardization can become skewed or biased if the input\n",
    "variable contains outlier values.\n",
    "To overcome this, the median and interquartile range can be used when standardizing\n",
    "numerical input variables, generally referred to as robust scaling.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand why many machine learning algorithms prefer scaled numerical input variables\n",
    "- Learn robust scaling techniques that use percentiles to scale numerical input variables containing outliers\n",
    "- Master using `RobustScaler` to scale numerical input variables using median and interquartile range\n",
    "- Evaluate the impact of different scaling ranges on model performance\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Examine raw dataset distributions\n",
    "- Apply robust scaling transformation\n",
    "- Evaluate model performance with different scaling ranges\n",
    "- Compare results between scaled and unscaled data\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment\n",
    "- Basic understanding of Python programming concepts\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages and import necessary libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from numpy import mean, std\n",
    "from pandas import DataFrame, read_csv\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "\n",
    "pima_indians_diabetes_csv = \"../../Data/pima-indians-diabetes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaling Data\n",
    "\n",
    "If there are input\n",
    "variables that have very large values relative to the other input variables, these large values\n",
    "can dominate or skew some machine learning algorithms. The result is that the algorithms pay\n",
    "most of their attention to the large values and ignore the variables with smaller values.\n",
    "\n",
    "Sometimes an input variable may have outlier values. Outliers can skew a probability distribution and make data scaling using standardization\n",
    "difficult as the calculated mean and standard deviation will be skewed by the presence of\n",
    "the outliers. \n",
    "\n",
    "Robust standardization or robust data\n",
    "scaling that calculates the median (50th percentile) and the 25th and\n",
    "75th percentiles. The values of each variable then have their median subtracted and are divided\n",
    "by the interquartile range (IQR) which is the difference between the 75th and 25th percentiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient data as\n",
    "either an onset of diabetes within five years or not.\n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class \n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the diabetes dataset\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "print(dataset.head())\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# summarize each variable\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the 8\n",
    "input variables, one output variable, and 768 rows of data. A statistical summary of the input\n",
    "variables is provided show that each variable has a very different scale. This makes it a good\n",
    "dataset for exploring data scaling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a histogram for each input variable. The plots confirm the differing scale\n",
    "for each input variable and show that the variables have differing scales. Importantly, we can see\n",
    "some of the distributions show the presence of outliers. The dataset provides a good candidate for using a robust scaler transform to standardize the data in the presence of di ering input\n",
    "variable scales and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit and evaluate a machine learning model on the raw dataset. We will use\n",
    "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
    "stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the raw diabetes dataset\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype(\"float32\")\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define and configure the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the model achieved a mean classi cationfiaccuracy of about\n",
    "71.7 percent.\n",
    "\n",
    "Next, let's explore a robust scaling transform of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQR Robust Scaler Transform\n",
    "\n",
    "We can apply the robust scaler to the diabetes dataset directly. We will use the default\n",
    "configuration and scale values to the IQR. First, a **RobustScaler** instance is defined with\n",
    "default hyperparameters. Once defined, we can call the **fit.transform()** function and pass it\n",
    "to our dataset to create a robust scale transformed version of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a robust scaler transform of the diabetes dataset\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a robust scaler transform of the dataset\n",
    "trans = RobustScaler()\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# summarize\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the\n",
    "distributions have been adjusted. The median values are now zero and the standard deviation\n",
    "values are now close to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram plots of the variables are created, although the distributions don't look much\n",
    "different from their original distributions seen in the previous section. We can see that the\n",
    "center of mass for each distribution is now close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
    "robust scaler transform of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the diabetes dataset with robust scaler transform\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype(\"float32\")\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define the pipeline\n",
    "trans = RobustScaler()\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report pipeline performance\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the robust scaler transform results in a lift in\n",
    "performance from 71.7 percent accuracy without the transform to about 73.4 percent with the\n",
    "transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Robust Scaler Range\n",
    "\n",
    "The range used to scale each variable is chosen by default as the IQR is bounded by the 25th\n",
    "and 75th percentiles. This is specified by the quantile range argument as a tuple. Other\n",
    "values can be specified and might improve the performance of the model, such as a wider range,\n",
    "allowing fewer values to be considered outliers, or a more narrow range, allowing more values\n",
    "to be considered outliers. The example below explores the effect of different definitions of the\n",
    "range from 1st to the 99th percentiles to 30th to 70th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the scaling range of the robust scaler transform\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    # load dataset\n",
    "    dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "    data = dataset.values\n",
    "    # separate into input and output columns\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    # ensure inputs are floats and output is an integer label\n",
    "    X = X.astype(\"float32\")\n",
    "    y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for value in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        # define the pipeline\n",
    "        trans = RobustScaler(quantile_range=(value, 100 - value))\n",
    "        model = KNeighborsClassifier()\n",
    "        models[str(value)] = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "    return models\n",
    "\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ranges such as 10-90 and 15-85 perform better than the default\n",
    "of 25-75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, tick_labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box and whisker plots are created to summarize the classification accuracy scores for each\n",
    "IQR range. We can see a subtle difference in the distribution and mean accuracy with the larger\n",
    "ranges of 15-85 vs 25-75. percentiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Robust scaling provides an effective way to standardize numerical variables when outliers are present. Different scaling ranges (like 10-90 or 15-85 percentiles) can outperform the default 25-75 percentile range, showing the importance of testing different scaling parameters.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
