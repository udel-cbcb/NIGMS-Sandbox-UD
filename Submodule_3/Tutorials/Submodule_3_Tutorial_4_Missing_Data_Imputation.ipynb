{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da5a86d-687e-4976-8dad-3111fbaf484d",
   "metadata": {},
   "source": [
    "# Missing Data Imputation\n",
    "\n",
    "This module will demonstrate missing data imputation techniques.\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Datasets may have missing values, and this can cause problems for many machine learning\n",
    "algorithms. As such, it is good practice to identify and replace missing values for each column in\n",
    "your input data prior to modeling your prediction task. This is called missing data imputation,\n",
    "or imputing for short.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Learn how to load a CSV file with missing values and mark the missing values with NaN values\n",
    "and report the number and percentage of missing values for each column.\n",
    "* Learn how to impute missing values with statistics as a data preparation method when evaluating models and when fitting a final model to make predictions on new data.\n",
    "* Learn how to impute missing values with nearest neighbor models as a data preparation method when evaluating models and when fitting a final model to make predictions on new data.\n",
    "* Learn how to impute missing values with iterative imputation models as a data preparation method when evaluating models and when fitting a final model to make predictions on new data.\n",
    "\n",
    "### Tasks to complete:\n",
    "\n",
    "- Load and analyze dataset with missing values\n",
    "- Implement statistical imputation methods\n",
    "- Implement k-nearest neighbor imputation\n",
    "- Implement iterative imputation\n",
    "- Compare performance of different imputation approaches\n",
    "- Make predictions with imputed data\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python programming\n",
    "- Familiarity with NumPy libraries\n",
    "- Knowledge of basic statistical concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a41bd-fce9-4d6d-92a6-4dddbfa3f5c2",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages and import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d16f3-1aee-4178-9a98-849f4ad49f50",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58f577-2e49-4d26-b67e-7489bb51571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install matplotlib, numpy, pandas, and scikit-learn libraries using pip package manager.\n",
    "%pip install matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bccac7-8750-4dbb-8671-8da794745a74",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c27b9-5d83-4138-aafe-5272ecf79e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Path class for object-oriented file path manipulation.\n",
    "from pathlib import Path\n",
    "# Import the pyplot module from matplotlib for plotting and visualization.\n",
    "from matplotlib import pyplot\n",
    "# Import functions from numpy for numerical operations, including handling NaN values and calculating mean and standard deviation.\n",
    "from numpy import isnan, mean, nan, std\n",
    "# Import the read_csv function from pandas to read CSV files into DataFrames.\n",
    "from pandas import read_csv\n",
    "# Import the RandomForestClassifier class from sklearn.ensemble for building a Random Forest classification model.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import the enable_iterative_imputer function to enable experimental iterative imputer in scikit-learn.\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# Import the IterativeImputer class for imputing missing values using an iterative approach.\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "# Import classes and functions from sklearn.model_selection for model evaluation, including cross-validation and stratified k-fold.\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "# Import the Pipeline class from sklearn.pipeline to construct a pipeline of data preprocessing and modeling steps.\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06607d-3996-4171-8a3b-84a0529a608a",
   "metadata": {},
   "source": [
    "## Hepatitis Dataset\n",
    "\n",
    "### Overview\n",
    "The **Hepatitis dataset** is a medical dataset from the UCI Machine Learning Repository. It contains patient data related to hepatitis, which can be used for classification tasks, such as predicting patient survival.\n",
    "\n",
    "### Data Characteristics\n",
    "- **Number of Instances:** 155 patients\n",
    "- **Number of Attributes:** 20 (including the class label)\n",
    "- **Attribute Types:** Categorical and numerical\n",
    "- **Missing Values:** Present in some attributes\n",
    "\n",
    "### Attribute Information\n",
    "1. **Class (Target Variable)**: Die (1), Live (2)  \n",
    "2. **Age**: Patient's age in years  \n",
    "3. **Sex**: Male (1), Female (2)  \n",
    "4. **Steroid**: Yes (1), No (2)  \n",
    "5. **Antivirals**: Yes (1), No (2)  \n",
    "6. **Fatigue**: Yes (1), No (2)  \n",
    "7. **Malaise**: Yes (1), No (2)  \n",
    "8. **Anorexia**: Yes (1), No (2)  \n",
    "9. **Liver Big**: Yes (1), No (2)  \n",
    "10. **Liver Firm**: Yes (1), No (2)  \n",
    "11. **Spleen Palpable**: Yes (1), No (2)  \n",
    "12. **Spiders**: Yes (1), No (2)  \n",
    "13. **Ascites**: Yes (1), No (2)  \n",
    "14. **Varices**: Yes (1), No (2)  \n",
    "15. **Bilirubin**: Continuous (0.39 – 8.0)\n",
    "16. **Alkaline Phosphatase (ALP)**: Continuous (33 – 250)\n",
    "17. **SGOT (AST)**: Continuous (13 – 600)\n",
    "18. **Albumin**: Continuous (2.1 – 6.0)\n",
    "19. **Protime**: Continuous (10 – 100)\n",
    "20. **Histology**: Yes (1), No (2)  \n",
    "\n",
    "### Usage\n",
    "This dataset is used in medical research and machine learning to study hepatitis progression and survival prediction.\n",
    "\n",
    "### Source\n",
    "- UCI Machine Learning Repository: [Hepatitis Dataset](https://archive.ics.uci.edu/dataset/46/hepatitis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f03b0-864f-4914-9288-1af72c26e5e0",
   "metadata": {},
   "source": [
    "### Loading and summarizing a dataset with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d7ccd-f72e-4c7d-9655-34b8904f4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifies the file path to the hepatitis dataset.\n",
    "hepatitis_data = \"../../Data/hepatitis.data\"\n",
    "\n",
    "# Defines a list of column names for the hepatitis dataset, based on 'hepatitis.names' file.\n",
    "columns = [\n",
    "    'class',       # 1=Die, 2=Live (TARGET)\n",
    "    'age',\n",
    "    'sex',         # 1=male, 2=female\n",
    "    'steroid',     # 1=no, 2=yes\n",
    "    'antivirals',  # 1=no, 2=yes\n",
    "    'fatigue',\n",
    "    'malaise',\n",
    "    'anorexia',\n",
    "    'liver_big',\n",
    "    'liver_firm',\n",
    "    'spleen_palpable',\n",
    "    'spiders',\n",
    "    'ascites',\n",
    "    'varices',\n",
    "    'bilirubin',\n",
    "    'alk_phosphate',\n",
    "    'sgot',\n",
    "    'albumin',\n",
    "    'protime',\n",
    "    'histology'\n",
    "]\n",
    "\n",
    "# Loads the hepatitis dataset from the specified file.\n",
    "df = read_csv(\n",
    "    hepatitis_data, # Specifies the file path.\n",
    "    header=None, # Indicates the file has no header row.\n",
    "    names=columns, # Assigns the defined column names to the DataFrame.\n",
    "    na_values=\"?\", # Treats '?' as missing values and converts them to NaN.\n",
    "    sep=',' # Specifies that the data is comma-separated.\n",
    ")\n",
    "\n",
    "# Prints a header to indicate the following output is the first 3 rows of the DataFrame.\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "# Prints the first 3 rows of the DataFrame to display sample data.\n",
    "print(df.head(3))\n",
    "\n",
    "# Prints a header to indicate the following output is missing value analysis per column.\n",
    "print(\"\\nMissing values per column:\")\n",
    "# Iterates through each column name in the 'columns' list.\n",
    "for col in columns:\n",
    "    # Calculates the number of missing values (NaN) in the current column.\n",
    "    n_miss = df[col].isnull().sum()\n",
    "    # Calculates the percentage of missing values in the current column.\n",
    "    perc = n_miss / len(df) * 100\n",
    "    # Prints the column name, the count of missing values, and the percentage of missing values, formatted for readability.\n",
    "    print(f\"> {col:15}: {n_miss:2} ({perc:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a61935-0df6-4313-9309-4dd02a2ec973",
   "metadata": {},
   "source": [
    "### Split data into input and output elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c3aa0-773d-4b80-a62e-817bcdac6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (class is first column)\n",
    "data = df.values\n",
    "X = data[:, 1:]  # Features (columns 1-19)\n",
    "y = data[:, 0]    # Target (column 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94cf29-65c9-4996-85d2-d4b569becd59",
   "metadata": {},
   "source": [
    "### Define evaluation workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add51b6d-65a9-416a-9824-80ab9cf059fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called 'evaluate_imputation' that takes an imputer object as input.\n",
    "def evaluate_imputation(imputer):\n",
    "    # Create a pipeline that first applies the given imputer and then trains a RandomForestClassifier model.\n",
    "    pipeline = Pipeline(steps=[('imputer', imputer),\n",
    "                              ('model', RandomForestClassifier())])\n",
    "    # Define cross-validation strategy using RepeatedStratifiedKFold for robust evaluation.\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # Perform cross-validation using the pipeline, features X, target y, 'accuracy' scoring, and the defined cross-validation strategy.\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='accuracy',\n",
    "                            cv=cv, n_jobs=-1)\n",
    "    # Return the scores obtained from cross-validation.\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca556d9e-f5ee-4eb8-b33e-350b92100977",
   "metadata": {},
   "source": [
    "### Test imputation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a105daa-7d95-4a4f-9c09-b577ba54f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary 'strategies' to hold different imputation methods.\n",
    "strategies = {\n",
    "    # Key 'Mean': Uses SimpleImputer to replace missing values with the mean of the column.\n",
    "    'Mean': SimpleImputer(strategy='mean'),\n",
    "    # Key 'KNN-3': Uses KNNImputer to impute missing values based on 3 nearest neighbors.\n",
    "    'KNN-3': KNNImputer(n_neighbors=3),\n",
    "    # Key 'Iterative': Uses IterativeImputer for more complex imputation, allowing up to 10 iterations.\n",
    "    'Iterative': IterativeImputer(max_iter=10)\n",
    "}\n",
    "\n",
    "# Print a header to indicate the start of strategy performance evaluation.\n",
    "print(\"\\nStrategy Performance:\")\n",
    "# Initialize an empty dictionary 'results' to store the performance scores for each strategy.\n",
    "results = {}\n",
    "# Loop through each imputation strategy defined in the 'strategies' dictionary.\n",
    "for name, imp in strategies.items():\n",
    "    # Call the function 'evaluate_imputation' (assumed to be defined elsewhere) to evaluate the current imputation strategy 'imp'.\n",
    "    scores = evaluate_imputation(imp)\n",
    "    # Store the returned scores in the 'results' dictionary with the strategy 'name' as the key.\n",
    "    results[name] = scores\n",
    "    # Print the name of the imputation strategy and its average accuracy and standard deviation (assuming 'scores' is a list of accuracy scores).\n",
    "    print(f\"{name:10} | Accuracy: {mean(scores):.3f} (±{std(scores):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dffe4e-813d-4bd9-bbc6-286af088bb08",
   "metadata": {},
   "source": [
    "### Visualization of impacts of imputation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de98dd-9ddb-4623-a3f3-1b409a35b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "# Create a new figure for plotting with a specified size of 10x5 inches.\n",
    "pyplot.figure(figsize=(10, 5))\n",
    "# Generate a boxplot using data from 'results.values()' and labels from 'results.keys()', also display the mean values in the boxplot.\n",
    "pyplot.boxplot(results.values(), labels=results.keys(), showmeans=True)\n",
    "# Set the title of the plot to 'Hepatitis Survival Prediction - Imputation Comparison'.\n",
    "pyplot.title('Hepatitis Survival Prediction - Imputation Comparison')\n",
    "# Set the label for the y-axis to 'Accuracy'.\n",
    "pyplot.ylabel('Accuracy')\n",
    "# Display the generated plot.\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd747f35-f410-4b3c-ad68-36b55194a3ff",
   "metadata": {},
   "source": [
    "This analysis compares different imputation methods for handling missing values in the hepatitis dataset and evaluates their impact on model accuracy.\n",
    "\n",
    "### **Imputation Methods Compared:**\n",
    "- **Mean Imputation:** Missing values are replaced with the mean of the respective feature.\n",
    "- **KNN-3 Imputation:** Uses the K-Nearest Neighbors (KNN) method with `k=3` to impute missing values.\n",
    "- **Iterative Imputation:** Employs an iterative method, such as Multivariate Imputation by Chained Equations (MICE).\n",
    "\n",
    "### **Box Plot Interpretation:**\n",
    "- **X-Axis:** Represents different imputation methods.\n",
    "- **Y-Axis:** Accuracy of the survival prediction model.\n",
    "- **Box Components:**\n",
    "  - **Boxes:** Represent the interquartile range (IQR), covering the middle 50% of accuracy values.\n",
    "  - **Orange Line:** Median accuracy.\n",
    "  - **Green Triangle:** Mean accuracy.\n",
    "  - **Whiskers:** Show the spread of data, excluding outliers.\n",
    "  - **Circles (Outliers):** Unusual values that significantly differ from the rest.\n",
    "\n",
    "### **Insights:**\n",
    "- **Mean Imputation:** Shows a relatively stable accuracy distribution with a high median but includes an extreme outlier around **1.0**.\n",
    "- **KNN-3 Imputation:** Has a broader accuracy range, with a lower minimum accuracy compared to the other methods.\n",
    "- **Iterative Imputation:** Exhibits the highest median accuracy but also shows more variability with multiple outliers.\n",
    "\n",
    "The choice of imputation method significantly impacts model performance. The **Iterative approach** provides the best average accuracy but comes with greater variability. The **Mean method** is more stable, making it a safer option if avoiding outliers is a priority.\n",
    "\n",
    "Further analysis could involve testing additional imputation strategies or tuning hyperparameters for improved performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626bee8-14e1-4c8d-87b5-97099d7292ec",
   "metadata": {},
   "source": [
    "### Prediction example with realistic clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d250bc7-3b4b-45a4-b4bf-dfc34c40aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline for data preprocessing and model training.\n",
    "best_pipeline = Pipeline(steps=[\n",
    "    # Step 1: Impute missing values using IterativeImputer with a maximum of 10 iterations.\n",
    "    ('imputer', IterativeImputer(max_iter=10)),\n",
    "    # Step 2: Train a RandomForestClassifier model.\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Fit the entire pipeline (imputer and RandomForestClassifier) on the features (X) and target variable (y).\n",
    "best_pipeline.fit(X, y)\n",
    "\n",
    "# New patient data with realistic clinical measurements, including missing values represented as 'nan'.\n",
    "new_patient = [\n",
    "    34,     # age: 34 years\n",
    "    2,      # sex: female\n",
    "    2,      # steroid: yes\n",
    "    2,      # antivirals: yes\n",
    "    nan,    # fatigue: missing\n",
    "    2,      # malaise: yes\n",
    "    2,      # anorexia: yes\n",
    "    2,      # liver_big: yes\n",
    "    2,      # liver_firm: yes\n",
    "    2,      # spleen_palpable: yes\n",
    "    2,      # spiders: yes\n",
    "    nan,    # ascites: missing\n",
    "    2,      # varices: yes\n",
    "    1.8,    # bilirubin: 1.8 mg/dl\n",
    "    nan,    # alk_phosphate: missing\n",
    "    45,     # sgot: 45 U/ml\n",
    "    3.2,    # albumin: 3.2 mg/dl\n",
    "    nan,    # protime: missing\n",
    "    2       # histology: yes\n",
    "]\n",
    "\n",
    "# Predict the outcome (0 or 1) for the new patient using the trained pipeline, passing the new patient's data as a list within a list.\n",
    "pred = best_pipeline.predict([new_patient])\n",
    "# Print the predicted outcome, interpreting class 1 as 'Die' and class 0 as 'Live', and also print the raw class prediction (0 or 1).\n",
    "print(f\"\\nPredicted Outcome: {'Die' if pred[0] == 1 else 'Live'} (Class: {pred[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631f08d-72bf-4ae4-b9ba-fbd83cc94282",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this module, we learned several approaches for handling missing data:\n",
    "\n",
    "- Statistical imputation using mean, median, mode and constant values\n",
    "- K-nearest neighbor imputation with different numbers of neighbors\n",
    "- Iterative imputation with different ordering strategies and iterations\n",
    "- How to properly implement imputation in a machine learning pipeline\n",
    "- How to evaluate and compare different imputation methods\n",
    "- How to use imputation when making predictions on new data\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.￼"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
