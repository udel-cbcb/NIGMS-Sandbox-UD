{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data Imputation\n",
    "\n",
    "This module will demonstrate missing data imputation techniques.\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Datasets may have missing values, and this can cause problems for many machine learning\n",
    "algorithms. As such, it is good practice to identify and replace missing values for each column in\n",
    "your input data prior to modeling your prediction task. This is called missing data imputation,\n",
    "or imputing for short.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Learn how to load a CSV file with missing values and mark the missing values with NaN values\n",
    "and report the number and percentage of missing values for each column.\n",
    "* Learn how to impute missing values with statistics as a data preparation method when evaluating models and when fitting a final model to make predictions on new data.\n",
    "* Learn how to impute missing values with nearest neighbor models as a data preparation method when evaluating models and when fitting a final model to make predictions on new data.\n",
    "* Learn how to impute missing values with iterative imputation models as a data preparation method when evaluating models and when fitting a final model to make predictions on new data.\n",
    "\n",
    "### Tasks to complete:\n",
    "\n",
    "- Load and analyze dataset with missing values\n",
    "- Implement statistical imputation methods\n",
    "- Implement k-nearest neighbor imputation\n",
    "- Implement iterative imputation\n",
    "- Compare performance of different imputation approaches\n",
    "- Make predictions with imputed data\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python programming\n",
    "- Familiarity with NumPy libraries\n",
    "- Knowledge of basic statistical concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages and import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from numpy import isnan, mean, nan, std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Specify the data files\n",
    "horse_colic_data = \"../../Data/horse-colic.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horse Colic Dataset\n",
    "\n",
    "The horse colic dataset describes medical characteristics of horses with colic and whether they\n",
    "lived or died. There are 300 rows and 26 input variables with one output variable. It is a binary\n",
    "classification prediction task that involves predicting 1 if the horse lived and 2 if the horse died.\n",
    "There are many fields we could select to predict in this dataset. In this case, we will predict\n",
    "whether the problem was surgical or not (column index 23), making it a binary classification\n",
    "problem. The dataset has numerous missing values for many of the columns where each missing\n",
    "value is marked with a question mark character (\"\\?\"). \n",
    "\n",
    "Number of Instances: 368 (300 for training, 68 for testing)\n",
    "\n",
    "Number of attributes: 28\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "```\n",
    "1: Surgery, 2: Age, 3: Hospital Number, 4: Rectal Temperature, 5: Pulse, 6: Respiratory Rate, 7: Temperature of Extremities,\n",
    "8: Peripheral Pulse, 9: Mucous Membranes, 10: Capillary Refill Time, 11: Pain, 12: Peristalsis, 13: Abdominal Distension, \n",
    "14: Nasogastric Tube, 15: Nasogastric Reflux, 16: Nasogastric Reflux PH, 17: Rectal Examination, 18: Abdomen, \n",
    "19: Packed Cell Volume, 20: Total Protein, 21: Abdominocentesis Appearance, 22: Abdomcentesis Total Protein, 23: Outcome, \n",
    "24: Surgical Lesion, 25, 26, 27: Type of Lesion, 28. cp_data\n",
    "```\n",
    "\n",
    "\n",
    "You can learn more about the dataset\n",
    "here:\n",
    "\n",
    "* Horse Colic Dataset ([horse-colic.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv))\n",
    "* Horse Colic Dataset Description ([horse-colic.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.names))\n",
    "\n",
    "The description of Horse Colic Dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Horse+Colic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and summarizing a dataset with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "df = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# summarize the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# summarize the number of rows with missing values for each column\n",
    "for i in range(df.shape[1]):\n",
    "    # count number of rows with missing values\n",
    "    n_miss = df[[i]].isnull().sum()\n",
    "    perc = n_miss / df.shape[0] * 100\n",
    "    print(\"> %d, Missing: %d (%.1f%%)\" % (i, n_miss, perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Imputation\n",
    "\n",
    "A dataset may have missing values. These are rows of data where one or more values or columns\n",
    "in that row are not present. The values may be missing completely or they may be marked with\n",
    "a special character or value, such as a question mark (\"\\?\").\n",
    "\n",
    "Values could be missing for many reasons, often specific to the problem domain, and might\n",
    "include reasons such as corrupt measurements or data unavailability.\n",
    "\n",
    "Most machine learning algorithms require numeric input values, and a value to be present\n",
    "for each row and column in a dataset. As such, missing values can cause problems for machine\n",
    "learning algorithms. Because of this, it is common to identify missing values in a dataset and\n",
    "replace them with a numeric value. This is called data imputing, or missing data imputation.\n",
    "\n",
    "A simple and popular approach to data imputation involves using statistical methods to\n",
    "estimate a value for a column from those values that are present, then replace all missing values\n",
    "in the column with the calculated statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Imputation With `SimpleImputer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical imputation transform for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "df = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = df.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# summarize total missing\n",
    "print(\"Missing old: %d\" % sum(isnan(X).flatten()))\n",
    "\n",
    "# define imputer\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "\n",
    "# summarize total missing\n",
    "print(\"Missing new: %d\" % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SimpleImputer` and Model Evaluation\n",
    "\n",
    "It is a good practice to evaluate machine learning models on a dataset using k-fold cross-\n",
    "validation. To correctly apply statistical missing data imputation and avoid data leakage, it is\n",
    "required that the statistics calculated for each column are calculated on the training dataset\n",
    "only, then applied to the train and test sets for each fold in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate mean imputation and random forest for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "df = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = df.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# define modeling pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "pipeline = Pipeline(steps=[(\"i\", imputer), (\"m\", model)])\n",
    "\n",
    "# define model evaluation\n",
    "# KFold\n",
    "#   is a cross-validator that divides the dataset into k folds.\n",
    "# Stratified\n",
    "#   is to ensure that each fold of dataset has the same proportion of observations with a given label.\n",
    "# Repeated\n",
    "#   provides a way to improve the estimated performance of a machine learning model.\n",
    "# This involves simply repeating the cross-validation procedure multiple times and reporting the mean\n",
    "# result across all folds from all runs. This mean result is expected to be a more accurate estimate\n",
    "# of the true unknown underlying mean performance of the model on the dataset, as calculated using the standard error.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "# returns an array of scores of the estimator for each run of the cross validation.\n",
    "scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "print(\"Mean Accuracy: %.3f (%.3f)\" % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different Imputed Statistics\n",
    "\n",
    "We can design an experiment to test\n",
    "each statistical strategy and discover what works best for this dataset, comparing the mean,\n",
    "median, mode (most frequent), and constant (0) strategies. The mean accuracy of each approach\n",
    "can then be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare statistical imputation strategies for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "df = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = df.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# evaluate each strategy on the dataset\n",
    "results = list()\n",
    "strategies = [\"mean\", \"median\", \"most_frequent\", \"constant\"]\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(\n",
    "        steps=[(\"i\", SimpleImputer(strategy=s)), (\"m\", RandomForestClassifier())]\n",
    "    )\n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    # store results\n",
    "    results.append(scores)\n",
    "    print(\">%s %.3f (%.3f)\" % (s, mean(scores), std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, tick_labels=strategies, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the run, a box and whisker plot is created for each set of results, allowing the\n",
    "distribution of results to be compared. We can see that the distribution of accuracy scores for\n",
    "the constant strategy may be better than the other strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SimpleImputer` Transform When Making a Prediction\n",
    "\n",
    "We may wish to create a final modeling pipeline with the constant imputation strategy and\n",
    "random forest algorithm, then make a prediction for new data. This can be achieved by defining\n",
    "the pipeline and fitting it on all available data, then calling the predict() function passing\n",
    "new data in as an argument. Importantly, the row of new data must mark any missing values\n",
    "using the NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant imputation strategy and prediction for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# create the modeling pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[(\"i\", SimpleImputer(strategy=\"constant\")), (\"m\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# define new data\n",
    "row = [\n",
    "    2,\n",
    "    1,\n",
    "    530101,\n",
    "    38.50,\n",
    "    66,\n",
    "    28,\n",
    "    3,\n",
    "    3,\n",
    "    nan,\n",
    "    2,\n",
    "    5,\n",
    "    4,\n",
    "    4,\n",
    "    nan,\n",
    "    nan,\n",
    "    nan,\n",
    "    3,\n",
    "    5,\n",
    "    45.00,\n",
    "    8.40,\n",
    "    nan,\n",
    "    nan,\n",
    "    2,\n",
    "    11300,\n",
    "    00000,\n",
    "    00000,\n",
    "    2,\n",
    "]\n",
    "\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row])\n",
    "\n",
    "# summarize prediction\n",
    "print(\"Predicted Class: %d\" % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbor (KNN) Imputation\n",
    "\n",
    "A popular approach to missing data imputation is to use a model to\n",
    "predict the missing values. This requires a model to be created for each input variable that has\n",
    "missing values. Although any one among a range of different models can be used to predict the\n",
    "missing values, the k-nearest neighbor (KNN) algorithm has proven to be generally effective,\n",
    "often referred to as nearest neighbor imputation.\n",
    "\n",
    "An effective approach to data imputing is to use a model to predict the missing values. A\n",
    "model is created for each feature that has missing values, taking as input values of perhaps all\n",
    "other input features.\n",
    "\n",
    "If input variables are numeric, then regression models can be used for prediction, and this\n",
    "case is quite common. A range of different models can be used, although a simple k-nearest\n",
    "neighbor (KNN) model has proven to be effective in experiments. The use of a KNN model to\n",
    "predict or fill missing values is referred to as Nearest Neighbor Imputation or KNN imputation.\n",
    "\n",
    "Configuration of KNN imputation often involves selecting the distance measure (e.g. Euclidean) and the number of contributing neighbors for each prediction, the k hyperparameter of\n",
    "the KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the horse colic dataset\n",
    "from pandas import read_csv\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# summarize the first few rows\n",
    "print(dataframe.head())\n",
    "\n",
    "# summarize the number of rows with missing values for each column\n",
    "for i in range(dataframe.shape[1]):\n",
    "    # count number of rows with missing values\n",
    "    n_miss = dataframe[[i]].isnull().sum()\n",
    "    perc = n_miss / dataframe.shape[0] * 100\n",
    "    print(\"> %d, Missing: %d (%.1f%%)\" % (i, n_miss, perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the list of all columns in the dataset and the number and percentage of\n",
    "missing values. We can see that some columns (e.g. column indexes 1 and 2) have no missing\n",
    "values and other columns (e.g. column indexes 15 and 21) have many or even a majority of\n",
    "missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Imputation with `KNNImputer`\n",
    "\n",
    "The scikit-learn machine learning library provides the `KNNImputer` class that supports nearest\n",
    "neighbor imputation. In this section, we will explore how to effectively use the `KNNImputer`\n",
    "class.\n",
    "\n",
    "The `KNNImputer` is a data transform that is first configured based on the method used to\n",
    "estimate the missing values. The default distance measure is a Euclidean distance measure that\n",
    "is NaN aware, e.g. will not include NaN values when calculating the distance between members\n",
    "of the training dataset. This is set via the metric argument. The number of neighbors is set to\n",
    " five by default and can be configured by the n neighbors argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn imputation transform for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# summarize total missing\n",
    "print(\"Missing old: %d\" % sum(isnan(X).flatten()))\n",
    "\n",
    "# define imputer\n",
    "imputer = KNNImputer()\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "\n",
    "# summarize total missing\n",
    "print(\"Missing new: %d\" % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `KNNImputer` and Model Evaluation\n",
    "\n",
    "It is a good practice to evaluate machine learning models on a dataset using k-fold cross-\n",
    "validation. To correctly apply nearest neighbor missing data imputation and avoid data leakage,\n",
    "it is required that the models calculated for each column are calculated on the training dataset\n",
    "only, then applied to the train and test sets for each fold in the dataset. This can be achieved\n",
    "by creating a modeling pipeline where the first step is the nearest neighbor imputation, then\n",
    "the second step is the model. We will implement this using the Pipeline class. For example,\n",
    "the Pipeline below uses a `KNNImputer` with the default strategy, followed by a random forest\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn imputation and random forest for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# define modeling pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = KNNImputer()\n",
    "pipeline = Pipeline(steps=[(\"i\", imputer), (\"m\", model)])\n",
    "\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "print(\"Mean Accuracy: %.3f (%.3f)\" % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `KNNImputer` and Different Number of Neighbors\n",
    "\n",
    "The key hyperparameter for the KNN algorithm is `k` that controls the number of nearest\n",
    "neighbors that are used to contribute to a prediction. It is good practice to test a suite of\n",
    "different values for `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare knn imputation strategies for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# evaluate each strategy on the dataset\n",
    "results = list()\n",
    "strategies = [i for i in [1, 3, 5, 7, 9, 15, 18, 21]]\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(\n",
    "        steps=[(\"i\", KNNImputer(n_neighbors=s)), (\"m\", RandomForestClassifier())]\n",
    "    )\n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    # store results\n",
    "    results.append(scores)\n",
    "    print(\">%s %.3f (%.3f)\" % (s, mean(scores), std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, tick_labels=strategies, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `KNNImputer` Transform When Making a Prediction\n",
    "\n",
    "We may wish to create a final modeling pipeline with the nearest neighbor imputation and\n",
    "random forest algorithm, then make a prediction for new data. This can be achieved by defining\n",
    "the pipeline and fitting it on all available data, then calling the predict() function, passing\n",
    "new data in as an argument. Importantly, the row of new data must mark any missing values\n",
    "using the NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn imputation strategy and prediction for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# create the modeling pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[(\"i\", KNNImputer(n_neighbors=3)), (\"m\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# define new data\n",
    "row = [\n",
    "    2,\n",
    "    1,\n",
    "    530101,\n",
    "    38.50,\n",
    "    66,\n",
    "    28,\n",
    "    3,\n",
    "    3,\n",
    "    nan,\n",
    "    2,\n",
    "    5,\n",
    "    4,\n",
    "    4,\n",
    "    nan,\n",
    "    nan,\n",
    "    nan,\n",
    "    3,\n",
    "    5,\n",
    "    45.00,\n",
    "    8.40,\n",
    "    nan,\n",
    "    nan,\n",
    "    2,\n",
    "    11300,\n",
    "    00000,\n",
    "    00000,\n",
    "    2,\n",
    "]\n",
    "\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row])\n",
    "\n",
    "# summarize prediction\n",
    "print(\"Predicted Class: %d\" % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Imputation\n",
    "\n",
    "One approach to imputing\n",
    "missing values is to use an iterative imputation model. Iterative imputation refers to a process\n",
    "where each feature is modeled as a function of the other features, e.g. a regression problem\n",
    "where missing values are predicted. Each feature is imputed sequentially, one after the other,\n",
    "allowing prior imputed values to be used as part of a model in predicting subsequent features.\n",
    "\n",
    "It is iterative because this process is repeated multiple times, allowing ever improved estimates\n",
    "of missing values to be calculated as missing values across all features are estimated. This\n",
    "approach may be generally referred to as fully conditional specification (FCS) or multivariate\n",
    "imputation by chained equations (MICE).\n",
    "\n",
    "Different regression algorithms can be used to estimate the missing values for each feature,\n",
    "although linear methods are often used for simplicity. The number of iterations of the procedure\n",
    "is often kept small, such as 10. Finally, the order that features are processed sequentially can be\n",
    "considered, such as from the feature with the least missing values to the feature with the most\n",
    "missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Imputation With `IterativeImputer`\n",
    "\n",
    "The scikit-learn machine learning library provides the `IterativeImputer` class that supports\n",
    "iterative imputation. In this section, we will explore how to e ectively use the `IterativeImputer`\n",
    "class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `IterativeImputer` Data Transform\n",
    "\n",
    "It is a data transform that is  first configured based on the method used to estimate the missing\n",
    "values. By default, a BayesianRidge model is employed that uses a function of all other input\n",
    "features. Features are filled in ascending order, from those with the fewest missing values to\n",
    "those with the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative imputation transform for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# summarize total missing\n",
    "print(\"Missing old: %d\" % sum(isnan(X).flatten()))\n",
    "\n",
    "# define imputer\n",
    "imputer = IterativeImputer()\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "\n",
    "# summarize total missing\n",
    "print(\"Missing new: %d\" % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `IterativeImputer` and Model Evaluation\n",
    "\n",
    "It is a good practice to evaluate machine learning models on a dataset using k-fold cross-\n",
    "validation. To correctly apply iterative missing data imputation and avoid data leakage, it is\n",
    "required that the models for each column are calculated on the training dataset only, then\n",
    "applied to the train and test sets for each fold in the dataset. This can be achieved by creating\n",
    "a modeling pipeline where the  rst step is the iterative imputation, then the second step is the\n",
    "model. This can be achieved using the Pipeline class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate iterative imputation and random forest for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# define modeling pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = IterativeImputer()\n",
    "pipeline = Pipeline(steps=[(\"i\", imputer), (\"m\", model)])\n",
    "\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "print(\"Mean Accuracy: %.3f (%.3f)\" % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `IterativeImputer` and Different Imputation Order\n",
    "\n",
    "By default, imputation is performed in ascending order from the feature with the least missing\n",
    "values to the feature with the most. This makes sense as we want to have more complete data\n",
    "when it comes time to estimating missing values for columns where the majority of values\n",
    "are missing. Nevertheless, we can experiment with different imputation order strategies, such\n",
    "as descending, right-to-left (Arabic), left-to-right (Roman), and random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare iterative imputation strategies for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# evaluate each strategy on the dataset\n",
    "results = list()\n",
    "strategies = [\"ascending\", \"descending\", \"roman\", \"arabic\", \"random\"]\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"i\", IterativeImputer(imputation_order=s)),\n",
    "            (\"m\", RandomForestClassifier()),\n",
    "        ]\n",
    "    )\n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    # store results\n",
    "    results.append(scores)\n",
    "    print(\">%s %.3f (%.3f)\" % (s, mean(scores), std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, tick_labels=strategies, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `IterativeImputer` and Different Number of Iterations\n",
    "\n",
    "By default, the `IterativeImputer` will repeat the number of iterations 10 times. It is possible\n",
    "that a large number of iterations may begin to bias or skew the estimate and that few iterations\n",
    "may be preferred. The number of iterations of the procedure can be specified via the max iter\n",
    "argument. It may be interesting to evaluate different numbers of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare iterative imputation number of iterations for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# evaluate each strategy on the dataset\n",
    "results = list()\n",
    "strategies = [str(i) for i in range(1, 21)]\n",
    "for s in strategies:\n",
    "    # create the modeling pipeline\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"i\", IterativeImputer(max_iter=int(s))),\n",
    "            (\"m\", RandomForestClassifier()),\n",
    "        ]\n",
    "    )\n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    # store results\n",
    "    results.append(scores)\n",
    "    print(\">%s %.3f (%.3f)\" % (s, mean(scores), std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, tick_labels=strategies, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative imputation strategy and prediction for the horse colic dataset\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(horse_colic_data, header=None, na_values=\"?\")\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# create the modeling pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[(\"i\", IterativeImputer(max_iter=8)), (\"m\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# define new data\n",
    "row = [\n",
    "    2,\n",
    "    1,\n",
    "    530101,\n",
    "    38.50,\n",
    "    66,\n",
    "    28,\n",
    "    3,\n",
    "    3,\n",
    "    nan,\n",
    "    2,\n",
    "    5,\n",
    "    4,\n",
    "    4,\n",
    "    nan,\n",
    "    nan,\n",
    "    nan,\n",
    "    3,\n",
    "    5,\n",
    "    45.00,\n",
    "    8.40,\n",
    "    nan,\n",
    "    nan,\n",
    "    2,\n",
    "    11300,\n",
    "    00000,\n",
    "    00000,\n",
    "    2,\n",
    "]\n",
    "\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row])\n",
    "\n",
    "# summarize prediction\n",
    "print(\"Predicted Class: %d\" % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `IterativeImputer` Transform When Making a Prediction\n",
    "\n",
    "We may wish to create a final modeling pipeline with the iterative imputation and random\n",
    "forest algorithm, then make a prediction for new data. This can be achieved by defining the\n",
    "pipeline and fitting it on all available data, then calling the predict() function, passing new\n",
    "data in as an argument. Importantly, the row of new data must mark any missing values using\n",
    "the NaN value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this module, we learned several approaches for handling missing data:\n",
    "\n",
    "- Statistical imputation using mean, median, mode and constant values\n",
    "- K-nearest neighbor imputation with different numbers of neighbors\n",
    "- Iterative imputation with different ordering strategies and iterations\n",
    "- How to properly implement imputation in a machine learning pipeline\n",
    "- How to evaluate and compare different imputation methods\n",
    "- How to use imputation when making predictions on new data\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.￼\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
