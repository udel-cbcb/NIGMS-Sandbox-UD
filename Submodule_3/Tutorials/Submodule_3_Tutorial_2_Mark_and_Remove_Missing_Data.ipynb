{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark and Remove Missing Data\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we will learn how to handle missing data in datasets, specifically focusing on marking and removing missing values. By applying these techniques, you'll be able to prepare high-quality datasets for machine learning, improving model reliability and accuracy.\n",
    "\n",
    "We'll use the Pima Indians Diabetes dataset as an example to demonstrate these techniques.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn how to identify and mark invalid or corrupt values as missing in a dataset\n",
    "- Understand how the presence of marked missing values affects machine learning algorithms\n",
    "- Learn how to remove rows with missing data from a dataset\n",
    "- Evaluate a learning algorithm on a dataset after removing rows with missing values\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python programming\n",
    "- Familiarity with pandas, numpy, and scikit-learn libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages, import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/chenc/miniconda3/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /Users/chenc/miniconda3/lib/python3.9/site-packages (1.3.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/chenc/miniconda3/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/chenc/miniconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/chenc/miniconda3/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/chenc/miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/chenc/miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/chenc/miniconda3/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/chenc/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp39-cp39-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp39-cp39-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "featuretools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.3.4 which is incompatible.\n",
      "langchain 0.0.337 requires anyio<4.0, but you have anyio 4.2.0 which is incompatible.\n",
      "scispacy 0.5.3 requires scipy<1.11, but you have scipy 1.11.4 which is incompatible.\n",
      "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, but you have pandas 1.3.4 which is incompatible.\n",
      "woodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary Python libraries using pip:\n",
    "# numpy:  Fundamental package for numerical computation in Python. It provides support for arrays, matrices, and mathematical functions to operate on these structures efficiently.\n",
    "# pandas:  Library providing high-performance, easy-to-use data structures and data analysis tools. It's particularly useful for working with tabular data (like CSV files, spreadsheets, SQL databases).\n",
    "# scikit-learn:  Popular machine learning library in Python. It provides tools for classification, regression, clustering, dimensionality reduction, model selection, and preprocessing.\n",
    "%pip install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NumPy library for numerical operations, often used for handling arrays and matrices.\n",
    "import numpy as np\n",
    "# Import the Pandas library for data manipulation and analysis, particularly for working with DataFrames.\n",
    "import pandas as pd\n",
    "# Import the LinearDiscriminantAnalysis class from scikit-learn's discriminant_analysis module.\n",
    "# LDA is a classifier with linear decision boundaries, often used for dimensionality reduction and classification.\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# Import KFold and cross_val_score from scikit-learn's model_selection module.\n",
    "# KFold is used for splitting data into k-folds for cross-validation.\n",
    "# cross_val_score is used to evaluate a score for a model using cross-validation.\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient as\n",
    "either an onset of diabetes withinâ€€five years or not.\n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class\n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "- Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "- Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))\n",
    "\n",
    "The description of Diabetes Dataset can be found [here](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and summarize the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the file path to the Pima Indians Diabetes dataset CSV file.\n",
    "# This path assumes the CSV file is located in the \"../../Data/\" directory relative to the current script's location.\n",
    "pima_indians_diabetes_csv = \"../../Data/pima-indians-diabetes.csv\"\n",
    "\n",
    "# Define the column names for the dataset\n",
    "columns = [\n",
    "    'Pregnancies',               # Number of times pregnant\n",
    "    'Glucose',                   # Plasma glucose concentration (mg/dL)\n",
    "    'BloodPressure',             # Diastolic blood pressure (mm Hg)\n",
    "    'SkinThickness',             # Triceps skinfold thickness (mm)\n",
    "    'Insulin',                   # 2-Hour serum insulin (mu U/ml)\n",
    "    'BMI',                       # Body mass index (weight in kg/(height in m)^2)\n",
    "    'DiabetesPedigreeFunction',  # Diabetes pedigree function (genetic risk)\n",
    "    'Age',                       # Age in years\n",
    "    'Outcome'                    # Class variable (0: Non-diabetic, 1: Diabetic)\n",
    "    ]\n",
    "# Load the dataset from a CSV file defined by a variable 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' argument indicates that the CSV file does not have a header row.\n",
    "# 'names=columns' Assign column names defined above\n",
    "dataset = pd.read_csv(pima_indians_diabetes_csv, header=None, names=columns)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame to get a quick overview of the data.\n",
    "# This helps to understand the structure and sample data of the dataset.\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
      "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
      "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
      "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
      "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
      "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
      "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
      "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
      "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
      "\n",
      "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
      "count  768.000000                768.000000  768.000000  768.000000  \n",
      "mean    31.992578                  0.471876   33.240885    0.348958  \n",
      "std      7.884160                  0.331329   11.760232    0.476951  \n",
      "min      0.000000                  0.078000   21.000000    0.000000  \n",
      "25%     27.300000                  0.243750   24.000000    0.000000  \n",
      "50%     32.000000                  0.372500   29.000000    0.000000  \n",
      "75%     36.600000                  0.626250   41.000000    1.000000  \n",
      "max     67.100000                  2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Print summary statistics of the dataset.\n",
    "# This will include measures like mean, standard deviation, min, max, and quartiles for numerical columns,\n",
    "# and count, unique, top, and frequency for categorical columns in the 'dataset' DataFrame.\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several columns in the dataset contain minimum values of zero. However, for specific variables (e.g., blood pressure), a zero value is biologically implausible and likely indicates missing or corrupted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns contain invalid zero values: Plasma glucose concentration, Diastolic blood pressure, Triceps skinfold thickness, 2-Hour serum insulin, and Body mass index. We can verify this by examining the raw data, such as by displaying the first 20 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0             6      148             72             35        0  33.6   \n",
      "1             1       85             66             29        0  26.6   \n",
      "2             8      183             64              0        0  23.3   \n",
      "3             1       89             66             23       94  28.1   \n",
      "4             0      137             40             35      168  43.1   \n",
      "5             5      116             74              0        0  25.6   \n",
      "6             3       78             50             32       88  31.0   \n",
      "7            10      115              0              0        0  35.3   \n",
      "8             2      197             70             45      543  30.5   \n",
      "9             8      125             96              0        0   0.0   \n",
      "10            4      110             92              0        0  37.6   \n",
      "11           10      168             74              0        0  38.0   \n",
      "12           10      139             80              0        0  27.1   \n",
      "13            1      189             60             23      846  30.1   \n",
      "14            5      166             72             19      175  25.8   \n",
      "15            7      100              0              0        0  30.0   \n",
      "16            0      118             84             47      230  45.8   \n",
      "17            7      107             74              0        0  29.6   \n",
      "18            1      103             30             38       83  43.3   \n",
      "19            1      115             70             30       96  34.6   \n",
      "\n",
      "    DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                      0.627   50        1  \n",
      "1                      0.351   31        0  \n",
      "2                      0.672   32        1  \n",
      "3                      0.167   21        0  \n",
      "4                      2.288   33        1  \n",
      "5                      0.201   30        0  \n",
      "6                      0.248   26        1  \n",
      "7                      0.134   29        0  \n",
      "8                      0.158   53        1  \n",
      "9                      0.232   54        1  \n",
      "10                     0.191   30        0  \n",
      "11                     0.537   34        1  \n",
      "12                     1.441   57        0  \n",
      "13                     0.398   59        1  \n",
      "14                     0.587   51        1  \n",
      "15                     0.484   32        1  \n",
      "16                     0.551   31        1  \n",
      "17                     0.254   31        1  \n",
      "18                     0.183   33        0  \n",
      "19                     0.529   32        1  \n"
     ]
    }
   ],
   "source": [
    "# Print the first 20 rows of the loaded dataset.\n",
    "# 'dataset.head(20)' accesses the first 20 rows of the DataFrame 'dataset'.\n",
    "# 'print()' function displays these first 20 rows to the console,\n",
    "# providing a summary of the initial data in the DataFrame.\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of missing values can be quantified for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the number of missing values for each variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace column names with their index values\n",
    "dataset.columns = range(len(dataset.columns))\n",
    "\n",
    "# Count the number of missing values (represented as 0 in this dataset) for specific columns.\n",
    "# We are checking columns at index 1, 2, 3, 4, and 5 of the DataFrame.\n",
    "# `(dataset[[1, 2, 3, 4, 5]] == 0)` creates a boolean DataFrame where True indicates a value is 0.\n",
    "# `.sum()` then sums the True values along each column, effectively counting the zeros in each specified column.\n",
    "num_missing = (dataset[[1, 2, 3, 4, 5]] == 0).sum()\n",
    "\n",
    "# Print the results, which will show the count of zero values for each of the selected columns.\n",
    "# These zero values are being treated as missing values in this specific context of the 'pima_indians_diabetes_csv' dataset.\n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns 1, 2, and 5 contain relatively few zero values, while columns 3 and 4 show significantly more - nearly half of all rows. This disparity suggests we may need to apply different missing value treatment strategies for different columns to maintain adequate sample sizes for predictive model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python's data science stack (Pandas, NumPy, and Scikit-Learn), missing values are represented as NaN (Not a Number). These NaN values are automatically excluded from statistical operations like sum() and count().\n",
    "\n",
    "To handle missing data:\n",
    "* Convert invalid values to NaN using DataFrame.replace() on specific columns\n",
    "* Identify missing values with isnull(), which returns a boolean mask (True for NaN)\n",
    "* Count missing values per column using isnull().sum()\n",
    "\n",
    "This workflow enables systematic missing data analysis before applying imputation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking missing values with nan values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '0' values with 'np.nan' (Not a Number) in specific columns of the DataFrame.\n",
    "# The columns at index 1, 2, 3, 4, and 5 are selected for replacement.\n",
    "# This is likely done because '0' in these columns might represent missing or invalid data in the context of the dataset.\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, np.nan)\n",
    "\n",
    "# Print the count of NaN (Not a Number) values for each column in the DataFrame.\n",
    "# 'dataset.isnull()' creates a boolean DataFrame indicating missing values (True for NaN, False otherwise).\n",
    "# '.sum()' then sums the True values along each column, effectively counting the number of NaN values per column.\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A preliminary examination of the first 20 records confirms this observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review data with missing values marked with a nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '0' values with 'np.nan' (Not a Number) in specific columns of the DataFrame.\n",
    "# The columns at index 1, 2, 3, 4, and 5 are selected for replacement.\n",
    "# This is likely done because '0' in these columns might represent missing or invalid data in the context of the dataset.\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, np.nan)\n",
    "\n",
    "# Print the first 20 rows of the DataFrame to display a sample of the data after loading and replacing values.\n",
    "# 'dataset.head(20)' returns the first 20 rows of the DataFrame, which is then printed to the console.\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Cause Problems\n",
    "Most scikit-learn estimators will raise ValueError exceptions when encountering missing values, requiring either imputation or removal prior to modeling.\n",
    "\n",
    "**Note!** You should see a message about an error occurring when you try to run the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '0' values with 'nan' in specific columns (columns at index 1, 2, 3, 4, 5).\n",
    "# This is done because in the Pima Indians Diabetes Dataset, '0' in these columns represents missing values.\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, np.nan)\n",
    "\n",
    "# Split the dataset into input features (X) and the target variable (y).\n",
    "# 'values' converts the DataFrame to a NumPy array.\n",
    "values = dataset.values\n",
    "# X is assigned the first 8 columns (index 0 to 7) which are the input features.\n",
    "X = values[:, 0:8]\n",
    "# y is assigned the 9th column (index 8) which is the target variable (diabetes outcome).\n",
    "y = values[:, 8]\n",
    "\n",
    "# Define the Linear Discriminant Analysis (LDA) model.\n",
    "# LDA is a classifier that assumes a linear decision boundary and fits class conditional densities using Bayes' rule.\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Define the cross-validation procedure using K-Fold.\n",
    "# KFold splits the dataset into k consecutive folds (here, n_splits=3).\n",
    "# 'shuffle=True' shuffles the data before splitting to ensure folds are more representative.\n",
    "# 'random_state=1' ensures reproducibility of the shuffling.\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# Evaluate the model's performance using cross-validation and the accuracy scoring metric.\n",
    "# 'cross_val_score' performs cross-validation and returns an array of scores for each fold.\n",
    "# The code is wrapped in a try-except block to handle potential ValueError exceptions.\n",
    "try:\n",
    "    result = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "    # If cross-validation is successful, print the mean accuracy across all folds, formatted to 3 decimal places.\n",
    "    print(\"Accuracy: %.3f\" % result.mean())\n",
    "# Catch ValueError exceptions, which might occur if LDA assumptions are violated or data is not suitable.\n",
    "except ValueError as e:\n",
    "    # If a ValueError occurs during cross-validation, print an informative message indicating an expected error and the error details.\n",
    "    print(f\"********************* An (expected) error occurred *********************\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rows With Missing Values\n",
    "\n",
    "The simplest approach for handling missing values is to remove:\n",
    "- Entire predictors (columns) containing missing values\n",
    "- And/or samples (rows) with missing values\n",
    "\n",
    "#### Implementation in Pandas:\n",
    "```python\n",
    "# Create new DataFrame with missing rows removed\n",
    "cleaned_df = original_df.dropna()\n",
    "\n",
    "# Alternative: Remove rows with missing values in specific columns\n",
    "cleaned_df = original_df.dropna(subset=['important_column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of removing rows that contain missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the DataFrame to summarize the number of rows and columns in the raw data.\n",
    "print(dataset.shape)\n",
    "\n",
    "# Replace '0' values with 'np.nan' (Not a Number) in specific columns (index 1, 2, 3, 4, 5) of the DataFrame.\n",
    "# This is done to represent missing values, as '0' might be used as a placeholder for missing data in this dataset.\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, np.nan)\n",
    "\n",
    "# Remove rows that contain any missing values (NaN) from the DataFrame.\n",
    "# 'inplace=True' modifies the DataFrame directly; no new DataFrame is returned.\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "# Print the shape of the DataFrame again to summarize the data after rows with missing values have been removed.\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a cleaned dataset suitable for evaluating algorithms that require complete cases, such as Linear Discriminant Analysis (LDA).\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Contains no missing values (NaN)\n",
    "- Maintains sufficient sample size for reliable evaluation\n",
    "- Preserves the original data structure where possible\n",
    "\n",
    "**Typical Algorithms Requiring Complete Data:**\n",
    "- LDA (Linear Discriminant Analysis)\n",
    "- Logistic Regression\n",
    "- SVM (Support Vector Machines)\n",
    "- Most scikit-learn estimators\n",
    "\n",
    "**Verification Check:**\n",
    "```python\n",
    "print(f\"Remaining samples: {len(cleaned_df)}\")\n",
    "print(f\"Missing values per column:\\n{cleaned_df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on data after rows with missing data are removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '0' values with 'nan' (Not a Number) in columns 1, 2, 3, 4, and 5 of the DataFrame.\n",
    "# This is done to represent missing or invalid data, as '0' might not be a valid value for these columns in the context of the dataset.\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, np.nan)\n",
    "\n",
    "# Drop rows from the DataFrame that contain any missing values (NaN).\n",
    "# 'inplace=True' modifies the DataFrame directly.\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "# Extract the values from the DataFrame into a NumPy array.\n",
    "values = dataset.values\n",
    "# Assign the first 8 columns of the 'values' array to 'X' as input features.\n",
    "X = values[:, 0:8]\n",
    "# Assign the 9th column (index 8) of the 'values' array to 'y' as the target variable (output).\n",
    "y = values[:, 8]\n",
    "\n",
    "# Define the Linear Discriminant Analysis (LDA) model.\n",
    "# LDA is a classifier with a linear decision boundary, often used for dimensionality reduction and classification.\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Define the cross-validation procedure using K-Fold.\n",
    "# 'n_splits=3' specifies 3-fold cross-validation.\n",
    "# 'shuffle=True' shuffles the data before splitting into folds.\n",
    "# 'random_state=1' ensures reproducibility of the shuffling.\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# Evaluate the model's performance using cross-validation.\n",
    "# 'cross_val_score' performs cross-validation and returns the scores for each fold.\n",
    "# 'model' is the LDA model to be evaluated.\n",
    "# 'X' is the input features.\n",
    "# 'y' is the target variable.\n",
    "# 'cv=cv' uses the K-Fold cross-validation strategy defined earlier.\n",
    "# 'scoring=\"accuracy\"' specifies that accuracy is the metric to be used for evaluation.\n",
    "result = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "# Report the mean accuracy score across all folds from the cross-validation.\n",
    "# 'result.mean()' calculates the average accuracy.\n",
    "# \"Accuracy: %.3f\" formats the mean accuracy to 3 decimal places for printing.\n",
    "print(\"Accuracy: %.3f\" % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we learned how to:\n",
    "\n",
    "- Identify and mark missing values in a dataset\n",
    "- Understand the impact of missing values on machine learning algorithms\n",
    "- Remove rows with missing data\n",
    "- Evaluate a machine learning model on a cleaned dataset\n",
    "\n",
    "These skills are crucial for preparing real-world datasets for analysis and machine learning tasks.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter notebook and delete any unnecessary resources when you're finished with this tutorial."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
