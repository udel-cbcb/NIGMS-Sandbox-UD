{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark and Remove Missing Data\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we will learn how to handle missing data in datasets, specifically focusing on marking and removing missing values. By applying these techniques, you'll be able to prepare high-quality datasets for machine learning, improving model reliability and accuracy.\n",
    "\n",
    "We'll use the Pima Indians Diabetes dataset as an example to demonstrate these techniques.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn how to identify and mark invalid or corrupt values as missing in a dataset\n",
    "- Understand how the presence of marked missing values affects machine learning algorithms\n",
    "- Learn how to remove rows with missing data from a dataset\n",
    "- Evaluate a learning algorithm on a dataset after removing rows with missing values\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python programming\n",
    "- Familiarity with pandas, numpy, and scikit-learn libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages, import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python libraries using pip:\n",
    "# numpy:  Fundamental package for numerical computation in Python. It provides support for arrays, matrices, and mathematical functions to operate on these structures efficiently.\n",
    "# pandas:  Library providing high-performance, easy-to-use data structures and data analysis tools. It's particularly useful for working with tabular data (like CSV files, spreadsheets, SQL databases).\n",
    "# scikit-learn:  Popular machine learning library in Python. It provides tools for classification, regression, clustering, dimensionality reduction, model selection, and preprocessing.\n",
    "%pip install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NumPy library for numerical operations, often used for handling arrays and matrices.\n",
    "import numpy as np\n",
    "# Import the Pandas library for data manipulation and analysis, particularly for working with DataFrames.\n",
    "import pandas as pd\n",
    "# Import the LinearDiscriminantAnalysis class from scikit-learn's discriminant_analysis module.\n",
    "# LDA is a classifier with linear decision boundaries, often used for dimensionality reduction and classification.\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# Import KFold and cross_val_score from scikit-learn's model_selection module.\n",
    "# KFold is used for splitting data into k-folds for cross-validation.\n",
    "# cross_val_score is used to evaluate a score for a model using cross-validation.\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Define the file path to the Pima Indians Diabetes dataset CSV file.\n",
    "# This path assumes the CSV file is located in the \"../../Data/\" directory relative to the current script's location.\n",
    "pima_indians_diabetes_csv = \"../../Data/pima-indians-diabetes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient as\n",
    "either an onset of diabetes withinâ€€five years or not.\n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class\n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "- Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "- Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))\n",
    "\n",
    "The description of Diabetes Dataset can be found [here](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and summarize the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file named 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' argument indicates that the CSV file does not have a header row.\n",
    "dataset = pd.read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame to get a quick overview of the data.\n",
    "# This helps to understand the structure and sample data of the dataset.\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics of the dataset.\n",
    "# This will include measures like mean, standard deviation, min, max, and quartiles for numerical columns,\n",
    "# and count, unique, top, and frequency for categorical columns in the 'dataset' DataFrame.\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are columns that have a minimum value of zero (0).\n",
    "On some columns, a value of zero does not make sense and indicates an invalid or missing value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the following columns have an invalid zero minimum value: 2. Plasma glucose concentration 3. Diastolic blood pressure 4. Triceps skinfold thickness 5. 2-Hour serum insulin 6. Body mass index\n",
    "\n",
    "We can confirm this by looking at the raw data and printing out the first 20 rows of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and review rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file.\n",
    "# 'pd.read_csv()' function from pandas library is used to read the CSV file.\n",
    "# 'pima_indians_diabetes_csv' is assumed to be a variable holding the path to the CSV file.\n",
    "# 'header=None' argument specifies that the CSV file does not have a header row,\n",
    "# so pandas will automatically assign column indices (0, 1, 2, ...).\n",
    "dataset = pd.read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Print the first 20 rows of the loaded dataset.\n",
    "# 'dataset.head(20)' accesses the first 20 rows of the DataFrame 'dataset'.\n",
    "# 'print()' function displays these first 20 rows to the console,\n",
    "# providing a summary of the initial data in the DataFrame.\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a count of the number of missing values on each of these columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the number of missing values for each variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add comments to the code below\n",
    "\n",
    "# Load the dataset from a CSV file named 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = pd.read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Count the number of missing values (represented as 0 in this dataset) for specific columns.\n",
    "# We are checking columns at index 1, 2, 3, 4, and 5 of the DataFrame.\n",
    "# `(dataset[[1, 2, 3, 4, 5]] == 0)` creates a boolean DataFrame where True indicates a value is 0.\n",
    "# `.sum()` then sums the True values along each column, effectively counting the zeros in each specified column.\n",
    "num_missing = (dataset[[1, 2, 3, 4, 5]] == 0).sum()\n",
    "\n",
    "# Print the results, which will show the count of zero values for each of the selected columns.\n",
    "# These zero values are being treated as missing values in this specific context of the 'pima_indians_diabetes_csv' dataset.\n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that columns 1, 2 and 5 have just a few zero values, whereas columns 3 and 4\n",
    "show a lot more, nearly half of the rows. This highlights that different missing value strategies\n",
    "may be needed for different columns, e.g. to ensure that there are still a sufficient number of\n",
    "records left to train a predictive model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, specifically Pandas, NumPy and Scikit-Learn, we mark missing values as NaN.\n",
    "Values with a NaN value are ignored from operations like sum, count, etc. We can mark values\n",
    "as NaN easily with the Pandas DataFrame by using the replace() function on a subset of\n",
    "the columns we are interested in. After we have marked the missing values, we can use the\n",
    "isnull() function to mark all of the NaN values in the dataset as True and get a count of the\n",
    "missing values for each column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking missing values with nan values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file named 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' argument indicates that the CSV file does not have a header row.\n",
    "dataset = pd.read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Replace '0' values with 'np.nan' (Not a Number) in specific columns of the DataFrame.\n",
    "# The columns at index 1, 2, 3, 4, and 5 are selected for replacement.\n",
    "# This is likely done because '0' in these columns might represent missing or invalid data in the context of the dataset.\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, np.nan)\n",
    "\n",
    "# Print the count of NaN (Not a Number) values for each column in the DataFrame.\n",
    "# 'dataset.isnull()' creates a boolean DataFrame indicating missing values (True for NaN, False otherwise).\n",
    "# '.sum()' then sums the True values along each column, effectively counting the number of NaN values per column.\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm by printing out the first 20 rows of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review data with missing values marked with a nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file specified by the variable 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' indicates that the CSV file does not have a header row, so pandas will automatically assign column indices.\n",
    "dataset = pd.read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Replace '0' values with 'np.nan' (Not a Number) in specific columns of the DataFrame.\n",
    "# The columns at index 1, 2, 3, 4, and 5 are selected for replacement.\n",
    "# This is likely done because '0' in these columns might represent missing or invalid data in the context of the dataset.\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, np.nan)\n",
    "\n",
    "# Print the first 20 rows of the DataFrame to display a sample of the data after loading and replacing values.\n",
    "# 'dataset.head(20)' returns the first 20 rows of the DataFrame, which is then printed to the console.\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Cause Problems\n",
    "\n",
    "Having missing values in a dataset can cause errors with some machine learning algorithms.\n",
    "\n",
    "*Note!* You should see a message about an error occurring when you try to run the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file named 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' argument indicates that the CSV file does not have a header row.\n",
    "dataset = pd.read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Replace '0' values with 'nan' in specific columns (columns at index 1, 2, 3, 4, 5).\n",
    "# This is done because in the Pima Indians Diabetes Dataset, '0' in these columns represents missing values.\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, np.nan)\n",
    "\n",
    "# Split the dataset into input features (X) and the target variable (y).\n",
    "# 'values' converts the DataFrame to a NumPy array.\n",
    "values = dataset.values\n",
    "# X is assigned the first 8 columns (index 0 to 7) which are the input features.\n",
    "X = values[:, 0:8]\n",
    "# y is assigned the 9th column (index 8) which is the target variable (diabetes outcome).\n",
    "y = values[:, 8]\n",
    "\n",
    "# Define the Linear Discriminant Analysis (LDA) model.\n",
    "# LDA is a classifier that assumes a linear decision boundary and fits class conditional densities using Bayes' rule.\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Define the cross-validation procedure using K-Fold.\n",
    "# KFold splits the dataset into k consecutive folds (here, n_splits=3).\n",
    "# 'shuffle=True' shuffles the data before splitting to ensure folds are more representative.\n",
    "# 'random_state=1' ensures reproducibility of the shuffling.\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# Evaluate the model's performance using cross-validation and the accuracy scoring metric.\n",
    "# 'cross_val_score' performs cross-validation and returns an array of scores for each fold.\n",
    "# The code is wrapped in a try-except block to handle potential ValueError exceptions.\n",
    "try:\n",
    "    result = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "    # If cross-validation is successful, print the mean accuracy across all folds, formatted to 3 decimal places.\n",
    "    print(\"Accuracy: %.3f\" % result.mean())\n",
    "# Catch ValueError exceptions, which might occur if LDA assumptions are violated or data is not suitable.\n",
    "except ValueError as e:\n",
    "    # If a ValueError occurs during cross-validation, print an informative message indicating an expected error and the error details.\n",
    "    print(f\"********************* An (expected) error occurred *********************\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rows With Missing Values\n",
    "\n",
    "The simplest approach for dealing with missing values is to remove entire predictor(s)\n",
    "and/or sample(s) that contain missing values.\n",
    "\n",
    "We can do this by creating a new Pandas DataFrame with the rows containing missing values\n",
    "removed. Pandas provides the `dropna()` function that can be used to drop either columns or\n",
    "rows with missing data. We can use `dropna()` to remove all rows with missing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of removing rows that contain missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file named 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = pd.read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Print the shape of the DataFrame to summarize the number of rows and columns in the raw data.\n",
    "print(dataset.shape)\n",
    "\n",
    "# Replace '0' values with 'np.nan' (Not a Number) in specific columns (index 1, 2, 3, 4, 5) of the DataFrame.\n",
    "# This is done to represent missing values, as '0' might be used as a placeholder for missing data in this dataset.\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, np.nan)\n",
    "\n",
    "# Remove rows that contain any missing values (NaN) from the DataFrame.\n",
    "# 'inplace=True' modifies the DataFrame directly; no new DataFrame is returned.\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "# Print the shape of the DataFrame again to summarize the data after rows with missing values have been removed.\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset that we could use to evaluate an algorithm sensitive to missing values\n",
    "like LDA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on data after rows with missing data are removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file named 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = pd.read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Replace '0' values with 'nan' (Not a Number) in columns 1, 2, 3, 4, and 5 of the DataFrame.\n",
    "# This is done to represent missing or invalid data, as '0' might not be a valid value for these columns in the context of the dataset.\n",
    "dataset[[1, 2, 3, 4, 5]] = dataset[[1, 2, 3, 4, 5]].replace(0, np.nan)\n",
    "\n",
    "# Drop rows from the DataFrame that contain any missing values (NaN).\n",
    "# 'inplace=True' modifies the DataFrame directly.\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "# Extract the values from the DataFrame into a NumPy array.\n",
    "values = dataset.values\n",
    "# Assign the first 8 columns of the 'values' array to 'X' as input features.\n",
    "X = values[:, 0:8]\n",
    "# Assign the 9th column (index 8) of the 'values' array to 'y' as the target variable (output).\n",
    "y = values[:, 8]\n",
    "\n",
    "# Define the Linear Discriminant Analysis (LDA) model.\n",
    "# LDA is a classifier with a linear decision boundary, often used for dimensionality reduction and classification.\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Define the cross-validation procedure using K-Fold.\n",
    "# 'n_splits=3' specifies 3-fold cross-validation.\n",
    "# 'shuffle=True' shuffles the data before splitting into folds.\n",
    "# 'random_state=1' ensures reproducibility of the shuffling.\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# Evaluate the model's performance using cross-validation.\n",
    "# 'cross_val_score' performs cross-validation and returns the scores for each fold.\n",
    "# 'model' is the LDA model to be evaluated.\n",
    "# 'X' is the input features.\n",
    "# 'y' is the target variable.\n",
    "# 'cv=cv' uses the K-Fold cross-validation strategy defined earlier.\n",
    "# 'scoring=\"accuracy\"' specifies that accuracy is the metric to be used for evaluation.\n",
    "result = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "# Report the mean accuracy score across all folds from the cross-validation.\n",
    "# 'result.mean()' calculates the average accuracy.\n",
    "# \"Accuracy: %.3f\" formats the mean accuracy to 3 decimal places for printing.\n",
    "print(\"Accuracy: %.3f\" % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we learned how to:\n",
    "\n",
    "- Identify and mark missing values in a dataset\n",
    "- Understand the impact of missing values on machine learning algorithms\n",
    "- Remove rows with missing data\n",
    "- Evaluate a machine learning model on a cleaned dataset\n",
    "\n",
    "These skills are crucial for preparing real-world datasets for analysis and machine learning tasks.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter notebook and delete any unnecessary resources when you're finished with this tutorial."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
