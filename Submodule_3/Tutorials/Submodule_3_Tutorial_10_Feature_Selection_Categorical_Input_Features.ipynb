{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection: Select Categorical Input Features\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Feature selection is the process of identifying and selecting a subset of input features that are\n",
    "most relevant to the target variable. Feature selection is often straightforward when working\n",
    "with real-valued data, such as using the Pearson's correlation coefficient, but can be challenging\n",
    "when working with categorical data. The two most commonly used feature selection methods for\n",
    "categorical input data when the target variable is also categorical (e.g. classification predictive\n",
    "modeling) are the **chi-squared statistic** and the **mutual information statistic**.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn about the breast cancer predictive modeling problem with categorical inputs and binary classification target variable\n",
    "- Understand how to evaluate the importance of categorical features using the chi-squared and mutual information statistics\n",
    "- Learn how to perform feature selection for categorical data when fitting and evaluating a classification model\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Evaluate models using different feature selection methods\n",
    "- Compare performance between full feature set and selected features\n",
    "- Create visualizations of feature importance scores\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Basic understanding of classification models\n",
    "- Knowledge of basic statistical concepts\n",
    "\n",
    "## Get Started\n",
    "\n",
    "Setup steps:\n",
    "- Import required libraries (matplotlib, pandas, scikit-learn)\n",
    "- Download breast cancer dataset\n",
    "- Prepare data loading and preprocessing functions\n",
    "- Set up feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages and import the necessary libraries.\n",
    "\n",
    "### Install packages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary packages using pip in a Jupyter notebook environment\n",
    "\n",
    "# 'matplotlib' is a plotting library for creating static, interactive, and animated visualizations\n",
    "%pip install matplotlib \n",
    "\n",
    "# 'numpy' is a fundamental package for scientific computing in Python, providing support for arrays and matrices\n",
    "%pip install numpy\n",
    "\n",
    "# 'pandas' is a powerful data manipulation and analysis library, often used for working with structured data\n",
    "%pip install pandas\n",
    "\n",
    "# 'scikit-learn' is a machine learning library that provides simple and efficient tools for data mining and data analysis\n",
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary modules and classes\n",
    "\n",
    "# Importing pyplot from matplotlib for plotting graphs and visualizations\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Importing read_csv from pandas to load CSV data into a DataFrame\n",
    "from pandas import read_csv\n",
    "\n",
    "# Importing SelectKBest and feature selection methods for selecting the best features\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "# Importing LogisticRegression from sklearn to use as a machine learning model for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Importing accuracy_score from sklearn to evaluate the performance of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Importing train_test_split from sklearn to split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing LabelEncoder and OrdinalEncoder for encoding categorical features as numeric values\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Dataset\n",
    "\n",
    "Breast cancer dataset classifies breast cancer\n",
    "patient data as either a recurrence or no recurrence of cancer. \n",
    "\n",
    "```\n",
    "Number of Instances: 286\n",
    "Number of Attributes: 9 + the class attribute\n",
    "Attribute Information:\n",
    "   1. Class: no-recurrence-events, recurrence-events\n",
    "   2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "   3. menopause: lt40, ge40, premeno.\n",
    "   4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
    "   5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
    "   6. node-caps: yes, no.\n",
    "   7. deg-malig: 1, 2, 3.\n",
    "   8. breast: left, right.\n",
    "   9. breast-quad: left-up, left-low, right-up,\tright-low, central.\n",
    "  10. irradiat:\tyes, no.\n",
    "Missing Attribute Values: (denoted by \"?\")\n",
    "   Attribute #:  Number of instances with missing values:\n",
    "   6.             8\n",
    "   9.             1.\n",
    "Class Distribution:\n",
    "    1. no-recurrence-events: 201 instances\n",
    "    2. recurrence-events: 85 instances \n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "* Breast Cancer Dataset ([breast-cancer.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv))\n",
    "* Breast Cancer Dataset Description ([breast-cancer.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Breast Cancer data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the breast cancer dataset\n",
    "# The dataset is assumed to be located in the 'Data' folder, relative to the current working directory\n",
    "breast_cancer_csv = \"../../Data/breast-cancer.csv\"  # Path to the CSV file containing the breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and encoding the categorical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of loading and preparing the breast cancer dataset\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # Load the dataset from the provided CSV file.\n",
    "    # 'read_csv' reads the file into a DataFrame without headers (header=None)\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # Retrieve the dataset as a NumPy array, including both input and output variables.\n",
    "    data = dataset.values\n",
    "\n",
    "    # Split the data into input variables (X) and output variable (y).\n",
    "    # 'X' contains all columns except the last (input features), while 'y' contains the last column (target/output)\n",
    "    X = data[:, :-1]  # Input features (all rows, all columns except the last one)\n",
    "    y = data[:, -1]   # Output (target values, all rows, only the last column)\n",
    "\n",
    "    # Format all input features (X) as strings.\n",
    "    # This ensures that the input data is treated as categorical or textual if necessary.\n",
    "    X = X.astype(str)\n",
    "\n",
    "    # Return the input features (X) and output (y).\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Function to prepare and encode the input data for training and testing\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    # Create an OrdinalEncoder instance to convert categorical data into integer codes\n",
    "    oe = OrdinalEncoder()  # encode each variable to integers\n",
    "\n",
    "    # Fit the encoder on the training data to learn the unique categories\n",
    "    oe.fit(X_train)\n",
    "\n",
    "    # Transform both the training and testing data using the fitted encoder\n",
    "    X_train_enc = oe.transform(X_train)  # Encode the training data\n",
    "    X_test_enc = oe.transform(X_test)    # Encode the test data\n",
    "\n",
    "    # Return the encoded training and testing data\n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "\n",
    "\n",
    "# Function to prepare and encode target variables for training and testing datasets\n",
    "def prepare_targets(y_train, y_test):\n",
    "    # Initialize the LabelEncoder, which converts categorical labels into numeric labels\n",
    "    le = LabelEncoder()  # LabelEncoder is designed for encoding a single variable\n",
    "    \n",
    "    # Fit the encoder on the training data to learn the mapping of labels\n",
    "    le.fit(y_train)\n",
    "    \n",
    "    # Transform both the training and testing labels into numeric form\n",
    "    y_train_enc = le.transform(y_train)  # Encode the training labels\n",
    "    y_test_enc = le.transform(y_test)    # Encode the testing labels\n",
    "    \n",
    "    # Return the encoded labels for both training and testing datasets\n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "\n",
    "# Load the breast cancer dataset (assumed to be in CSV format)\n",
    "# X represents the input features, and y represents the target labels\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# 33% of the data will be used for testing (test_size=0.33), and 67% for training\n",
    "# random_state ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# Prepare the input data by applying necessary preprocessing to the features\n",
    "# X_train_enc and X_test_enc are the encoded (transformed) input data for training and testing\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# Prepare the output data by encoding or transforming the target labels\n",
    "# y_train_enc and y_test_enc are the encoded target data for training and testing\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# Print the shapes of the training and testing datasets to confirm the splitting and preprocessing\n",
    "print(\"Train\", X_train_enc.shape, y_train_enc.shape)\n",
    "print(\"Test\", X_test_enc.shape, y_test_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 191 examples for training and 95 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Feature Selection\n",
    "\n",
    "There are two popular feature selection techniques that can be used for categorical input data\n",
    "and a categorical (class) target variable. They are:\n",
    "* Chi-Squared Statistic.\n",
    "* Mutual Information Statistic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared Feature Selection\n",
    "\n",
    "Pearson's chi-squared statistical hypothesis\n",
    "test is an example of a test for independence between categorical variables. The results of this\n",
    "test can be used for feature selection, where those features that are independent of the target\n",
    "variable can be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of chi squared feature selection for categorical data\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # Select features according to the k highest scores.\n",
    "    # k : int or \"all\", default=10, Number of top features to select.\n",
    "    # The \"all\" option bypasses selection, for use in a parameter search.\n",
    "    fs = SelectKBest(score_func=chi2, k=\"all\")\n",
    "    # Run score function on (X, y) and get the appropriate features.\n",
    "    fs.fit(X_train, y_train)\n",
    "    # Reduce X to the selected features.\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "    print(\"Feature %d: %f\" % (i, fs.scores_[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see the scores are small and it is hard to get an idea from the number\n",
    "alone as to which features are more relevant. Perhaps features 3, 4, 5, and 8 are most relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature selection scores\n",
    "\n",
    "# 'fs.scores_' contains the importance or scores of each feature (e.g., from feature selection)\n",
    "# A bar plot is created using pyplot.bar to visualize the scores of the features\n",
    "# The x-axis represents the feature indices (e.g., 0, 1, 2, ..., len(fs.scores_)-1)\n",
    "# The y-axis represents the corresponding scores of each feature\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "\n",
    "# Display the plot\n",
    "pyplot.show()  # Show the generated bar plot with feature scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bar chart of the feature importance scores for each input feature is created. This clearly\n",
    "shows that feature 3 might be the most relevant (according to chi-squared) and that perhaps\n",
    "four of the nine input features are the most relevant. We could set k = 4 when configuring the\n",
    "SelectKBest to select these top four features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Feature Selection\n",
    "\n",
    "Mutual information from the field of information theory is the application of information gain\n",
    "(typically used in the construction of decision trees) to feature selection. Mutual information is\n",
    "calculated between two variables and measures the reduction in uncertainty for one variable\n",
    "given a known value of the other variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of mutual information feature selection for categorical data\n",
    "\n",
    "# Function to load a dataset from a file\n",
    "def load_dataset(filename):\n",
    "    # Load the dataset from a CSV file as a pandas DataFrame.\n",
    "    # 'header=None' indicates that the dataset has no header row (column names).\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # Retrieve the data as a numpy array, which includes both inputs and outputs.\n",
    "    data = dataset.values\n",
    "\n",
    "    # Split the dataset into input variables (X) and output variable (y).\n",
    "    # X will contain all columns except the last one (input features).\n",
    "    # y will contain only the last column (output/target).\n",
    "    X = data[:, :-1]  # All columns except the last one for inputs\n",
    "    y = data[:, -1]   # Only the last column for the output\n",
    "\n",
    "    # Format all fields in the input array (X) as strings.\n",
    "    # This is useful for certain data types that might need to be handled as categorical strings.\n",
    "    X = X.astype(str)\n",
    "\n",
    "    # Return the input variables (X) and output variables (y)\n",
    "    return X, y\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "    print(\"Feature %d: %f\" % (i, fs.scores_[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that some of the features have a very low score, suggesting that\n",
    "perhaps they can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature scores to visualize their importance\n",
    "\n",
    "# 'fs.scores_' contains the scores (or importance values) of the features\n",
    "# We use 'range(len(fs.scores_))' to generate x-axis positions for each feature score\n",
    "# 'pyplot.bar' creates a bar chart with the feature indices on the x-axis and their corresponding scores on the y-axis\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)  # Create a bar plot for feature scores\n",
    "\n",
    "# Display the plot\n",
    "pyplot.show()  # Show the generated bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bar chart of the feature importance scores for each input feature is created. Importantly,\n",
    "a different mixture of features is promoted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling With Selected Features\n",
    "\n",
    "A robust approach is to evaluate models using different\n",
    "feature selection methods (and numbers of features) and select the method that results in a\n",
    "model with the best performance. We will evaluate a Logistic Regression model\n",
    "with all features compared to a model built from features selected by chi-squared and those\n",
    "features selected via mutual information. Logistic regression is a good model for testing feature\n",
    "selection methods as it can perform better if irrelevant features are removed from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Built Using All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model using all input features\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(X_train_enc, y_train_enc)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_enc)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print(\"Accuracy: %.2f\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the model achieves a classification accuracy of about 75 percent.\n",
    "We would prefer to use a subset of features that achieves a classification accuracy that is as\n",
    "good or better than this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Built Using Chi-Squared Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model fit using chi squared input features\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # retrieve numpy array\n",
    "    data = dataset.values\n",
    "\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    # format all fields as string\n",
    "    X = X.astype(str)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    oe = OrdinalEncoder()\n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train)\n",
    "    X_test_enc = oe.transform(X_test)\n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=chi2, k=4)\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(X_train_fs, y_train_enc)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print(\"Accuracy: %.2f\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that the model achieved an accuracy of about 74 percent, a slight drop in\n",
    "performance. It is possible that some of the features removed are, in fact, adding value directly\n",
    "or in concert with the selected features. At this stage, we would probably prefer to use all of\n",
    "the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Built Using Mutual Information Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model fit using mutual information input features\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(X_train_fs, y_train_enc)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print(\"Accuracy: %.2f\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see drop in classification accuracy. To be sure that\n",
    "the effect is real, it would be a good idea to repeat each experiment multiple times and compare\n",
    "the mean performance. It may also be a good idea to explore using k-fold cross-validation\n",
    "instead of a simple train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We explored different feature selection techniques for categorical data, specifically chi-squared and mutual information statistics. The analysis showed that while feature selection can help identify important variables, using all features may sometimes yield better performance. The choice of feature selection method should be validated through careful model evaluation.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to:\n",
    "- Close any open plot windows\n",
    "- Clear any stored variables\n",
    "- Shutdown the notebook kernel when finished\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
