{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection: Select Categorical Input Features\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Feature selection is the process of identifying and selecting a subset of input features that are most relevant to the target variable. While feature selection is often straightforward for **real-valued data** (e.g., using Pearson's correlation coefficient), it can be more challenging when working with **categorical data**.\n",
    "\n",
    "When both the input features and the target variable are categorical (e.g., in classification tasks), the two most commonly used feature selection methods are:\n",
    "\n",
    "1. **Chi-Squared Statistic**:\n",
    "   - Measures the dependence between each feature and the target variable.\n",
    "   - Suitable for categorical data with a categorical target.\n",
    "\n",
    "2. **Mutual Information Statistic**:\n",
    "   - Quantifies the amount of information obtained about the target variable through each feature.\n",
    "   - Effective for capturing non-linear relationships between features and the target.\n",
    "\n",
    "These methods help identify the most relevant features, improving model performance and interpretability.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn about the breast cancer predictive modeling problem with categorical inputs and binary classification target variable\n",
    "- Understand how to evaluate the importance of categorical features using the chi-squared and mutual information statistics\n",
    "- Learn how to perform feature selection for categorical data when fitting and evaluating a classification model\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Evaluate models using different feature selection methods\n",
    "- Compare performance between full feature set and selected features\n",
    "- Create visualizations of feature importance scores\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Basic understanding of classification models\n",
    "- Knowledge of basic statistical concepts\n",
    "\n",
    "## Get Started\n",
    "\n",
    "Setup steps:\n",
    "- Import required libraries (matplotlib, pandas, scikit-learn)\n",
    "- Download breast cancer dataset\n",
    "- Prepare data loading and preprocessing functions\n",
    "- Set up feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary packages using pip in a Jupyter notebook environment\n",
    "\n",
    "# 'matplotlib' is a plotting library for creating static, interactive, and animated visualizations\n",
    "%pip install matplotlib \n",
    "\n",
    "# 'numpy' is a fundamental package for scientific computing in Python, providing support for arrays and matrices\n",
    "%pip install numpy\n",
    "\n",
    "# 'pandas' is a powerful data manipulation and analysis library, often used for working with structured data\n",
    "%pip install pandas\n",
    "\n",
    "# 'scikit-learn' is a machine learning library that provides simple and efficient tools for data mining and data analysis\n",
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary modules and classes\n",
    "\n",
    "# Importing pyplot from matplotlib for plotting graphs and visualizations\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Importing read_csv from pandas to load CSV data into a DataFrame\n",
    "from pandas import read_csv\n",
    "\n",
    "# Importing SelectKBest and feature selection methods for selecting the best features\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "# Importing LogisticRegression from sklearn to use as a machine learning model for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Importing accuracy_score from sklearn to evaluate the performance of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Importing train_test_split from sklearn to split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing LabelEncoder and OrdinalEncoder for encoding categorical features as numeric values\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Dataset\n",
    "\n",
    "Breast cancer dataset classifies breast cancer\n",
    "patient data as either a recurrence or no recurrence of cancer. \n",
    "\n",
    "```\n",
    "Number of Instances: 286\n",
    "Number of Attributes: 9 + the class attribute\n",
    "Attribute Information:\n",
    "   1. Class: no-recurrence-events, recurrence-events\n",
    "   2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "   3. menopause: lt40, ge40, premeno.\n",
    "   4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
    "   5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
    "   6. node-caps: yes, no.\n",
    "   7. deg-malig: 1, 2, 3.\n",
    "   8. breast: left, right.\n",
    "   9. breast-quad: left-up, left-low, right-up,\tright-low, central.\n",
    "  10. irradiat:\tyes, no.\n",
    "Missing Attribute Values: (denoted by \"?\")\n",
    "   Attribute #:  Number of instances with missing values:\n",
    "   6.             8\n",
    "   9.             1.\n",
    "Class Distribution:\n",
    "    1. no-recurrence-events: 201 instances\n",
    "    2. recurrence-events: 85 instances \n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "* Breast Cancer Dataset ([breast-cancer.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv))\n",
    "* Breast Cancer Dataset Description ([breast-cancer.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and encoding the categorical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the breast cancer dataset\n",
    "# The dataset is assumed to be located in the 'Data' folder, relative to the current working directory\n",
    "breast_cancer_csv = \"../../Data/breast-cancer.csv\"  # Path to the CSV file containing the breast cancer dataset\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_dataset(filename):\n",
    "    # Load the dataset from the provided CSV file.\n",
    "    # 'read_csv' reads the file into a DataFrame without headers (header=None)\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # Retrieve the dataset as a NumPy array, including both input and output variables.\n",
    "    data = dataset.values\n",
    "\n",
    "    # Split the data into input variables (X) and output variable (y).\n",
    "    # 'X' contains all columns except the last (input features), while 'y' contains the last column (target/output)\n",
    "    X = data[:, :-1]  # Input features (all rows, all columns except the last one)\n",
    "    y = data[:, -1]   # Output (target values, all rows, only the last column)\n",
    "\n",
    "    # Format all input features (X) as strings.\n",
    "    # This ensures that the input data is treated as categorical or textual if necessary.\n",
    "    X = X.astype(str)\n",
    "\n",
    "    # Return the input features (X) and output (y).\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Function to prepare and encode the input data for training and testing\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    # Create an OrdinalEncoder instance to convert categorical data into integer codes\n",
    "    oe = OrdinalEncoder()  # encode each variable to integers\n",
    "\n",
    "    # Fit the encoder on the training data to learn the unique categories\n",
    "    oe.fit(X_train)\n",
    "\n",
    "    # Transform both the training and testing data using the fitted encoder\n",
    "    X_train_enc = oe.transform(X_train)  # Encode the training data\n",
    "    X_test_enc = oe.transform(X_test)    # Encode the test data\n",
    "\n",
    "    # Return the encoded training and testing data\n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "# Function to prepare and encode target variables for training and testing datasets\n",
    "def prepare_targets(y_train, y_test):\n",
    "    # Initialize the LabelEncoder, which converts categorical labels into numeric labels\n",
    "    le = LabelEncoder()  # LabelEncoder is designed for encoding a single variable\n",
    "    \n",
    "    # Fit the encoder on the training data to learn the mapping of labels\n",
    "    le.fit(y_train)\n",
    "    \n",
    "    # Transform both the training and testing labels into numeric form\n",
    "    y_train_enc = le.transform(y_train)  # Encode the training labels\n",
    "    y_test_enc = le.transform(y_test)    # Encode the testing labels\n",
    "    \n",
    "    # Return the encoded labels for both training and testing datasets\n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "\n",
    "# Load the breast cancer dataset (assumed to be in CSV format)\n",
    "# X represents the input features, and y represents the target labels\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# 33% of the data will be used for testing (test_size=0.33), and 67% for training\n",
    "# random_state ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# Prepare the input data by applying necessary preprocessing to the features\n",
    "# X_train_enc and X_test_enc are the encoded (transformed) input data for training and testing\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# Prepare the output data by encoding or transforming the target labels\n",
    "# y_train_enc and y_test_enc are the encoded target data for training and testing\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# Print the shapes of the training and testing datasets to confirm the splitting and preprocessing\n",
    "print(\"Train\", X_train_enc.shape, y_train_enc.shape)\n",
    "print(\"Test\", X_test_enc.shape, y_test_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been partitioned into 191 training examples and 95 test examples, resulting in a roughly 2:1 train-test split that provides sufficient data for model development while retaining an adequate holdout set for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Feature Selection\n",
    "\n",
    "When working with categorical input features and a categorical target variable, two statistically-grounded feature selection methods are particularly effective:\n",
    "* Chi-Squared Statistic.\n",
    "* Mutual Information Statistic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared Feature Selection\n",
    "\n",
    "**Pearson's chi-squared test** is a statistical hypothesis test used to assess the independence between categorical variables. The results of this test can be leveraged for **feature selection** in machine learning. Specifically:\n",
    "\n",
    "- **Independent Features**: Features that are found to be independent of the target variable can be removed from the dataset.\n",
    "- **Dependent Features**: Features that show a significant dependence on the target variable are retained for model training.\n",
    "\n",
    "By applying Pearson's chi-squared test, you can identify and eliminate irrelevant features, improving model efficiency and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of chi squared feature selection for categorical data\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # Select features according to the k highest scores.\n",
    "    # Initialize the SelectKBest feature selection method\n",
    "    # - score_func=chi2: Use the chi-squared statistic to score features\n",
    "    # - k=\"all\": Evaluate all features (no feature selection is performed yet)\n",
    "    fs = SelectKBest(score_func=chi2, k=\"all\")\n",
    "    \n",
    "    # Run score function on (X, y) and get the appropriate features.\n",
    "    fs.fit(X_train, y_train)\n",
    "    \n",
    "    # Reduce X to the selected features.\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    \n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# 33% of the data will be used for testing (test_size=0.33), and 67% for training\n",
    "# random_state ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# Prepare the input data by applying necessary preprocessing to the features\n",
    "# X_train_enc and X_test_enc are the encoded (transformed) input data for training and testing\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# Prepare the output data by encoding or transforming the target labels\n",
    "# y_train_enc and y_test_enc are the encoded target data for training and testing\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# Feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "\n",
    "# what are scores for the features\n",
    "# The fs.scores_ attribute in scikit-learn's chi2 feature selector represents the raw chi-squared (χ²) statistic values (not normalized importance scores)\n",
    "# For each feature, this is the computed χ² value from the Pearson's chi-squared test of independence between the feature and target\n",
    "# Higher values indicate stronger dependence between the feature and target\n",
    "for i in range(len(fs.scores_)):\n",
    "    print(\"Feature %d: %f\" % (i, fs.scores_[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed feature importance scores exhibit relatively low magnitudes across all variables, making it challenging to discern clear patterns from the numerical values alone. However, preliminary interpretation suggests features 3, 4, 5, and 8 may hold greater predictive relevance, as they consistently appear at the upper end of the score distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature selection scores\n",
    "\n",
    "# 'fs.scores_' contains the importance or scores of each feature (e.g., from feature selection)\n",
    "# A bar plot is created using pyplot.bar to visualize the scores of the features\n",
    "# The x-axis represents the feature indices (e.g., 0, 1, 2, ..., len(fs.scores_)-1)\n",
    "# The y-axis represents the corresponding scores of each feature\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "\n",
    "# Add x-axis and y-axis labels\n",
    "pyplot.xlabel(\"Feature Indices\")\n",
    "pyplot.ylabel(\"Feature Scores\")\n",
    "\n",
    "# Set x-axis ticks to show every integer (0, 1, 2, 3, ...)\n",
    "pyplot.xticks([i for i in range(len(fs.scores_))])\n",
    "\n",
    "# Display the plot\n",
    "pyplot.show()  # Show the generated bar plot with feature scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance Visualization\n",
    "\n",
    "The **bar chart** visualizes the feature importance scores for each input feature. The chart reveals the following insights:\n",
    "\n",
    "- **Most Relevant Feature**: Feature 3 appears to be the most relevant according to the chi-squared statistic.\n",
    "- **Top Features**: Approximately **four out of the nine input features** show significantly higher importance scores compared to the others.\n",
    "\n",
    "##### Actionable Insight\n",
    "Based on this analysis, we can configure the `SelectKBest` feature selection method to retain only the **top four features** by setting `k=4`. This will help streamline the dataset and improve model efficiency without sacrificing predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Feature Selection\n",
    "\n",
    "**Mutual information** is a concept from **information theory** that is widely applied in feature selection. It is based on the idea of **information gain**, which is commonly used in the construction of decision trees. Here's how it works:\n",
    "\n",
    "- **Definition**: Mutual information measures the reduction in uncertainty for one variable when the value of another variable is known.\n",
    "- **Application**: It quantifies the dependency between two variables, making it a powerful tool for identifying relevant features in a dataset.\n",
    "\n",
    "By calculating mutual information between each input feature and the target variable, we can determine which features provide the most information for predicting the target, enabling effective feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of mutual information feature selection for categorical data\n",
    "# Feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Perform feature selection using the SelectKBest method with mutual information.\n",
    "\n",
    "    Args:\n",
    "        X_train (array-like): Training input features.\n",
    "        y_train (array-like): Training target variable.\n",
    "        X_test (array-like): Testing input features.\n",
    "\n",
    "    Returns:\n",
    "        X_train_fs (array-like): Transformed training features after feature selection.\n",
    "        X_test_fs (array-like): Transformed testing features after feature selection.\n",
    "        fs (object): Fitted SelectKBest object for further analysis.\n",
    "    \"\"\"\n",
    "    # Initialize SelectKBest with mutual information as the scoring function\n",
    "    # - score_func=mutual_info_classif: Use mutual information for feature scoring\n",
    "    # - k='all': Evaluate all features (no feature selection is performed yet)\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "\n",
    "    # Fit the feature selector on the training data\n",
    "    fs.fit(X_train, y_train)\n",
    "\n",
    "    # Transform the training and testing data using the fitted selector\n",
    "    X_train_fs = fs.transform(X_train)  # Apply feature selection to training data\n",
    "    X_test_fs = fs.transform(X_test)    # Apply feature selection to testing data\n",
    "\n",
    "    # Return the transformed datasets and the fitted feature selector object\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "\n",
    "# Function to load a dataset from a file\n",
    "def load_dataset(filename):\n",
    "    # Load the dataset from a CSV file as a pandas DataFrame.\n",
    "    # 'header=None' indicates that the dataset has no header row (column names).\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # Retrieve the data as a numpy array, which includes both inputs and outputs.\n",
    "    data = dataset.values\n",
    "\n",
    "    # Split the dataset into input variables (X) and output variable (y).\n",
    "    # X will contain all columns except the last one (input features).\n",
    "    # y will contain only the last column (output/target).\n",
    "    X = data[:, :-1]  # All columns except the last one for inputs\n",
    "    y = data[:, -1]   # Only the last column for the output\n",
    "\n",
    "    # Format all fields in the input array (X) as strings.\n",
    "    # This is useful for certain data types that might need to be handled as categorical strings.\n",
    "    X = X.astype(str)\n",
    "\n",
    "    # Return the input variables (X) and output variables (y)\n",
    "    return X, y\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# 33% of the data will be used for testing (test_size=0.33), and 67% for training\n",
    "# random_state ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# Prepare the input data by applying necessary preprocessing to the features\n",
    "# X_train_enc and X_test_enc are the encoded (transformed) input data for training and testing\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# Prepare the output data by encoding or transforming the target labels\n",
    "# y_train_enc and y_test_enc are the encoded target data for training and testing\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# Feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "    print(\"Feature %d: %f\" % (i, fs.scores_[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance analysis reveals several variables with notably low statistical scores, indicating their limited predictive value for the target variable. These underperforming features demonstrate either minimal correlation with the outcome or redundant information already captured by other variables. Their removal could potentially streamline the model without sacrificing accuracy, while offering benefits such as reduced computational overhead, improved interpretability, and possibly better generalization by eliminating noise. However, we should validate this through ablation testing - comparing model performance with and without these features - before finalizing the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature scores to visualize their importance\n",
    "\n",
    "# 'fs.scores_' contains the scores (or importance values) of the features\n",
    "# We use 'range(len(fs.scores_))' to generate x-axis positions for each feature score\n",
    "# 'pyplot.bar' creates a bar chart with the feature indices on the x-axis and their corresponding scores on the y-axis\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)  # Create a bar plot for feature scores\n",
    "\n",
    "# Add x-axis and y-axis labels\n",
    "pyplot.xlabel(\"Feature Indices\")\n",
    "pyplot.ylabel(\"Feature Scores\")\n",
    "\n",
    "# Set x-axis ticks to show every integer (0, 1, 2, 3, ...)\n",
    "pyplot.xticks([i for i in range(len(fs.scores_))])\n",
    "\n",
    "# Display the plot\n",
    "pyplot.show()  # Show the generated bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance analysis, visualized through a bar chart, reveals distinct patterns in variable significance compared to previous methods. Notably, this approach highlights a substantially different subset of predictive features, suggesting that:\n",
    "* The current selection technique prioritizes alternative data relationships\n",
    "* Complementary information may exist across different feature sets\n",
    "* Model performance could benefit from ensemble feature selection strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling With Selected Features\n",
    "\n",
    "A robust approach to feature selection involves evaluating models using different methods and varying numbers of features, then selecting the approach that yields the best model performance. In this analysis, we will compare the performance of a **Logistic Regression model** under three scenarios:\n",
    "\n",
    "1. **All Features**: The model is trained using all available input features.\n",
    "2. **Chi-Squared Feature Selection**: The model is trained using features selected by the **chi-squared statistic**.\n",
    "3. **Mutual Information Feature Selection**: The model is trained using features selected by **mutual information**.\n",
    "\n",
    "### Why Logistic Regression?\n",
    "Logistic Regression is an excellent choice for testing feature selection methods because:\n",
    "- It can **benefit significantly** from the removal of irrelevant features.\n",
    "- It provides a clear baseline for evaluating the impact of feature selection on model performance.\n",
    "\n",
    "By comparing the results of these three scenarios, we can determine the most effective feature selection method for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Built Using All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the specified CSV file defined by a variable 'breast_cancer_csv'\n",
    "# - X: Input features\n",
    "# - y: Target variable\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# - test_size=0.33: 33% of the data is used for testing\n",
    "# - random_state=1: Ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# Prepare the input data (e.g., encoding categorical variables, scaling, etc.)\n",
    "# - X_train_enc: Transformed training input features\n",
    "# - X_test_enc: Transformed testing input features\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# Prepare the output data (e.g., encoding the target variable)\n",
    "# - y_train_enc: Encoded training target variable\n",
    "# - y_test_enc: Encoded testing target variable\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# Initialize a Logistic Regression model\n",
    "# - solver=\"lbfgs\": Specifies the optimization algorithm\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "\n",
    "# Train the model on the encoded training data\n",
    "model.fit(X_train_enc, y_train_enc)\n",
    "\n",
    "# Use the trained model to make predictions on the encoded test data\n",
    "yhat = model.predict(X_test_enc)\n",
    "\n",
    "# Evaluate the model's predictions by calculating accuracy\n",
    "# - accuracy_score: Compares predicted values (yhat) with actual values (y_test_enc)\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(\"Accuracy: %.2f\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current model configuration achieves a baseline classification accuracy of 76%. Our feature selection objective is to identify an optimal subset that maintains or improves upon this performance threshold while reducing feature dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Built Using Chi-Squared Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file\n",
    "def load_dataset(filename):\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file and split it into input (X) and output (y) variables.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        X (numpy array): Input features.\n",
    "        y (numpy array): Target variable.\n",
    "    \"\"\"\n",
    "    # Load the dataset as a pandas DataFrame\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # Retrieve the numpy array from the DataFrame\n",
    "    data = dataset.values\n",
    "\n",
    "    # Split into input (X) and output (y) variables\n",
    "    X = data[:, :-1]  # All columns except the last\n",
    "    y = data[:, -1]   # Last column\n",
    "\n",
    "    # Format all fields as strings\n",
    "    X = X.astype(str)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Prepare input data by encoding categorical variables\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Encode categorical input features using OrdinalEncoder.\n",
    "\n",
    "    Args:\n",
    "        X_train (numpy array): Training input features.\n",
    "        X_test (numpy array): Testing input features.\n",
    "\n",
    "    Returns:\n",
    "        X_train_enc (numpy array): Encoded training input features.\n",
    "        X_test_enc (numpy array): Encoded testing input features.\n",
    "    \"\"\"\n",
    "    # Initialize the OrdinalEncoder\n",
    "    oe = OrdinalEncoder()\n",
    "\n",
    "    # Fit the encoder on the training data and transform both training and testing data\n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train)\n",
    "    X_test_enc = oe.transform(X_test)\n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "\n",
    "# Prepare target data by encoding labels\n",
    "def prepare_targets(y_train, y_test):\n",
    "    \"\"\"\n",
    "    Encode the target variable using LabelEncoder.\n",
    "\n",
    "    Args:\n",
    "        y_train (numpy array): Training target variable.\n",
    "        y_test (numpy array): Testing target variable.\n",
    "\n",
    "    Returns:\n",
    "        y_train_enc (numpy array): Encoded training target variable.\n",
    "        y_test_enc (numpy array): Encoded testing target variable.\n",
    "    \"\"\"\n",
    "    # Initialize the LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Fit the encoder on the training data and transform both training and testing targets\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "\n",
    "# Perform feature selection using SelectKBest with chi-squared statistic\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Select the top k features using the chi-squared statistic.\n",
    "\n",
    "    Args:\n",
    "        X_train (numpy array): Training input features.\n",
    "        y_train (numpy array): Training target variable.\n",
    "        X_test (numpy array): Testing input features.\n",
    "\n",
    "    Returns:\n",
    "        X_train_fs (numpy array): Transformed training features with selected features.\n",
    "        X_test_fs (numpy array): Transformed testing features with selected features.\n",
    "    \"\"\"\n",
    "    # Initialize SelectKBest with chi-squared statistic and k=4 (top 4 features)\n",
    "    fs = SelectKBest(score_func=chi2, k=4)\n",
    "\n",
    "    # Fit the feature selector on the training data\n",
    "    fs.fit(X_train, y_train)\n",
    "\n",
    "    # Transform the training and testing data using the fitted selector\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs\n",
    "\n",
    "\n",
    "# Load the dataset from a CSV defined by a variable called breast\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# - test_size=0.33: 33% of the data is used for testing\n",
    "# - random_state=1: Ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# Prepare input data by encoding categorical variables\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# Prepare output data by encoding the target variable\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# Perform feature selection to select the top 4 features\n",
    "X_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "\n",
    "# Initialize and fit a Logistic Regression model\n",
    "# - solver=\"lbfgs\": Specifies the optimization algorithm\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(X_train_fs, y_train_enc)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "yhat = model.predict(X_test_fs)\n",
    "\n",
    "# Evaluate the model's predictions by calculating accuracy\n",
    "# - accuracy_score: Compares predicted values (yhat) with actual values (y_test_enc)\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(\"Accuracy: %.2f\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's accuracy of 75% represents a modest but notable decrease in performance compared to previous results. This degradation suggests that some of the eliminated features may have contributed meaningful predictive value, either independently or through interactions with other variables. Given these findings, retaining the complete set of input features appears to be the more prudent approach at this stage of development. The marginal reduction in dimensionality does not justify the corresponding performance trade-off, particularly when considering potential feature synergies that may be critical for optimal model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Built Using Mutual Information Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file\n",
    "def load_dataset(filename):\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file and split it into input (X) and output (y) variables.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        X (numpy array): Input features.\n",
    "        y (numpy array): Target variable.\n",
    "    \"\"\"\n",
    "    # Load the dataset as a pandas DataFrame\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # Retrieve the numpy array from the DataFrame\n",
    "    data = dataset.values\n",
    "\n",
    "    # Split into input (X) and output (y) variables\n",
    "    X = data[:, :-1]  # All columns except the last\n",
    "    y = data[:, -1]   # Last column\n",
    "\n",
    "    # Format all fields as strings\n",
    "    X = X.astype(str)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Prepare input data by encoding categorical variables\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Encode categorical input features using OrdinalEncoder.\n",
    "\n",
    "    Args:\n",
    "        X_train (numpy array): Training input features.\n",
    "        X_test (numpy array): Testing input features.\n",
    "\n",
    "    Returns:\n",
    "        X_train_enc (numpy array): Encoded training input features.\n",
    "        X_test_enc (numpy array): Encoded testing input features.\n",
    "    \"\"\"\n",
    "    # Initialize the OrdinalEncoder\n",
    "    oe = OrdinalEncoder()\n",
    "\n",
    "    # Fit the encoder on the training data and transform both training and testing data\n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train)\n",
    "    X_test_enc = oe.transform(X_test)\n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "\n",
    "# Prepare target data by encoding labels\n",
    "def prepare_targets(y_train, y_test):\n",
    "    \"\"\"\n",
    "    Encode the target variable using LabelEncoder.\n",
    "\n",
    "    Args:\n",
    "        y_train (numpy array): Training target variable.\n",
    "        y_test (numpy array): Testing target variable.\n",
    "\n",
    "    Returns:\n",
    "        y_train_enc (numpy array): Encoded training target variable.\n",
    "        y_test_enc (numpy array): Encoded testing target variable.\n",
    "    \"\"\"\n",
    "    # Initialize the LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Fit the encoder on the training data and transform both training and testing targets\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "\n",
    "# Perform feature selection using SelectKBest with mutual information\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Select the top k features using mutual information.\n",
    "\n",
    "    Args:\n",
    "        X_train (numpy array): Training input features.\n",
    "        y_train (numpy array): Training target variable.\n",
    "        X_test (numpy array): Testing input features.\n",
    "\n",
    "    Returns:\n",
    "        X_train_fs (numpy array): Transformed training features with selected features.\n",
    "        X_test_fs (numpy array): Transformed testing features with selected features.\n",
    "    \"\"\"\n",
    "    # Initialize SelectKBest with mutual information and k=4 (top 4 features)\n",
    "    # The mutual_info_classif function estimates mutual information between features and the target variable \n",
    "    # using a non-parametric method based on nearest neighbors (k-nearest neighbors, KNN).\n",
    "    # By default, it includes a small amount of noise to continuous variables to handle discretization, \n",
    "    # controlled by the random_state parameter. If random_state is not set (i.e., left as None), \n",
    "    # this noise addition introduces some randomness, which can lead to slight variations in results across runs. \n",
    "    # Setting random_state to a fixed integer (e.g., random_state=42) makes the process deterministic and reproducible.\n",
    "    fs = SelectKBest(score_func=lambda X, y: mutual_info_classif(X, y, random_state=42), k=4)\n",
    "\n",
    "    # Fit the feature selector on the training data\n",
    "    fs.fit(X_train, y_train)\n",
    "\n",
    "    # Transform the training and testing data using the fitted selector\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "X, y = load_dataset(breast_cancer_csv)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# - test_size=0.33: 33% of the data is used for testing\n",
    "# - random_state=1: Ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# Prepare input data by encoding categorical variables\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "# Prepare output data by encoding the target variable\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# Perform feature selection to select the top 4 features\n",
    "X_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "\n",
    "# Initialize and fit a Logistic Regression model\n",
    "# - solver=\"lbfgs\": Specifies the optimization algorithm\n",
    "# - random_state=1: Ensures reproducibility\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model.fit(X_train_fs, y_train_enc)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "yhat = model.predict(X_test_fs)\n",
    "\n",
    "# Evaluate the model's predictions by calculating accuracy\n",
    "# - accuracy_score: Compares predicted values (yhat) with actual values (y_test_enc)\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model demonstrates improved predictive performance with an accuracy of 78%, representing a meaningful increase over previous results. To validate whether this improvement reflects a genuine enhancement rather than random variation, we recommend implementing more rigorous evaluation protocols. Specifically, conducting multiple experiments with different random seeds would provide a distribution of performance metrics, while adopting k-fold cross-validation (with k=5 or k=10) would offer more reliable performance estimates by utilizing multiple train-test partitions. These approaches would not only confirm the significance of the observed improvement but also yield more statistically robust insights into the model's true generalization capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We explored different feature selection techniques for categorical data, specifically chi-squared and mutual information statistics. The analysis showed that while feature selection can help identify important variables, using all features may sometimes yield better performance. The choice of feature selection method should be validated through careful model evaluation.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to:\n",
    "- Close any open plot windows\n",
    "- Clear any stored variables\n",
    "- Shutdown the notebook kernel when finished\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
