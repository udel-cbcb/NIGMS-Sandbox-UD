{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: Derive New Input Variables\n",
    "\n",
    "Apdated from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module covers polynomial feature engineering for predictive modeling tasks. We explore how to create new features by transforming existing input variables using polynomial combinations and interactions between features.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn how machine learning algorithms perform with polynomial input features\n",
    "- Understand how to use polynomial features transform to create new versions of input variables\n",
    "- Examine how the degree of polynomial impacts the number of input features created\n",
    "- Apply polynomial feature transformation to real datasets\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Implement polynomial feature transforms\n",
    "- Evaluate model performance with different polynomial degrees\n",
    "- Compare results across transformations\n",
    "- Create visualizations of feature relationships\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.x environment\n",
    "- Basic understanding of Python programming\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with NumPy and scikit-learn libraries\n",
    "\n",
    "## Get Started\n",
    "\n",
    "Setup steps:\n",
    "\n",
    "- Install required Python packages:\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - scikit-learn\n",
    "  - matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages, import the necessary libraries, and define a helper function to download data using the `requests` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib numpy pandas requests scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import asarray, mean, std\n",
    "from pandas import DataFrame, read_csv\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility functions\n",
    "\n",
    "Define a helper function for downloading example datasets.  \n",
    "\n",
    "*Note!* It is not essential that you understand the following code.  It is just for getting the example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, to_file):\n",
    "    \"\"\"Download content from the given URL and save it to a file.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to download the content from.\n",
    "        to_file (str): The name of the file to save the downloaded content to.\n",
    "\n",
    "    \"\"\"\n",
    "    response = requests.get(url, timeout=10, headers={\"user-agent\": \"curl/7.81.0\"})\n",
    "    Path(to_file).write_bytes(response.content)\n",
    "    print(f\"downloaded file '{to_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features\n",
    "\n",
    "Polynomial features are those features created by raising existing features to an exponent. For\n",
    "example, if a dataset had one input feature X, then a polynomial feature would be the addition\n",
    "of a new feature (column) where values were calculated by squaring the values in X, e.g. $X^2$.\n",
    "This process can be repeated for each input variable in the dataset, creating a transformed\n",
    "version of each. As such, polynomial features are a type of feature engineering, e.g. the creation\n",
    "of new input features based on the existing features. The degree of the polynomial is used to\n",
    "control the number of features added, e.g. a degree of 3 will add two new variables for each\n",
    "input variable. Typically a small degree is used such as 2 or 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also common to add new variables that represent the interaction between features, e.g. \n",
    "a new column that represents one variable multiplied by another. This too can be repeated\n",
    "for each input variable creating a new interaction variable for each pair of input variables. A\n",
    "squared or cubed version of an input variable will change the probability distribution, separating\n",
    "the small and large values, a separation that is increased with the size of the exponent.\n",
    "\n",
    "This separation can help some machine learning algorithms make better predictions and is\n",
    "common for regression predictive modeling tasks and generally tasks that have numerical input\n",
    "variables. Typically linear algorithms, such as linear regression and logistic regression, respond\n",
    "well to the use of polynomial input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Feature Transform\n",
    "\n",
    "The polynomial features transform is available in the scikit-learn Python machine learning\n",
    "library via the `PolynomialFeatures` class.\n",
    "\n",
    "It generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate the types of polynomial features created\n",
    "\n",
    "# define the dataset\n",
    "data = asarray([[2, 3], [2, 3], [2, 3]])\n",
    "print(data)\n",
    "\n",
    "# perform a polynomial features transform of the dataset\n",
    "# generate polynomial and interaction features.\n",
    "# 'degree' specifies the maximal degree of the polynomial features\n",
    "trans = PolynomialFeatures(degree=2)\n",
    "data = trans.fit_transform(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the raw data with two features (columns) and each feature\n",
    "has the same value, either 2 or 3. Then the polynomial features are created, resulting in six\n",
    "features, matching what was described above.\n",
    "\n",
    "The *degree* argument controls the number of features created and defaults to 2. The\n",
    "*interaction_only* argument means that only the raw values (degree 1) and the interaction\n",
    "(pairs of values multiplied with each other) are included, defaulting to False. The *include_bias*\n",
    "argument defaults to True to include the bias feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sonar Dataset\n",
    "\n",
    "The sonar dataset is a standard machine learning dataset for binary classification. It involves\n",
    "60 real-valued inputs and a two-class target variable. The data set contains\n",
    "bouncing sonar\n",
    "signals off a metal cylinder or rocks obtained from a variety of different aspect angles. Each number\n",
    "represents the energy within a particular frequency band, integrated over\n",
    "a certain period of time. There are 208 examples in the dataset\n",
    "and the classes are reasonably balanced. The dataset describes sonar returns of rocks or simulated mines. You can learn more\n",
    "about the dataset from here:\n",
    "\n",
    "* Sonar Dataset ([sonar.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv))\n",
    "* Sonar Dataset Description ([sonar.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Sonar data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\n",
    "    url=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\",\n",
    "    to_file=\"sonar.csv\",\n",
    ")\n",
    "\n",
    "download(\n",
    "    url=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.names\",\n",
    "    to_file=\"sonar.names\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the variables from the sonar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the sonar dataset\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(\"sonar.csv\", header=None)\n",
    "print(dataset.head())\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# summarize each variable\n",
    "print(dataset.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the 60\n",
    "input variables, one output variable, and 208 rows of data. A statistical summary of the input\n",
    "variables is provided showing that values are numeric and range approximately from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a histogram is created for each input variable. If we ignore the clutter of the plots and\n",
    "focus on the histograms themselves, we can see that many variables have a skewed distribution.\n",
    "The dataset provides a good candidate for using a quantile transform to make the variables\n",
    "more-Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit and evaluate a machine learning model on the raw dataset. We will use\n",
    "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
    "stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the raw sonar dataset\n",
    "\n",
    "# KFold is a cross-validator that divides the dataset into k folds.\n",
    "#\n",
    "# Stratified is to ensure that each fold of dataset has the same proportion of\n",
    "#   observations with a given label.\n",
    "#\n",
    "# Repeated provides a way to improve the estimated performance of a machine\n",
    "#   learning model.\n",
    "#\n",
    "# This involves simply repeating the cross-validation procedure multiple times\n",
    "# and reporting the mean result across all folds from all runs. This mean result\n",
    "# is expected to be a more accurate estimate of the true unknown underlying mean\n",
    "# performance of the model on the dataset, as calculated using the standard\n",
    "# error.\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(\"sonar.csv\", header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype(\"float32\")\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define and configure the model\n",
    "# n_neighbors : int, default=5\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the model achieved a mean classification accuracy of about 79.7\n",
    "percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Feature Transform\n",
    "\n",
    "We can apply the polynomial features transform to the Sonar dataset directly. In this case, we\n",
    "will use a degree of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a polynomial features transform of the sonar dataset\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(\"sonar.csv\", header=None)\n",
    "\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a polynomial features transform of the dataset\n",
    "trans = PolynomialFeatures(degree=3)\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# summarize\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We\n",
    "can see that our features increased from 61 (60 input features) for the raw dataset to 39,711\n",
    "features (39,710 input features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
    "polynomial features transform of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the sonar dataset with polynomial features transform\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(\"sonar.csv\", header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype(\"float32\")\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define the pipeline\n",
    "trans = PolynomialFeatures(degree=3)\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report pipeline performance\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the polynomial features transform results in a lift in\n",
    "performance from 79.7 percent accuracy without the transform to about 80.0 percent with the\n",
    "transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Polynomial Degree\n",
    "\n",
    "The degree of the polynomial dramatically increases the number of input features. To get an\n",
    "idea of how much this impacts the number of features, we can perform the transform with a\n",
    "range of different degrees and compare the number of features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the effect of the degree on the number of created features\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset(filename):\n",
    "    # load dataset\n",
    "    dataset = read_csv(filename, header=None)\n",
    "    data = dataset.values\n",
    "    # separate into input and output columns\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    # ensure inputs are floats and output is an integer label\n",
    "    X = X.astype(\"float32\")\n",
    "    y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset(\"sonar.csv\")\n",
    "\n",
    "# calculate change in number of features\n",
    "num_features = []\n",
    "degrees = list(range(1, 5))\n",
    "for d in degrees:\n",
    "    # create transform\n",
    "    trans = PolynomialFeatures(degree=d)\n",
    "    # fit and transform\n",
    "    data = trans.fit_transform(X)\n",
    "    # record number of features\n",
    "    num_features.append(data.shape[1])\n",
    "    # summarize\n",
    "    print(\"Degree: %d, Features: %d\" % (d, data.shape[1]))\n",
    "\n",
    "# plot degree vs number of features\n",
    "plt.plot(degrees, num_features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a degree of 1 has no effect and that the number of features dramatically\n",
    "increases from 2 through to 4. This highlights that for anything other than very small datasets,\n",
    "a degree of 2 or 3 should be used to avoid a dramatic increase in input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More features may result in more overfitting, and in turn, worse results. It may be a good\n",
    "idea to treat the degree for the polynomial features transform as a hyperparameter and test\n",
    "different values for your dataset. The example below explores degree values from 1 to 4 and\n",
    "evaluates their effect on classification accuracy with the chosen model.\n",
    "\n",
    "*Note: The following code block could take a few minutes to complete.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the effect of degree on accuracy for the polynomial features transform\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset(filename):\n",
    "    # load dataset\n",
    "    dataset = read_csv(filename, header=None)\n",
    "    data = dataset.values\n",
    "    # separate into input and output columns\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    # ensure inputs are floats and output is an integer label\n",
    "    X = X.astype(\"float32\")\n",
    "    y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = {}\n",
    "    for d in range(1, 5):\n",
    "        # define the pipeline\n",
    "        trans = PolynomialFeatures(degree=d)\n",
    "        model = KNeighborsClassifier()\n",
    "        models[str(d)] = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "    return models\n",
    "\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # Feel free to adjust `n_jobs` to use as many cores as you would like.\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=1)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset(\"sonar.csv\")\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that performance is generally worse than no transform (degree\n",
    "1) except for a degree 3. It might be interesting to explore scaling the data before or after\n",
    "performing the transform to see how it impacts model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box and whisker plots can be created to summarize the classification accuracy scores for each\n",
    "polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance for comparison\n",
    "plt.boxplot(results, label=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that performance remains \n",
    "flat, perhaps with the first signs of\n",
    "overfitting with a degree of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this module, we learned how to create polynomial features to potentially improve model performance. We explored the impact of different polynomial degrees on feature space dimensionality and model accuracy. The techniques demonstrated show how feature engineering can expose non-linear relationships in data that may improve predictive modeling results.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.ï¿¼\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
