{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a354d9-27d8-4ff1-8b66-91fd2b93fb02",
   "metadata": {},
   "source": [
    "# Feature Engineering: Derive New Input Variables\n",
    "\n",
    "Apdated from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module covers polynomial feature engineering for predictive modeling tasks. We explore how to create new features by transforming existing input variables using polynomial combinations and interactions between features.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn how machine learning algorithms perform with polynomial input features\n",
    "- Understand how to use polynomial features transform to create new versions of input variables\n",
    "- Examine how the degree of polynomial impacts the number of input features created\n",
    "- Apply polynomial feature transformation to real datasets\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Implement polynomial feature transforms\n",
    "- Evaluate model performance with different polynomial degrees\n",
    "- Compare results across transformations\n",
    "- Create visualizations of feature relationships\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.x environment\n",
    "- Basic understanding of Python programming\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with NumPy and scikit-learn libraries\n",
    "\n",
    "## Get Started\n",
    "\n",
    "Setup steps:\n",
    "\n",
    "- Install required Python packages:\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - scikit-learn\n",
    "  - matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a842d-db96-4ddd-a04f-98dbbef96987",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654f4aa-7474-43a3-81cd-3e3d79046ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs matplotlib, a comprehensive library for creating static, interactive, and animated visualizations in Python.\n",
    "%pip install matplotlib\n",
    "\n",
    "# Installs numpy, the fundamental package for numerical computation in Python, providing support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "%pip install numpy\n",
    "\n",
    "# Installs pandas, a powerful data manipulation and analysis library, offering data structures like DataFrames for efficiently handling and analyzing structured data.\n",
    "%pip install pandas\n",
    "\n",
    "# Installs scikit-learn, a popular machine learning library providing simple and efficient tools for data mining and data analysis, built on NumPy, SciPy, and matplotlib, featuring various classification, regression and clustering algorithms.\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa004d3-abda-42fe-bb97-aca838130a5f",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa68b1a-676b-4fbf-ab21-3c32e191c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the warnings module to handle warning messages during execution.\n",
    "import warnings\n",
    "\n",
    "# Import the Path class from the pathlib module for working with file paths in an object-oriented way.\n",
    "from pathlib import Path\n",
    "\n",
    "# Import the pyplot module from matplotlib for plotting graphs and visualizations.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import the asarray, mean, and std functions from the numpy library for numerical operations, specifically array conversion, mean, and standard deviation calculations.\n",
    "from numpy import asarray, mean, std\n",
    "\n",
    "# Import the DataFrame and read_csv classes from the pandas library for data manipulation and reading CSV files.\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "# Import the RepeatedStratifiedKFold and cross_val_score classes from sklearn.model_selection for model evaluation using repeated stratified k-fold cross-validation.\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "\n",
    "# Import the KNeighborsClassifier class from sklearn.neighbors for using the K-Nearest Neighbors classification algorithm.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import the Pipeline class from sklearn.pipeline to construct composite estimators.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import the LabelEncoder and PolynomialFeatures classes from sklearn.preprocessing for data preprocessing tasks like encoding labels and generating polynomial features.\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545feec-b54e-498c-a26d-332cb6402f62",
   "metadata": {},
   "source": [
    "## Polynomial Features\n",
    "\n",
    "Polynomial features are generated by raising existing features to a specified exponent. For instance, if a dataset contains a single input feature \\( X \\), a polynomial feature can be created by squaring its values, resulting in a new feature \\( X^2 \\). This process can be applied to all input variables in the dataset, transforming each into higher-order terms. Polynomial features are a form of **feature engineering**, where new input features are derived from existing ones. The **degree** of the polynomial determines the number of new features added; for example, a degree of 3 would introduce two additional variables for each input feature. Typically, small degrees like 2 or 3 are used.\n",
    "\n",
    "Additionally, **interaction terms** can be created by multiplying pairs of input variables, generating new features that capture relationships between them. For instance, a new column might represent the product of two variables. Both polynomial and interaction terms can alter the probability distribution of the data, amplifying the separation between small and large values as the exponent increases. This separation can enhance the performance of certain machine learning algorithms, particularly in **regression tasks** or other problems involving numerical input variables. Linear algorithms, such as **linear regression** and **logistic regression**, often benefit significantly from the inclusion of polynomial features.\n",
    "\n",
    "## Polynomial Feature Transform\n",
    "\n",
    "The polynomial feature transform is implemented in the **scikit-learn** Python library through the `PolynomialFeatures` class. This tool creates a new feature matrix containing all polynomial combinations of the input features up to the specified degree. For example, given a two-dimensional input sample \\([a, b]\\), the degree-2 polynomial features would include \\([1, a, b, a^2, ab, b^2]\\). This transformation helps capture nonlinear relationships in the data, improving model performance in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe5fa1-ed9f-40b0-88dd-8a27a76d1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the types of polynomial features created\n",
    "# Import the asarray function from the numpy library to create arrays.\n",
    "from numpy import asarray\n",
    "\n",
    "# Import the PolynomialFeatures class from sklearn.preprocessing to generate polynomial features.\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Define the dataset as a numpy array with shape (3, 2).\n",
    "data = asarray([[2, 3], [2, 3], [2, 3]])\n",
    "\n",
    "# Print the original dataset to the console.\n",
    "print(\"Original Data:\")\n",
    "\n",
    "# Print the 'data' numpy array.\n",
    "print(data)\n",
    "\n",
    "# Create a PolynomialFeatures object, setting the degree of the polynomial to 2.\n",
    "trans = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Fit the PolynomialFeatures transformer to the 'data' and then transform 'data' into polynomial features.\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# Print a newline character and a message indicating the polynomial features are of degree 2.\n",
    "print(\"\\nPolynomial Features (Degree=2):\")\n",
    "\n",
    "# Print the 'data' array after the polynomial feature transformation.\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65296e9-936b-4d6d-9eb4-c95589e9ae9f",
   "metadata": {},
   "source": [
    "When the example is executed, it first displays the raw data, which consists of two features (columns), each containing identical values of either 2 or 3. Subsequently, polynomial features are generated, resulting in a total of six features, consistent with the description provided earlier.\n",
    "\n",
    "The degree parameter determines the number of features created and is set to 2 by default. The interaction_only parameter, which defaults to False, specifies whether to include only the raw values (degree 1) and their interactions (products of pairs of values). Additionally, the include_bias parameter is set to True by default, ensuring the inclusion of a bias feature in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916414c-66fe-4945-a6a9-01fd5fe15c69",
   "metadata": {},
   "source": [
    "## Wisconsin Breast Cancer Dataset\n",
    "\n",
    "The Wisconsin Breast Cancer Dataset (WBCD) is a widely used dataset for breast cancer diagnosis. It contains features computed from digitized images of fine needle aspirate (FNA) of breast masses, which describe characteristics of cell nuclei.\n",
    "\n",
    "### **Dataset Information:**\n",
    "- **Number of Instances:** 569\n",
    "- **Number of Features:** 30 (excluding the ID column)\n",
    "- **Target Variable:** Diagnosis (Malignant or Benign)\n",
    "\n",
    "### **Feature Description:**\n",
    "Each instance in the dataset has 30 real-valued features extracted from images of cell nuclei, including:\n",
    "- Radius (mean of distances from center to points on the perimeter)\n",
    "- Texture (standard deviation of gray-scale values)\n",
    "- Perimeter\n",
    "- Area\n",
    "- Smoothness\n",
    "- Compactness\n",
    "- Concavity\n",
    "- Concave points\n",
    "- Symmetry\n",
    "- Fractal dimension\n",
    "\n",
    "### **Insights:**\n",
    "- Features such as **concavity, concave points, and radius** tend to have higher importance in distinguishing malignant from benign tumors.\n",
    "- **Malignant tumors** generally have higher values in most of these features compared to benign ones.\n",
    "- The dataset is commonly used in machine learning models for binary classification.\n",
    "\n",
    "### **Source:**\n",
    "This dataset is available from the UCI Machine Learning Repository: [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n",
    "\n",
    "\n",
    "The **Wisconsin Breast Cancer Dataset** is crucial for predictive modeling in medical research. It provides meaningful insights into breast cancer diagnosis and is often used for testing machine learning algorithms like Support Vector Machines, Random Forest, and Neural Networks.\n",
    "\n",
    "Further analysis could involve feature selection techniques and hyperparameter tuning to optimize classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a56bb-af79-44a8-ad9a-00dadd407a1b",
   "metadata": {},
   "source": [
    "### Loading data and summarizing the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb96eaf-d2f9-442e-8fb6-441088ef5ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wisconsin Breast Cancer Dataset from ../../Data\n",
    "# Defines the file path to the Wisconsin Breast Cancer dataset.\n",
    "data_path = \"../../Data/wdbc.data\"\n",
    "\n",
    "# Load and summarize the dataset\n",
    "# Reads the data from the specified file path into a pandas DataFrame, assuming no header row.\n",
    "dataset = read_csv(data_path, header=None)\n",
    "\n",
    "# Add column names (from the dataset description)\n",
    "# Defines a list of column names for the dataset, based on the dataset description.\n",
    "columns = [\n",
    "    \"id\", \"diagnosis\",  # Target: M=Malignant, B=Benign\n",
    "    \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\",\n",
    "    \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\",\n",
    "    \"concave_points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\",\n",
    "    \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\",\n",
    "    \"smoothness_se\", \"compactness_se\", \"concavity_se\",\n",
    "    \"concave_points_se\", \"symmetry_se\", \"fractal_dimension_se\",\n",
    "    \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\",\n",
    "    \"smoothness_worst\", \"compactness_worst\", \"concavity_worst\",\n",
    "    \"concave_points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\"\n",
    "]\n",
    "# Assigns the defined list of column names to the DataFrame.\n",
    "dataset.columns = columns\n",
    "\n",
    "# Summarize the dataset\n",
    "# Prints a header indicating the display of the first 5 rows.\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "\n",
    "# Prints the first 5 rows of the DataFrame to display sample data.\n",
    "print(dataset.head())\n",
    "\n",
    "# Prints a header indicating the display of the dataset shape.\n",
    "print(\"\\nDataset Shape:\", dataset.shape)\n",
    "\n",
    "# Prints the shape of the DataFrame (number of rows and columns).\n",
    "print(\"\\nSummary Statistics:\")\n",
    "\n",
    "# Prints summary statistics for each numerical column in the DataFrame (count, mean, std, min, quartiles, max).\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c25a9-fa6f-4506-a5d0-81d5cf85cd9d",
   "metadata": {},
   "source": [
    "### Histograms of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b9ab7-06ca-4428-a2b8-e229c2a7be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates histograms for all columns in the 'dataset' DataFrame starting from the third column (index 2) to the end. Sets x and y label sizes, and figure size.\n",
    "fig = dataset.iloc[:, 2:].hist(xlabelsize=4, ylabelsize=4, figsize=(12, 10))\n",
    "\n",
    "# Iterates through each histogram subplot in the figure and sets the title size to 4.\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# Sets the suptitle (overall title) for the entire figure to \"Feature Distributions\", adjusting vertical position for better layout.\n",
    "plt.suptitle(\"Feature Distributions\", y=1.02)\n",
    "\n",
    "# Displays the generated histograms plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb97d9-57fe-4cd2-9a55-0b815fd5c1c1",
   "metadata": {},
   "source": [
    "### Evaluate KNN on the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94131782-84a2-4d4e-bd26-3515b11f482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into input and output columns\n",
    "X = dataset.iloc[:, 2:].values  # Features (all columns except ID and diagnosis)\n",
    "y = dataset[\"diagnosis\"].values  # Target (M=Malignant, B=Benign)\n",
    "\n",
    "# Encode labels (M=1, B=0)\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Define and configure the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Evaluate the model\n",
    "# Defines cross-validation splitting strategy using RepeatedStratifiedKFold for robust evaluation.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Performs cross-validation to evaluate the 'model' using features 'X' and target 'y', scoring by 'accuracy'.\n",
    "n_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report model performance\n",
    "print(\"\\nKNN Accuracy (Raw Data): %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb468a-f544-45dd-91a4-4361c14f2d11",
   "metadata": {},
   "source": [
    "### Visualize a polynomial features transform of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6ad1f-ea3c-4c18-9212-fdbaa31d9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a polynomial features transform of the dataset\n",
    "# Initialize the PolynomialFeatures transformer with a degree of 3\n",
    "# This will create polynomial features up to the third degree (e.g., X, X^2, X^3)\n",
    "trans = PolynomialFeatures(degree=3)\n",
    "\n",
    "# Apply the polynomial transformation to the input data X\n",
    "# This generates a new feature matrix with all polynomial combinations of the input features\n",
    "data = trans.fit_transform(X)\n",
    "\n",
    "# Convert the array back to a dataframe\n",
    "dataset_transformed = DataFrame(data)\n",
    "\n",
    "# Summarize\n",
    "print(\"\\nTransformed Dataset Shape:\", dataset_transformed.shape)\n",
    "\n",
    "# Evaluate KNN on the dataset with polynomial features transform\n",
    "\n",
    "# Define a PolynomialFeatures transformer with a degree of 3\n",
    "# This will create polynomial features up to the third degree (e.g., X, X^2, X^3)\n",
    "trans = PolynomialFeatures(degree=3)\n",
    "\n",
    "# Initialize a K-Nearest Neighbors (KNN) classifier\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Create a pipeline that combines the polynomial feature transformation and the KNN model\n",
    "# - \"t\": Applies the polynomial feature transformation\n",
    "# - \"m\": Fits the KNN model to the transformed data\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# Evaluate the pipeline using cross-validation\n",
    "# RepeatedStratifiedKFold ensures stratified sampling and repeats the cross-validation process\n",
    "# - n_splits=10: The dataset is split into 10 folds for cross-validation\n",
    "# - n_repeats=3: The entire cross-validation process is repeated 3 times\n",
    "# - random_state=1: Ensures reproducibility by fixing the random seed\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Perform cross-validation on the pipeline\n",
    "# - pipeline: The model pipeline (polynomial features + KNN classifier)\n",
    "# - X: The input features\n",
    "# - y: The target labels\n",
    "# - scoring=\"accuracy\": Evaluates the model using accuracy as the metric\n",
    "# - cv=cv: Uses the RepeatedStratifiedKFold cross-validation strategy\n",
    "# - n_jobs=-1: Enables parallel processing to speed up computation\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report the performance of the pipeline\n",
    "# - mean(n_scores): Calculates the mean accuracy across all cross-validation folds\n",
    "# - std(n_scores): Calculates the standard deviation of the accuracy scores\n",
    "# - \"%.3f\": Formats the output to display values with 3 decimal places\n",
    "print(\"\\nKNN Accuracy (Polynomial Features): %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b0bfab-d669-4cab-8ed2-6ab2e8382a3c",
   "metadata": {},
   "source": [
    "### Compare the effect of the degree on the number of created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc83bf-b3b4-490a-bcee-ceab5ed00e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate change in number of features\n",
    "\n",
    "# Initialize a list to store the number of features generated for each degree\n",
    "num_features = []\n",
    "\n",
    "# Define a range of degrees to test (from 1 to 4)\n",
    "degrees = list(range(1, 5))\n",
    "\n",
    "# Iterate over each degree to calculate the number of polynomial features\n",
    "for d in degrees:\n",
    "    # Create a PolynomialFeatures transformer with the current degree\n",
    "    trans = PolynomialFeatures(degree=d)\n",
    "    \n",
    "    # Fit and transform the input data X to generate polynomial features\n",
    "    data = trans.fit_transform(X)\n",
    "    \n",
    "    # Record the number of features (columns) in the transformed data\n",
    "    num_features.append(data.shape[1])\n",
    "    \n",
    "    # Summarize the results by printing the degree and corresponding number of features\n",
    "    print(\"Degree: %d, Features: %d\" % (d, data.shape[1]))\n",
    "\n",
    "# Plot the relationship between the degree of polynomial and the number of features\n",
    "plt.plot(degrees, num_features, marker='o')  # Plot with markers for each degree\n",
    "plt.xlabel(\"Degree\")  # Label for the x-axis\n",
    "plt.ylabel(\"Number of Features\")  # Label for the y-axis\n",
    "plt.title(\"Polynomial Features vs Degree\")  # Title of the plot\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68315e7-8d53-4ef6-954f-dbc83c2c3976",
   "metadata": {},
   "source": [
    "### Understanding Polynomial Feature Expansion\n",
    "\n",
    "#### Plot Overview\n",
    "The plot visualizes how the number of features in a dataset changes as the degree of polynomial expansion increases. It demonstrates that the number of features grows exponentially with higher polynomial degrees.\n",
    "\n",
    "##### Key Observations:\n",
    "- **X-axis (Degree of Polynomial Features)**: Represents the degree used for polynomial feature generation.\n",
    "- **Y-axis (Number of Features)**: Represents the total number of features after applying polynomial expansion.\n",
    "- **Low Degrees (1 and 2)**: The number of features remains relatively small, making the dataset manageable.\n",
    "- **Higher Degrees (3 and above)**: The number of features increases dramatically, illustrating the **curse of dimensionality**.\n",
    "\n",
    "#### Interpretation:\n",
    "- **Degree 1**: Represents the original dataset with no polynomial expansion.\n",
    "- **Degrees 2 and 3**: Show a moderate increase in the number of features, capturing more complex relationships.\n",
    "- **Degree 4 and Beyond**: Exhibit an exponential rise in the number of features, which can lead to computational inefficiency and overfitting.\n",
    "\n",
    "##### Summary:\n",
    "This plot highlights the trade-off between model complexity and computational cost when using polynomial features. While higher-degree polynomials can model intricate patterns, they also risk overfitting and increased computational demands. Therefore, selecting an appropriate polynomial degree is crucial for balancing model performance and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d03c3-69c1-4434-89ef-538a5273140d",
   "metadata": {},
   "source": [
    "## Explore the effect of degree on accuracy for the polynomial features transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76b0a6-e2ac-43d1-b25a-aa53f5979154",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Defines a function called 'get_models' that returns a dictionary of machine learning pipelines.\n",
    "def get_models():\n",
    "    # Initializes an empty dictionary called 'models' to store the pipelines.\n",
    "    models = {}\n",
    "    # Iterates through degrees from 1 to 4 (exclusive of 5) to create polynomial features.\n",
    "    for d in range(1, 5):\n",
    "        # Define the pipeline\n",
    "        # Creates a PolynomialFeatures transformer with the current degree 'd'.\n",
    "        trans = PolynomialFeatures(degree=d)\n",
    "        # Creates a KNeighborsClassifier model with default parameters.\n",
    "        model = KNeighborsClassifier()\n",
    "        # Stores the pipeline in the 'models' dictionary with the degree 'd' as the key.\n",
    "        models[str(d)] = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "    # Returns the dictionary 'models' containing pipelines for different polynomial degrees.\n",
    "    return models\n",
    "\n",
    "# Evaluate a given model using cross-validation\n",
    "# Defines a function called 'evaluate_model' that takes a model, features (X), and labels (y) as input to evaluate model performance.\n",
    "def evaluate_model(model, X, y):\n",
    "    # Initializes RepeatedStratifiedKFold cross-validation with 10 splits, 3 repeats, and a fixed random state for reproducibility.\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # Performs cross-validation using the provided model, features (X), and labels (y), scoring accuracy, using the defined cross-validation strategy (cv), and utilizing all available CPU cores (n_jobs=-1).\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    # Returns the array of accuracy scores obtained from cross-validation.\n",
    "    return scores\n",
    "\n",
    "# Get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# Evaluate the models and store results\n",
    "results = []\n",
    "names = []\n",
    "# Iterate through each item (model name and model object) in the 'models' dictionary.\n",
    "for name, model in models.items():\n",
    "    # Evaluate the current 'model' using the 'evaluate_model' function with features 'X' and labels 'y', and store the scores.\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    # Append the calculated 'scores' to the 'results' list.\n",
    "    results.append(scores)\n",
    "    # Append the current 'model' name to the 'names' list.\n",
    "    names.append(name)\n",
    "    # Print the model's degree (name), mean accuracy score, and standard deviation of accuracy scores, formatted to 3 decimal places.\n",
    "    print(\"> Degree=%s, Accuracy: %.3f (%.3f)\" % (name, mean(scores), std(scores)))\n",
    "\n",
    "# Plot the performance of the KNN model with different polynomial degrees for comparison\n",
    "# - results: A list of accuracy scores for each polynomial degree\n",
    "# - labels=names: Labels for the x-axis, corresponding to the polynomial degrees\n",
    "# - showmeans=True: Displays the mean accuracy for each degree as a marker in the boxplot\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "\n",
    "# Label the x-axis to indicate the polynomial degrees being compared\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "\n",
    "# Label the y-axis to indicate the accuracy metric\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# Add a title to the plot to describe its purpose\n",
    "plt.title(\"KNN Performance with Polynomial Features\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b9346-77d4-4a4b-a91d-b79dd579b2a0",
   "metadata": {},
   "source": [
    "### KNN Performance with Polynomial Features\n",
    "\n",
    "#### Plot Explanation\n",
    "The box plot illustrates the **accuracy** of a K-Nearest Neighbors (KNN) classifier when using different polynomial feature transformations. The x-axis represents the **polynomial degree**, while the y-axis shows the **accuracy** of the model.\n",
    "\n",
    "##### Key Observations:\n",
    "- The **median accuracy** (represented by the orange line) remains relatively stable across polynomial degrees 1 to 4.\n",
    "- The **mean accuracy** (indicated by the green triangle) is consistent across different degrees, suggesting that increasing polynomial features does not significantly enhance performance.\n",
    "- The **variance (spread of the box)** increases slightly for higher polynomial degrees, indicating that additional features may introduce more variability in model performance.\n",
    "- The presence of **outliers** at lower degrees highlights occasional drops in accuracy for specific runs.\n",
    "\n",
    "#### Interpretation:\n",
    "- **Polynomial Degree 1**: The model performs well, with a high median accuracy and minimal variance.\n",
    "- **Polynomial Degrees 2-4**: While accuracy remains stable, the slight increase in variance suggests that additional polynomial features do not substantially improve KNN performance.\n",
    "\n",
    "#### Summary:\n",
    "- Applying polynomial feature expansion does not necessarily improve KNN model accuracy.\n",
    "- Higher polynomial degrees increase feature complexity but may not provide better predictive power, potentially leading to overfitting or computational inefficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b20b24-fe76-4afb-9976-b63a3059b8da",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this module, we learned how to create polynomial features to potentially improve model performance. We explored the impact of different polynomial degrees on feature space dimensionality and model accuracy. The techniques demonstrated show how feature engineering can expose non-linear relationships in data that may improve predictive modeling results.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.ï¿¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
