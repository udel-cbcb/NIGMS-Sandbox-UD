{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection:  Select Numerical Input Features\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Feature selection is the process of identifying and selecting a subset of input features that are most relevant to the target variable. While feature selection is often straightforward for **real-valued input and output data** (e.g., using Pearson's correlation coefficient), it can be more challenging when dealing with **numerical input data and a categorical target variable**.\n",
    "\n",
    "In classification tasks where the input features are numerical and the target variable is categorical, the two most commonly used feature selection methods are:\n",
    "\n",
    "1. **ANOVA F-test Statistic**:\n",
    "   - Measures the variance between groups relative to the variance within groups.\n",
    "   - Suitable for identifying features that have a significant relationship with the categorical target.\n",
    "\n",
    "2. **Mutual Information Statistic**:\n",
    "   - Quantifies the amount of information obtained about the target variable through each feature.\n",
    "   - Effective for capturing non-linear relationships between features and the target.\n",
    "\n",
    "These methods help identify the most relevant features, improving model performance and interpretability.\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn how to evaluate the importance of numerical features using ANOVA F-test statistics and mutual information statistics\n",
    "- Learn how to perform feature selection for numerical data when fitting and evaluating classification models\n",
    "- Learn how to tune and optimize feature selection parameters using grid search methods\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Implement ANOVA F-test feature selection\n",
    "- Implement mutual information feature selection \n",
    "- Build and evaluate models using selected features\n",
    "- Tune feature selection parameters using grid search\n",
    "- Visualize feature selection results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages and import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary libraries for data analysis and plotting\n",
    "# %pip is used to install packages in a Jupyter notebook cell\n",
    "# matplotlib: Library for creating static, animated, and interactive visualizations in Python\n",
    "# numpy: Fundamental package for scientific computing, handling arrays and numerical operations\n",
    "# pandas: Library for data manipulation and analysis, providing data structures like DataFrames\n",
    "# scikit-learn: A machine learning library for Python, providing simple and efficient tools for data mining and data analysis\n",
    "%pip install matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for plotting, numerical operations, and machine learning\n",
    "from matplotlib import pyplot  # Import pyplot for creating visualizations\n",
    "from numpy import mean, std    # Import mean and std functions from numpy for statistical calculations\n",
    "from pandas import read_csv    # Import read_csv to load CSV files into pandas DataFrame\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif  # Import feature selection methods\n",
    "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression model from sklearn\n",
    "from sklearn.metrics import accuracy_score  # Import accuracy_score to evaluate model performance\n",
    "from sklearn.model_selection import (  # Import model selection tools from sklearn\n",
    "    GridSearchCV,  # For hyperparameter tuning via grid search\n",
    "    RepeatedStratifiedKFold,  # For cross-validation with stratified splits\n",
    "    cross_val_score,  # For evaluating models using cross-validation\n",
    "    train_test_split,  # For splitting data into training and testing sets\n",
    ")\n",
    "from sklearn.pipeline import Pipeline  # Import Pipeline for chaining multiple steps together (e.g., preprocessing and model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient data as\n",
    "either an onset of diabetes within five years or not. \n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class \n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and splitting Diabetes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the dataset\n",
    "# Path to the dataset file (Pima Indians Diabetes dataset)\n",
    "pima_indians_diabetes_csv = \"../../Data/pima-indians-diabetes.csv\"  # Set the path to the CSV dataset\n",
    "\n",
    "# Function to load a dataset from a given file and split it into input and output variables\n",
    "def load_dataset(filename):\n",
    "    # Load the dataset as a pandas DataFrame from the specified file\n",
    "    dataset = read_csv(filename, header=None)  # Assumes the CSV has no header\n",
    "\n",
    "    # Retrieve the underlying numpy array from the DataFrame\n",
    "    data = dataset.values  # This gives us a 2D numpy array of the dataset\n",
    "\n",
    "    # Split the data into input variables (X) and output variables (y)\n",
    "    X = data[:, :-1]  # Select all rows, and all columns except the last one for input features\n",
    "    y = data[:, -1]   # Select all rows, but only the last column for the output variable\n",
    "\n",
    "    # Return the input and output variables\n",
    "    return X, y\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(pima_indians_diabetes_csv)\n",
    "\n",
    "# split into train (67%)  and test sets (33%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# summarize\n",
    "print(\"Train\", X_train.shape, y_train.shape)\n",
    "print(\"Test\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Feature Selection\n",
    "\n",
    "When working with numerical input features and a categorical target variable, two widely adopted feature selection techniques are particularly effective: the ANOVA F-statistic and mutual information statistics. The ANOVA F-test evaluates the linear relationship between each feature and the target by measuring variance between groups relative to variance within groups, making it ideal for identifying features with strong class separation. Mutual information, on the other hand, quantifies the dependency between variables without assuming linearity, capturing both linear and non-linear associations. While ANOVA is computationally efficient and interpretable, mutual information is more flexible for complex patterns. Together, these methods provide complementary perspectives—ANOVA for parametric, linear scenarios and mutual information for broader, non-parametric use cases—enabling robust feature selection when applied in tandem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA F-test Feature Selection\n",
    "\n",
    "**ANOVA** (Analysis of Variance) is a parametric statistical hypothesis test used to determine whether the means of two or more samples of data come from the same distribution. It is particularly useful when comparing three or more groups.\n",
    "\n",
    "#### What is an F-statistic?\n",
    "An **F-statistic** (or F-test) is a class of statistical tests that calculate the ratio between variances. For example:\n",
    "- The variance between two different samples.\n",
    "- The explained variance vs. unexplained variance in a statistical test like ANOVA.\n",
    "\n",
    "In the context of feature selection, the **ANOVA F-test** is a specific type of F-statistic used to evaluate the relationship between numerical input features and a categorical target variable.\n",
    "\n",
    "#### Application in Feature Selection\n",
    "ANOVA is particularly useful when:\n",
    "- One variable is **numeric** (e.g., input features).\n",
    "- The other variable is **categorical** (e.g., target variable in a classification task).\n",
    "\n",
    "The results of the ANOVA F-test can be used for feature selection:\n",
    "- Features that are **independent** of the target variable can be removed from the dataset.\n",
    "- Features that show a **significant relationship** with the target variable are retained.\n",
    "\n",
    "By using the ANOVA F-test, we can identify and eliminate irrelevant features, improving model efficiency and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of anova f-test feature selection for numerical data\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # retrieve numpy array\n",
    "    data = dataset.values\n",
    "\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # Initialize the SelectKBest feature selection method\n",
    "    # - score_func=f_classif: Use the ANOVA F-test statistic to score features\n",
    "    # - k=\"all\": Evaluate all features (no feature selection is performed yet)\n",
    "    fs = SelectKBest(score_func=f_classif, k=\"all\")\n",
    "\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(pima_indians_diabetes_csv)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "    print(\"Feature %d: %f\" % (i, fs.scores_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance analysis reveals significant variation in predictive relevance across variables, with several features demonstrating substantially higher test statistic values than others. Notably, Features 1, 5, and 7 emerge as particularly influential, suggesting they may capture meaningful patterns in the target variable. This divergence in importance scores implies that these three features likely contribute disproportionately to model performance, while others may contain redundant or noisy information. However, domain-specific validation is recommended to confirm their practical significance, as high statistical scores alone don't guarantee causal relationships or operational relevance for the specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature selection scores\n",
    "\n",
    "# 'fs.scores_' contains the scores (e.g., feature importance or relevance) for each feature\n",
    "# 'range(len(fs.scores_))' generates a list of positions (x-axis) for the bars, one for each feature\n",
    "# 'pyplot.bar' creates a bar plot where each bar corresponds to a feature's score\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)  # Create a bar plot of feature scores\n",
    "\n",
    "# Add x-axis and y-axis labels\n",
    "pyplot.xlabel(\"Feature Indices\")\n",
    "pyplot.ylabel(\"Feature Scores\")\n",
    "\n",
    "# Display the plot\n",
    "pyplot.show()  # Show the generated plot to the user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance scores, visualized in a bar chart, reveal that Feature 1 exhibits the highest predictive relevance based on the test statistic, while six of the eight input features demonstrate substantially higher scores than the remaining two. This suggests that restricting feature selection to the top six variables—achieved by configuring SelectKBest with k=6—could optimize the model by:\n",
    "* Reducing noise from low-importance features\n",
    "* Improving computational efficiency without sacrificing accuracy\n",
    "* Mitigating overfitting risks from redundant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Feature Selection\n",
    "\n",
    "**Mutual information** is a concept from **information theory** that is widely applied in feature selection. It is based on the idea of **information gain**, which is commonly used in the construction of decision trees. Here's how it works:\n",
    "\n",
    "- **Definition**: Mutual information measures the reduction in uncertainty for one variable when the value of another variable is known.\n",
    "- **Calculation**: It quantifies the dependency between two variables, making it a powerful tool for identifying relevant features in a dataset.\n",
    "\n",
    "#### Application in Feature Selection\n",
    "- **Discrete Variables**: Mutual information is straightforward to calculate for two discrete variables (e.g., categorical or ordinal data).\n",
    "- **Numerical Input and Categorical Output**: It can also be adapted for use with numerical input features and a categorical target variable, making it versatile for classification tasks.\n",
    "\n",
    "By calculating mutual information between each input feature and the target variable, we can determine which features provide the most information for predicting the target, enabling effective feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of mutual information feature selection for numerical input data\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # retrieve numpy array\n",
    "    data = dataset.values\n",
    "\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # Initialize SelectKBest with mutual information and k=\"all\"\n",
    "    # The mutual_info_classif function estimates mutual information between features and the target variable \n",
    "    # using a non-parametric method based on nearest neighbors (k-nearest neighbors, KNN).\n",
    "    # By default, it includes a small amount of noise to continuous variables to handle discretization, \n",
    "    # controlled by the random_state parameter. If random_state is not set (i.e., left as None), \n",
    "    # this noise addition introduces some randomness, which can lead to slight variations in results across runs. \n",
    "    # Setting random_state to a fixed integer (e.g., random_state=42) makes the process deterministic and reproducible.\n",
    "    fs = SelectKBest(score_func=lambda X, y: mutual_info_classif(X, y, random_state=42), k=\"all\")\n",
    "\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(pima_indians_diabetes_csv)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "    print(\"Feature %d: %f\" % (i, fs.scores_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance analysis reveals several variables with modestly low predictive scores, indicating they may contribute minimal value to the model. Notably, Features 1 and 5 emerge as the most statistically significant, suggesting they capture the strongest relationships with the target variable. While removing low-scoring features could streamline the model—potentially improving computational efficiency and reducing overfitting—their elimination should be validated through ablation testing. For instance, comparing model performance (e.g., accuracy, F1-score) before and after excluding these features would ensure robustness. If the impact is negligible, pruning them could yield a more parsimonious and interpretable model without sacrificing predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature selection scores\n",
    "\n",
    "# 'fs.scores_' contains the importance or score of each feature (e.g., in feature selection)\n",
    "# We create a bar plot where the x-axis represents the feature indices and the y-axis represents their scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)  # Create a bar plot with feature indices on the x-axis and feature scores on the y-axis\n",
    "\n",
    "# Add x-axis and y-axis labels\n",
    "pyplot.xlabel(\"Feature Indices\")\n",
    "pyplot.ylabel(\"Feature Scores\")\n",
    "\n",
    "# Display the plot to the user\n",
    "pyplot.show()  # Show the generated plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Higher MI Scores**: Features with higher bars (e.g., indices 1 and 5) contain more information relevant to predicting the target.\n",
    "- **Lower MI Scores**: Features with shorter bars (e.g., indices 0, 3, and 6) contribute less to predicting the target.\n",
    "- **Comparison Between Features**: The relative differences in bar heights suggest that some features are much more relevant than others.\n",
    "\n",
    "For feature selection, if you are reducing dimensionality, you might prioritize features with high MI scores and remove low-scoring ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling With Selected Features\n",
    "\n",
    "To ensure optimal model performance, a systematic evaluation of feature selection strategies is recommended. This involves testing multiple approaches (e.g., filter methods, wrapper methods, and embedded methods) across varying numbers of selected features, then selecting the combination that yields the highest predictive accuracy on validation data. By comparing performance metrics across these configurations—rather than relying on a single feature selection technique—we mitigate the risk of method-specific biases and increase the likelihood of identifying the most robust subset of features. This comprehensive approach is particularly valuable when dealing with high-dimensional datasets where the relationship between features and target variables may be complex or non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Built Using All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model using all input features\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # retrieve numpy array\n",
    "    data = dataset.values\n",
    "\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(pima_indians_diabetes_csv)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver=\"liblinear\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print(\"Accuracy: %.2f\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current model achieves a classification accuracy of 77.56%, establishing a performance benchmark for feature selection. Our objective is to identify an optimal feature subset that maintains or exceeds this accuracy while improving model efficiency. This requires evaluating candidate feature sets against the baseline to ensure no degradation in predictive performance, with potential benefits including reduced computational complexity, enhanced interpretability, and improved generalization by eliminating redundant or noisy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Built Using ANOVA F-test Features\n",
    "To identify the most informative features for our predictive model, we can leverage the ANOVA F-test, a statistical method that evaluates the strength of linear relationships between numerical input features and a categorical target variable. By computing F-scores for each feature—which measure the ratio of between-group variance to within-group variance—we rank all features by their predictive importance. Selecting the top four highest-scoring features provides an optimal balance between model simplicity and explanatory power, effectively reducing dimensionality while retaining the most statistically significant predictors. This approach is particularly advantageous when working with limited samples or when computational efficiency is prioritized, as it discards redundant or noisy variables that contribute little to classification accuracy. For implementation, scikit-learn’s SelectKBest with f_classif automates this process, enabling seamless integration into machine learning pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model using 4 features chosen with anova f-test\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # retrieve numpy array\n",
    "    data = dataset.values\n",
    "\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select a subset of features\n",
    "    fs = SelectKBest(score_func=f_classif, k=4)\n",
    "\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(pima_indians_diabetes_csv)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver=\"liblinear\")\n",
    "model.fit(X_train_fs, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print(\"Accuracy: %.2f\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model demonstrates a measurable improvement over the baseline, achieving an accuracy of 78.74% compared to the original 77.56%—a gain of 1.18 percentage points (representing a 1.5% relative improvement). This enhancement, while modest, suggests that the applied modifications (e.g., feature selection) have positively influenced predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Built Using Mutual Information Features\n",
    "\n",
    "To validate our feature selection approach, we replicate the analysis using mutual information statistics while maintaining consistent experimental parameters. By selecting the top four most informative features through this alternative method, we achieve several objectives: (1) enabling direct comparison with our chi-squared results to assess method-dependent variations, (2) focusing on features that capture non-linear relationships with the target variable, and (3) potentially identifying complementary predictors that might have been overlooked by the independence-test-based approach. This methodological cross-check serves as both a robustness test for our feature engineering pipeline and an opportunity to discover alternative predictive patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model using 4 features chosen with mutual information\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # retrieve numpy array\n",
    "    data = dataset.values\n",
    "\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # Initialize SelectKBest with mutual information and k=4 (top 4 features)\n",
    "    # The mutual_info_classif function estimates mutual information between features and the target variable \n",
    "    # using a non-parametric method based on nearest neighbors (k-nearest neighbors, KNN).\n",
    "    # By default, it includes a small amount of noise to continuous variables to handle discretization, \n",
    "    # controlled by the random_state parameter. If random_state is not set (i.e., left as None), \n",
    "    # this noise addition introduces some randomness, which can lead to slight variations in results across runs. \n",
    "    # Setting random_state to a fixed integer (e.g., random_state=42) makes the process deterministic and reproducible.\n",
    "    fs = SelectKBest(score_func=lambda X, y: mutual_info_classif(X, y, random_state=42), k=3)\n",
    "\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset(pima_indians_diabetes_csv)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1\n",
    ")\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression(solver=\"liblinear\")\n",
    "model.fit(X_train_fs, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print(\"Accuracy: %.2f\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite selecting a distinct set of four features compared to previous methods, the model’s performance remains statistically indistinguishable from the baseline. This intriguing result suggests that while alternative feature subsets may capture different aspects of the data, they ultimately provide comparable predictive power for this specific task. The finding highlights that raw feature count alone doesn’t determine model efficacy—what matters more is whether the selected features collectively encode sufficient discriminative information. Potential explanations include: (1) redundancy in the feature space where different subsets convey similar signals, (2) the existence of multiple near-optimal feature combinations, or (3) the model architecture’s capacity limitations. Further analysis could examine whether the new feature set offers advantages in interpretability, computational efficiency, or robustness to data shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the Number of Selected Features\n",
    "\n",
    "To optimize feature selection, we employ a **grid search** to systematically evaluate the performance impact of selecting different numbers of features (tuning the k parameter in SelectKBest). This replaces guesswork with empirical validation, ensuring we identify the feature subset that maximizes model performance. For robust evaluation, we use **repeated stratified k-fold cross-validation** (3 repeats of 10-fold splits via `RepeatedStratifiedKFold`), which accounts for variance in small datasets while preserving class distribution balance. This approach not only determines the ideal number of features but also provides statistically reliable performance estimates for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different numbers of features selected using anova f-test\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # retrieve numpy array\n",
    "    data = dataset.values\n",
    "\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = load_dataset(pima_indians_diabetes_csv)\n",
    "\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# define the pipeline to evaluate\n",
    "model = LogisticRegression(solver=\"liblinear\")\n",
    "fs = SelectKBest(score_func=f_classif)\n",
    "pipeline = Pipeline(steps=[(\"anova\", fs), (\"lr\", model)])\n",
    "\n",
    "# define the grid\n",
    "grid = dict()\n",
    "grid[\"anova__k\"] = [i + 1 for i in range(X.shape[1])]\n",
    "print(grid)\n",
    "\n",
    "# define the grid search\n",
    "\n",
    "# Exhaustive search over specified parameter values for an estimator.\n",
    "search = GridSearchCV(pipeline, grid, scoring=\"accuracy\", n_jobs=-1, cv=cv)\n",
    "\n",
    "# perform the search\n",
    "results = search.fit(X, y)\n",
    "\n",
    "# summarize best\n",
    "print(\"Best Mean Accuracy: %.3f\" % results.best_score_)\n",
    "print(\"Best Config: %s\" % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature selection analysis reveals that a streamlined subset of five features achieves near-optimal accuracy (77.0%), performing comparably to the full feature set baseline (77.56%). The results imply that the remaining features either contain redundant information or contribute primarily to model variance rather than meaningful signal, making this reduced feature set an attractive option for production deployment where interpretability and speed are prioritized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize feature selection, we should systematically evaluate the trade-off between the number of selected features (k) and model performance. Intuitively, accuracy typically improves as more relevant features are included—up to a critical point where additional variables introduce noise or redundancy. This relationship can be quantified by:\n",
    "* Iteratively applying SelectKBest across a range of k values\n",
    "* Measuring classification accuracy for each configuration via cross-validation\n",
    "* Visualizing results using side-by-side box plots to compare distributions of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different numbers of features selected using anova f-test\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    dataset = read_csv(filename, header=None)\n",
    "\n",
    "    # retrieve numpy array\n",
    "    data = dataset.values\n",
    "\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = load_dataset(pima_indians_diabetes_csv)\n",
    "\n",
    "# define number of features to evaluate\n",
    "num_features = [i + 1 for i in range(X.shape[1])]\n",
    "\n",
    "# enumerate each number of features\n",
    "results = list()\n",
    "for k in num_features:\n",
    "    # create pipeline\n",
    "    model = LogisticRegression(solver=\"liblinear\")\n",
    "    fs = SelectKBest(score_func=f_classif, k=k)\n",
    "    pipeline = Pipeline(steps=[(\"anova\", fs), (\"lr\", model)])\n",
    "\n",
    "    # evaluate the model\n",
    "    scores = evaluate_model(pipeline)\n",
    "    results.append(scores)\n",
    "\n",
    "    # summarize the results\n",
    "    print(\">%d %.3f (%.3f)\" % (k, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature selection analysis reveals comparable model accuracy when using either five or seven features, suggesting diminishing returns beyond the five most predictive variables. This plateau in performance indicates that the additional two features may contribute minimal unique information for the classification task. The marginal gain, if any, appears insufficient to justify the increased model complexity, particularly when considering potential overfitting risks and computational costs. These findings warrant further investigation into whether the two extra features consistently appear unimportant across different validation splits or if their value becomes apparent under specific data conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a boxplot to compare the performance of different models\n",
    "\n",
    "# 'results' contains the performance data for each model (e.g., accuracy scores, etc.)\n",
    "# The 'boxplot' function creates the boxplot to visually compare the model performances\n",
    "# 'showmeans=True' will display the mean values in the boxplot\n",
    "pyplot.boxplot(results, showmeans=True)\n",
    "\n",
    "# Add x-axis and y-axis labels\n",
    "pyplot.xlabel(\"Numbers of Features Selected\")\n",
    "pyplot.ylabel(\"Accuracy\")\n",
    "\n",
    "# Set the x-axis labels using 'xticklabels'\n",
    "pyplot.xticks(ticks=range(1, len(results) + 1))  # Set the feature names on the x-axis\n",
    "\n",
    "# Display the plot\n",
    "pyplot.show()  # Show the generated plot to the user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observation**\n",
    "- Feature 1 shows the widest spread, indicating more variability in accuracy when using this feature.\n",
    "- Features 5, 6, 7, and 8 have slightly higher mean accuracy compared to others.\n",
    "- Features 3, 4, and 5 have some outliers on the lower end, suggesting occasional poor performance.\n",
    "- Median accuracy values are fairly consistent across most features.\n",
    "\n",
    "**Possible Insights**\n",
    "- If this represents feature selection, you might favor features 6, 7, and 8, as they appear stable with higher mean accuracy.\n",
    "- Features with high variance (e.g., Feature 1) could be investigated further—perhaps their usefulness depends on data splits.\n",
    "- If the goal is stability, avoiding features with large outliers (e.g., Feature 3 or 5) might be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Through this tutorial, we learned how to perform feature selection on numerical data for classification tasks. We explored two key methods - ANOVA F-test and mutual information statistics - and learned how to evaluate their effectiveness through model performance. We also discovered how to systematically tune feature selection parameters using grid search to find the optimal number of features.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
