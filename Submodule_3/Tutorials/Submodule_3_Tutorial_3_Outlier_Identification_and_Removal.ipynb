{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Identification and Removal\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the identification and removal of outliers in datasets. We'll explore various techniques to detect and handle outliers, which are data points that significantly differ from other observations in a dataset.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand what outliers are and why they matter in data analysis\n",
    "- Learn different methods to identify outliers\n",
    "- Implement outlier removal techniques\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python programming\n",
    "- Familiarity with NumPy libraries\n",
    "- Knowledge of basic statistical concepts (mean, standard deviation, percentiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages, import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python libraries for data analysis and visualization.\n",
    "# These libraries are:\n",
    "#   - matplotlib: for plotting and visualization.\n",
    "#   - seaborn: for statistical data visualization built on top of matplotlib.\n",
    "#   - numpy: for numerical operations and array manipulation.\n",
    "#   - pandas: for data manipulation and analysis using DataFrames.\n",
    "#   - scikit-learn: for machine learning tasks, including model evaluation and metrics.\n",
    "%pip install matplotlib seaborn numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NumPy library for numerical operations, often used for array manipulation and mathematical functions.\n",
    "import numpy as np\n",
    "# Import the Pandas library for data manipulation and analysis, particularly for working with DataFrames.\n",
    "import pandas as pd\n",
    "# Import the Seaborn library for statistical data visualization, built on top of Matplotlib.\n",
    "import seaborn as sns\n",
    "# Import specific functions 'percentile' and 'random' from the NumPy library.\n",
    "# 'percentile' is used to calculate the nth percentile of data, and 'random' for random number generation.\n",
    "from numpy import percentile, random\n",
    "# Import the 'LinearRegression' class from scikit-learn's linear_model module.\n",
    "# This class is used for linear regression models.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Import the 'mean_absolute_error' function from scikit-learn's metrics module.\n",
    "# This function is used to calculate the mean absolute error between predictions and true values.\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# Import the 'train_test_split' function from scikit-learn's model_selection module.\n",
    "# This function is used to split datasets into training and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import the 'LocalOutlierFactor' class from scikit-learn's neighbors module.\n",
    "# This class is used for outlier detection using the Local Outlier Factor algorithm.\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Define a variable 'pima_indians_diabetes_csv' and assign it the file path to the Pima Indians Diabetes dataset CSV file.\n",
    "pima_indians_diabetes_csv = \"../../Data/pima-indians-diabetes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Identification and Removal\n",
    "\n",
    "### What are Outliers?\n",
    "\n",
    "An outlier is an observation that is unlike the other observations. They are rare, distinct, or do\n",
    "notâ€€fit in some way.\n",
    "\n",
    "We will generally define outliers as samples that are exceptionally far from the\n",
    "mainstream of the data.\n",
    "\n",
    "Outliers can have many causes, such as:\n",
    "\n",
    "- Measurement or input error.\n",
    "- Data corruption.\n",
    "- True outlier observation.\n",
    "\n",
    "There is no precise way to define and identify outliers in general because of the specifics of\n",
    "each dataset. Instead, you, or a domain expert, must interpret the raw observations and decide\n",
    "whether a value is an outlier or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers using Standard Deviation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a dataset of random observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the random number generator\n",
    "random.seed(1)\n",
    "\n",
    "# Generate univariate observations using numpy's randn\n",
    "data = 5 * np.random.randn(10000) + 50\n",
    "\n",
    "# Summarize\n",
    "print(\"mean=%.3f stdv=%.3f\" % (np.mean(data), np.std(data)))\n",
    "\n",
    "# Plot the data\n",
    "sns.displot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics of the 'data' array using NumPy.\n",
    "# np.mean(data) calculates the arithmetic mean (average) of all elements in the 'data' array.\n",
    "data_mean = np.mean(data)\n",
    "# np.std(data) calculates the standard deviation of all elements in the 'data' array.\n",
    "# Standard deviation measures the dispersion or spread of the data around the mean.\n",
    "data_std = np.std(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff value for outlier detection, which is 3 standard deviations from the mean.\n",
    "cut_off = data_std * 3\n",
    "# Calculate the lower bound for outliers, which is the mean minus the cutoff value.\n",
    "lower = data_mean - cut_off\n",
    "# Calculate the upper bound for outliers, which is the mean plus the cutoff value.\n",
    "upper = data_mean + cut_off\n",
    "\n",
    "# Initialize an empty list to store the identified outliers.\n",
    "outliers = []\n",
    "# Iterate through each data point 'x' in the 'data' list.\n",
    "for x in data:\n",
    "    # Check if the data point 'x' is less than the lower bound OR greater than the upper bound.\n",
    "    if x < lower or x > upper:\n",
    "        # If the condition is true, it means 'x' is an outlier, so add it to the 'outliers' list.\n",
    "        outliers.append(x)\n",
    "# Print the number of outliers identified.\n",
    "# \"%d\" is a format specifier for an integer, which will be replaced by the length of the 'outliers' list.\n",
    "print(\"Identified outliers: %d\" % len(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify non-outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-outlier observations from the 'data' list.\n",
    "# This line creates a new list called 'non_outliers' using a list comprehension.\n",
    "# It iterates through each element 'x' in the 'data' list.\n",
    "# For each 'x', it checks if 'x' is greater than or equal to 'lower' AND less than or equal to 'upper'.\n",
    "# If both conditions are true, 'x' is considered a non-outlier and is included in the 'non_outliers' list.\n",
    "non_outliers = [x for x in data if x >= lower and x <= upper]\n",
    "# Print the number of non-outlier observations found.\n",
    "# len(non_outliers) calculates the number of elements in the 'non_outliers' list, which represents the count of non-outlier observations.\n",
    "# The string \"Non-outlier observations: %d\" is a format string where %d will be replaced by the integer value of len(non_outliers).\n",
    "print(\"Non-outlier observations: %d\" % len(non_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers using Interquartile Range method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a dataset of random observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the random number generator to ensure reproducibility.\n",
    "# Setting the seed to 1 means that every time this code is run, it will produce the same random numbers, making the results consistent.\n",
    "random.seed(1)\n",
    "\n",
    "# Generate univariate observations (10000 data points) from a normal distribution using NumPy.\n",
    "# np.random.randn(10000) generates 10000 numbers from a standard normal distribution (mean=0, standard deviation=1).\n",
    "# Multiplying by 5 scales the standard deviation of the distribution to 5.\n",
    "# Adding 50 shifts the mean of the distribution to 50, centering it around 50 instead of 0.\n",
    "data = 5 * np.random.randn(10000) + 50\n",
    "\n",
    "# Plot the distribution of the generated data using seaborn's displot function.\n",
    "# sns.displot(data) creates a distribution plot, which by default includes a histogram and a kernel density estimate (KDE) of the 'data'. This visualizes the shape and spread of the generated dataset.\n",
    "sns.displot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'percentile' function from the NumPy library.\n",
    "from numpy import percentile\n",
    "\n",
    "# Calculate the 25th and 75th percentiles of the 'data' array.\n",
    "# percentile(data, 25) calculates the 25th percentile (Q1).\n",
    "# percentile(data, 75) calculates the 75th percentile (Q3).\n",
    "q25, q75 = percentile(data, 25), percentile(data, 75)\n",
    "# Calculate the Interquartile Range (IQR).\n",
    "# IQR is the difference between the 75th percentile (Q3) and the 25th percentile (Q1).\n",
    "iqr = q75 - q25\n",
    "# Print the calculated percentiles and IQR.\n",
    "# \"Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f\" is a format string to display the values.\n",
    "# %.3f formats each percentile and IQR value as a floating-point number with 3 decimal places.\n",
    "# q25 is the 25th percentile, q75 is the 75th percentile, and iqr is the Interquartile Range.\n",
    "print(\"Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f\" % (q25, q75, iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Interquartile Range (IQR) which is the difference between the 75th and 25th percentiles (q75 - q25).\n",
    "iqr = q75 - q25\n",
    "# Calculate the outlier cutoff range using the IQR method.\n",
    "# The lower cutoff is calculated as the 25th percentile (q25) minus 1.5 times the IQR.\n",
    "# The upper cutoff is calculated as the 75th percentile (q75) plus 1.5 times the IQR.\n",
    "lower, upper = q25 - iqr * 1.5, q75 + iqr * 1.5\n",
    "\n",
    "# Identify outliers in the 'data' list based on the calculated lower and upper cutoffs.\n",
    "# This line creates a list called 'outliers' containing all data points 'x' from the 'data' list\n",
    "# that are either less than the 'lower' cutoff or greater than the 'upper' cutoff.\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "# Print the number of outliers identified.\n",
    "# len(outliers) calculates the number of elements in the 'outliers' list, which represents the count of outliers.\n",
    "print(\"Identified outliers: %d\" % len(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify non-outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-outlier observations from the 'data' list.\n",
    "# This line creates a new list called 'non_outliers' using a list comprehension.\n",
    "# It iterates through each element 'x' in the 'data' list and checks if it falls within the defined bounds.\n",
    "non_outliers = [x for x in data if x >= lower and x <= upper]\n",
    "# Print the number of non-outlier observations identified.\n",
    "# len(non_outliers) calculates the number of elements in the 'non_outliers' list.\n",
    "# The print statement outputs a string indicating the count of non-outlier observations.\n",
    "print(\"Non-outlier observations: %d\" % len(non_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers using Automatic Outlier Detection method\n",
    "\n",
    "A simple approach to identifying outliers is to locate those examples that are far from the\n",
    "other examples in the multi-dimensional feature space. This can work well for feature spaces\n",
    "with low dimensionality (few features), although it can become less reliable as the number of\n",
    "features is increased, referred to as the **curse of dimensionality**. The local outlier factor, or\n",
    "LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier\n",
    "detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers\n",
    "based on the size of its local neighborhood. Those examples with the largest score are more\n",
    "likely to be outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient as\n",
    "either an onset of diabetes within five years or not. \n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class \n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "- Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "- Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize diabetes data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and summarize the dataset\n",
    "\n",
    "# Load the dataset from a CSV file named 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' argument indicates that the CSV file does not have a header row.\n",
    "df = pd.read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Retrieve the values from the DataFrame as a NumPy array.\n",
    "# This converts the pandas DataFrame into a numerical array for further processing.\n",
    "data = df.values\n",
    "\n",
    "# Split the dataset into input features (X) and output/target variable (y).\n",
    "# X is assigned all columns except the last one ([:-1]).\n",
    "# y is assigned the last column ([-1]), which is assumed to be the target variable.\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# Summarize the shape of the input features (X) and the output variable (y).\n",
    "# X.shape will output the dimensions of the input feature matrix (rows, columns).\n",
    "# y.shape will output the dimensions of the output variable array (rows,).\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features (X) and target variable (y) into training and testing sets.\n",
    "# X: Features dataset (presumably a pandas DataFrame or NumPy array).\n",
    "# y: Target variable dataset (presumably a pandas Series or NumPy array).\n",
    "# test_size=0.3: Specifies that 30% of the data will be used for the test set, and the remaining 70% for the training set.\n",
    "# random_state=1: Sets the random seed to 1. This ensures that the data split is reproducible.\n",
    "#                 If you run the code again with the same random_state, you will get the same split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Print the shapes of the resulting training and testing sets.\n",
    "# X_train.shape:  Outputs the dimensions (number of rows, number of columns) of the training features dataset.\n",
    "# X_test.shape:   Outputs the dimensions of the testing features dataset.\n",
    "# y_train.shape:  Outputs the shape (number of rows) of the training target variable dataset.\n",
    "# y_test.shape:   Outputs the shape of the testing target variable dataset.\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate module on raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression model to the training data.\n",
    "# X_train: Training features (independent variables).\n",
    "# y_train: Training target variable (dependent variable).\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data using the fitted linear regression model.\n",
    "# X_test: Test features (independent variables) for which predictions are to be made.\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's predictions using Mean Absolute Error (MAE).\n",
    "# MAE measures the average absolute difference between the predicted values (yhat) and the true values (y_test).\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "# Print the calculated Mean Absolute Error (MAE), formatted to 3 decimal places.\n",
    "# \"MAE: %.3f\" creates a string that includes \"MAE: \" followed by the MAE value rounded to 3 decimal places.\n",
    "print(\"MAE: %.3f\" % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove outliers from the data using Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can try removing outliers from the training dataset. The expectation is that the\n",
    "outliers are causing the linear regression model to learn a bias or skewed understanding of the\n",
    "problem, and that removing these outliers from the training set will allow a more effective model\n",
    "to be learned.\n",
    "\n",
    "The **Local Outlier Factor** (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. \n",
    "\n",
    "We can achieve this by defining the **LocalOutlierFactor** model and using it to\n",
    "make a prediction on the training dataset, marking each row in the training dataset as normal\n",
    "(1) or an outlier (-1). We will use the default hyperparameters for the outlier detection model,\n",
    "although it is a good idea to tune the configuration to the specifics of your dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Local Outlier Factor (LOF) model.\n",
    "# LOF is an unsupervised anomaly detection method that identifies outliers based on their local density deviation.\n",
    "lof = LocalOutlierFactor()\n",
    "\n",
    "# Fit the LOF model to the training dataset (X_train) and predict outlier labels.\n",
    "# fit_predict() method fits the model to X_train and returns labels:\n",
    "# -1 for outliers and 1 for inliers.\n",
    "yhat = lof.fit_predict(X_train)\n",
    "\n",
    "# Create a boolean mask to select inliers (non-outliers) from the training data.\n",
    "# 'yhat != -1' creates a boolean array where True indicates inliers (label 1) and False indicates outliers (label -1).\n",
    "mask = yhat != -1\n",
    "# Use the mask to filter the training data, keeping only the inlier rows.\n",
    "# X_train[mask, :] selects rows from X_train where mask is True and all columns.\n",
    "# y_train[mask] selects elements from y_train where mask is True.\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "# Print the shape of the updated training dataset after removing outliers.\n",
    "# This shows the number of rows and columns in X_train and the number of elements in y_train after outlier removal.\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Initialize a Linear Regression model.\n",
    "model = LinearRegression()\n",
    "# Fit the Linear Regression model to the filtered training data (X_train, y_train) which now excludes outliers.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained Linear Regression model to make predictions on the test dataset (X_test).\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's predictions by calculating the Mean Absolute Error (MAE).\n",
    "# mean_absolute_error() computes the average absolute difference between the true values (y_test) and the predicted values (yhat).\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "# Print the calculated Mean Absolute Error (MAE) formatted to 3 decimal places.\n",
    "# Lower MAE values indicate better model performance.\n",
    "print(\"MAE: %.3f\" % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see MAE (Mean Absolute Error) reduced from to 0.324 to 0.317."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've learned how to identify and remove outliers using various statistical methods. We've seen how outliers can affect data analysis and how their removal can lead to more accurate insights. Always to consider the context of your data when applying outlier removal techniques, as some apparent outliers may contain useful information.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
