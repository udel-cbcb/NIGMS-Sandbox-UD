{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Outlier Identification and Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn:\n",
    "\n",
    "* That an outlier is an unlikely observation in a dataset and may have one of many causes.\n",
    "* How to use simple univariate statistics like standard deviation and interquartile range to\n",
    "identify and remove outliers from a data sample.\n",
    "* How to use an outlier detection model to identify and remove rows from a training dataset\n",
    "in order to lift predictive modeling performance.\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Outlier Identification and Removal\n",
    "##What are Outliers?\n",
    "\n",
    "An outlier is an observation that is unlike the other observations. They are rare, distinct, or do\n",
    "notâ€€fit in some way.\n",
    "\n",
    "We will generally define outliers as samples that are exceptionally far from the\n",
    "mainstream of the data.\n",
    "\n",
    "Outliers can have many causes, such as:\n",
    "\n",
    "* Measurement or input error.\n",
    "\n",
    "* Data corruption.\n",
    "\n",
    "* True outlier observation.\n",
    "\n",
    "There is no precise way to define and identify outliers in general because of the specifics of\n",
    "each dataset. Instead, you, or a domain expert, must interpret the raw observations and decide\n",
    "whether a value is an outlier or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Remove outliers using Standard Deviation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifiy outliers using Standard Deviation method\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "# seed the random number generator\n",
    "seed(1)\n",
    "# generate univariate observations\n",
    "data = 5 * randn(10000) + 50\n",
    "# summarize\n",
    "print('mean=%.3f stdv=%.3f' % (mean(data), std(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate summary statistics\n",
    "data_mean, data_std = mean(data), std(data)\n",
    "# define outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "print('Identified outliers: %d' % len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify non outliers\n",
    "non_outliers = [x for x in data if x >= lower and x <= upper]\n",
    "print('Non-outlier observations: %d' % len(non_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Remove outliers using Interquartile Range method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers with interquartile range\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import percentile\n",
    "# seed the random number generator\n",
    "seed(1)\n",
    "# generate univariate observations\n",
    "data = 5 * randn(10000) + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate interquartile range\n",
    "q25, q75 = percentile(data, 25), percentile(data, 75)\n",
    "iqr = q75 - q25\n",
    "print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 1.5\n",
    "lower, upper = q25 - cut_off, q75 + cut_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "print('Identified outliers: %d' % len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify non outliers\n",
    "non_outliers = [x for x in data if x >= lower and x <= upper]\n",
    "print('Non-outlier observations: %d' % len(non_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Remove outliers using Automatic Outlier Detection method\n",
    "\n",
    "A simple approach to identifying outliers is to locate those examples that are far from the\n",
    "other examples in the multi-dimensional feature space. This can work well for feature spaces\n",
    "with low dimensionality (few features), although it can become less reliable as the number of\n",
    "features is increased, referred to as the **curse of dimensionality**. The local outlier factor, or\n",
    "LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier\n",
    "detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers\n",
    "based on the size of its local neighborhood. Those examples with the largest score are more\n",
    "likely to be outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Diabetes Dataset\n",
    "The dataset classifies patient as\n",
    "either an onset of diabetes within five years or not. \n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class \n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Download Diabetes data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!python -m wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\" -o pima-indians-diabetes.csv\n",
    "!python -m wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\" -o pima-indians-diabetes.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "# load the dataset\n",
    "df = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# summarize the shape of the dataset\n",
    "print(X.shape, y.shape)\n",
    "# split into train (70%) and test sets (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "# summarize the shape of the train and test sets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on the raw dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "df = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions using mean absolute error\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can try removing outliers from the training dataset. The expectation is that the\n",
    "outliers are causing the linear regression model to learn a bias or skewed understanding of the\n",
    "problem, and that removing these outliers from the training set will allow a more effective model\n",
    "to be learned.\n",
    "\n",
    "The **Local Outlier Factor** (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. \n",
    "\n",
    "We can achieve this by defining the **LocalOutlierFactor** model and using it to\n",
    "make a prediction on the training dataset, marking each row in the training dataset as normal\n",
    "(1) or an outlier (-1). We will use the default hyperparameters for the outlier detection model,\n",
    "although it is a good idea to tune the configuration to the specifics of your dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on training dataset with outliers removed\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "df = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# identify outliers in the training dataset\n",
    "# unsupervised anomaly detection method which computes the local density deviation of a given data point \n",
    "# with respect to its neighbors. \n",
    "# It considers as outliers the samples that have a substantially lower density than their neighbors.\n",
    "lof = LocalOutlierFactor()\n",
    "# Fit the model to the training set X and return the labels.\n",
    "yhat = lof.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# fit the model without outliers\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see MAE (Mean Absolute Error) reduced from to 0.324 to 0.317."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
