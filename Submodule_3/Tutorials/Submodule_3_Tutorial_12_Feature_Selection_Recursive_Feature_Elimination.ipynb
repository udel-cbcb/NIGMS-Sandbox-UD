{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection:  Recursive Feature Elimination\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Recursive Feature Elimination, or RFE for short, is a popular feature selection algorithm. RFE\n",
    "is popular because it is easy to configure and use, and effective at selecting those\n",
    "features (columns) in a training dataset that are more or most relevant in predicting the target\n",
    "variable. There are two important configuration options when using RFE: the choice in the\n",
    "number of features to select and the choice of the algorithm used to help choosing the features. Both\n",
    "of these hyperparameters can be explored, although the performance of the method is not\n",
    "strongly dependent on these hyperparameters being configured well.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn how RFE is an efficient approach for eliminating features from a training dataset for feature selection\n",
    "- Learn how to use RFE for feature selection for classification and regression predictive modeling problems\n",
    "- Learn how to explore the number of selected features and wrapped algorithm used by the RFE procedure\n",
    "- Understand how to evaluate different algorithms wrapped by RFE for optimal feature selection\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Implement RFE for classification problems\n",
    "- Implement RFE for regression problems \n",
    "- Explore RFE hyperparameters\n",
    "- Evaluate different estimator algorithms for RFE\n",
    "- Analyze selected features\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages and import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python packages using pip\n",
    "# 'matplotlib' for plotting\n",
    "# 'numpy' for numerical operations\n",
    "# 'scikit-learn' for machine learning tools\n",
    "\n",
    "%pip install matplotlib numpy scikit-learn  # Install the specified packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules for data processing, model building, and evaluation\n",
    "\n",
    "# Import pyplot from matplotlib for plotting graphs\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Import mean and std from numpy to calculate statistical measures (mean and standard deviation)\n",
    "from numpy import mean, std\n",
    "\n",
    "# Import datasets for generating synthetic data\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "\n",
    "# Import ensemble classifiers for building gradient boosting and random forest models\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# Import Recursive Feature Elimination (RFE) and its cross-validation version (RFECV) for feature selection\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "# Import linear models for classification\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "# Import model selection techniques for cross-validation\n",
    "from sklearn.model_selection import (\n",
    "    RepeatedKFold,  # Repeated k-fold cross-validation\n",
    "    RepeatedStratifiedKFold,  # Stratified k-fold cross-validation for classification tasks\n",
    "    cross_val_score,  # Function for performing cross-validation\n",
    ")\n",
    "\n",
    "# Import Pipeline for building a sequence of processing steps including preprocessing and model fitting\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import decision tree classifiers and regressors for building decision tree-based models\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE for Classification\n",
    "\n",
    "First, we can use the\n",
    "**make_classification**() function to create a synthetic binary classification problem with 1,000\n",
    "examples and 10 input features, five of which are informative and five of which are redundant.\n",
    "\n",
    "Next, we can evaluate an RFE feature selection algorithm on this dataset. We will use a\n",
    "**DecisionTreeClassifier** to choose features and set the number of features to five. We will\n",
    "then fit a new DecisionTreeClassifier model on the selected features. We will evaluate the\n",
    "model using repeated stratified k-fold cross-validation, with three repeats and 10 folds. We will\n",
    "report the mean and standard deviation of the accuracy of the model across all repeats and\n",
    "folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Recursive Feature Elimination (RFE) for classification\n",
    "\n",
    "# Define dataset\n",
    "# Generate a random n-class classification problem with 1000 samples, 10 features,\n",
    "# 5 informative features, and 5 redundant features. Random state ensures reproducibility.\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "# Initialize RFE with a DecisionTreeClassifier as the estimator\n",
    "# Set n_features_to_select=5 to keep 5 most important features\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "\n",
    "# Create the classification model using DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Create a pipeline that first applies RFE to select important features and then trains the model\n",
    "pipeline = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "# Evaluate model\n",
    "# Use RepeatedStratifiedKFold for cross-validation, which ensures the distribution of the target variable is maintained across folds.\n",
    "# 10 splits and 3 repeats will give more reliable results by testing multiple splits of the data.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Perform cross-validation to evaluate the pipeline's performance, using accuracy as the evaluation metric\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report performance\n",
    "# Calculate and print the mean and standard deviation of the cross-validation accuracy scores\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see the RFE that uses a decision tree and selects five features and\n",
    "then fits a decision tree on the selected features achieves a classification accuracy of about 88\n",
    "percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the RFE model pipeline as a final model and make predictions for classification. First, the RFE and model are fit on all available data, then the predict() function can\n",
    "be called to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with an RFE (Recursive Feature Elimination) pipeline\n",
    "\n",
    "# Define dataset\n",
    "# Generates a synthetic classification dataset with 1000 samples, 10 features (5 informative, 5 redundant)\n",
    "# The random_state is set for reproducibility of results\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "# Set up an RFE (Recursive Feature Elimination) model to select the top 5 features based on a DecisionTreeClassifier\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "# Create a basic DecisionTreeClassifier as the final model in the pipeline\n",
    "model = DecisionTreeClassifier()\n",
    "# Create a pipeline that first applies RFE for feature selection, then uses the DecisionTreeClassifier for prediction\n",
    "pipeline = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "# Fit the model on all available data\n",
    "# The pipeline is fit using all the data, where RFE first selects important features and then the DecisionTreeClassifier is trained\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Make a prediction for one example\n",
    "# Define a new data point with 10 feature values for prediction\n",
    "data = [\n",
    "    [\n",
    "        2.56999479,\n",
    "        -0.13019997,\n",
    "        3.16075093,\n",
    "        -4.35936352,\n",
    "        -1.61271951,\n",
    "        -1.39352057,\n",
    "        -2.48924933,\n",
    "        -1.93094078,\n",
    "        3.26130366,\n",
    "        2.05692145,\n",
    "    ]\n",
    "]\n",
    "# Make a prediction using the fitted pipeline\n",
    "yhat = pipeline.predict(data)\n",
    "\n",
    "# Print the predicted class\n",
    "# Output the predicted class label for the input data point\n",
    "# Ensure you're extracting the correct element from the array\n",
    "if yhat.ndim > 0:\n",
    "    print(\"Predicted: %.3f\" % (yhat.item()))  # Use .item() to get a single element\n",
    "else:\n",
    "    print(\"Predicted: %.3f\" % (yhat))  # If it's already a scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE for Regression\n",
    "\n",
    "Next, we will look at using RFE for a regression problem. First, we can use the\n",
    "**make_regression**() function to create a synthetic regression problem with 1,000 examples and\n",
    "10 input features, five of which are important and five of which are redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate RFE for regression\n",
    "\n",
    "# define dataset\n",
    "\n",
    "# Generate a random regression problem.\n",
    "# 'n_samples=1000' specifies the number of data points.\n",
    "# 'n_features=10' defines the total number of features.\n",
    "# 'n_informative=5' sets how many features are informative for the model (the rest are noise).\n",
    "# 'random_state=1' ensures reproducibility by fixing the random number generator seed.\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "\n",
    "# Create pipeline to combine feature selection and model fitting\n",
    "# 'RFE' (Recursive Feature Elimination) selects the top 'n_features_to_select' features using an estimator.\n",
    "# 'estimator=DecisionTreeRegressor()' uses a Decision Tree Regressor as the model for feature importance.\n",
    "# 'n_features_to_select=5' specifies that 5 features should be selected after performing RFE.\n",
    "rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=5)\n",
    "\n",
    "# Define the model to use after feature selection\n",
    "# 'DecisionTreeRegressor' is chosen to build the regression model.\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Create a pipeline that combines RFE and the DecisionTreeRegressor model\n",
    "# The pipeline consists of two steps: first 'RFE' for feature selection ('s'), then 'DecisionTreeRegressor' ('m').\n",
    "pipeline = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "# Repeated K-Fold cross validator.\n",
    "# Repeats K-Fold n times with different randomization in each repetition.\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# All scorer objects follow the convention that higher return values are better than lower return values.\n",
    "# Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error,\n",
    "# are available as neg_mean_squared_error which return the negated value of the metric.\n",
    "n_scores = cross_val_score(\n",
    "    pipeline, X, y, scoring=\"neg_mean_absolute_error\", cv=cv, n_jobs=-1\n",
    ")\n",
    "\n",
    "# report performance\n",
    "print(\"MAE: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see the RFE pipeline with a decision tree model achieves a MAE of\n",
    "about 27."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the RFE as part of the final model and make predictions for regression.\n",
    "First, the Pipeline is fit on all available data, then the predict() function can be called to\n",
    "make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a regression prediction with an RFE pipeline\n",
    "\n",
    "# Import necessary libraries (assumed to be already imported)\n",
    "# - make_regression: used to generate synthetic regression data\n",
    "# - RFE: Recursive Feature Elimination for feature selection\n",
    "# - DecisionTreeRegressor: decision tree model used for regression\n",
    "# - Pipeline: used to streamline multiple steps (feature selection and modeling) into a single workflow\n",
    "\n",
    "# Generate a synthetic regression dataset\n",
    "# X = features, y = target variable\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "# n_samples: Number of samples (1000)\n",
    "# n_features: Number of features (10)\n",
    "# n_informative: Number of informative features (5)\n",
    "# random_state: Seed for reproducibility\n",
    "\n",
    "# Create a pipeline for feature selection and model fitting\n",
    "# Initialize Recursive Feature Elimination (RFE) with DecisionTreeRegressor as the estimator\n",
    "# RFE will select the 5 most important features from the dataset\n",
    "rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=5)\n",
    "\n",
    "# Initialize a DecisionTreeRegressor model (to be used after feature selection)\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Combine the feature selection (RFE) and model (DecisionTreeRegressor) into a single pipeline\n",
    "# 'steps' defines the sequence of operations: first RFE for feature selection, then DecisionTreeRegressor for prediction\n",
    "pipeline = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "# Fit the pipeline on the entire dataset (X = features, y = target)\n",
    "# The pipeline will first perform RFE to select features, then train the DecisionTreeRegressor model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Make a prediction using the trained pipeline on a single example (data)\n",
    "# 'data' represents a single input sample to predict the target variable (y)\n",
    "data = [\n",
    "    [\n",
    "        -2.02220122,\n",
    "        0.31563495,\n",
    "        0.82797464,\n",
    "        -0.30620401,\n",
    "        0.16003707,\n",
    "        -1.44411381,\n",
    "        0.87616892,\n",
    "        -0.50446586,\n",
    "        0.23009474,\n",
    "        0.76201118,\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Use the fitted pipeline to make a prediction for the input 'data'\n",
    "yhat = pipeline.predict(data)\n",
    "\n",
    "# Print the predicted value, formatted to 3 decimal places\n",
    "# Ensure you're extracting the correct element from the array\n",
    "if yhat.ndim > 0:\n",
    "    print(\"Predicted: %.3f\" % (yhat.item()))  # Use .item() to get a single element\n",
    "else:\n",
    "    print(\"Predicted: %.3f\" % (yhat))  # If it's already a scalar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE Hyperparameters\n",
    "\n",
    "In this section, we will take a closer look at some of the hyperparameters you should consider\n",
    "tuning for the RFE method for feature selection and their effect on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Number of Features\n",
    "\n",
    "An important hyperparameter for the RFE algorithm is the number of features to select. In\n",
    "the previous section, we used an arbitrary number of selected features, five, which matches\n",
    "the number of informative features in the synthetic dataset. In practice, we cannot know the\n",
    "best number of features to select with RFE; instead, it is good practice to test different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the number of selected features for RFE\n",
    "\n",
    "# Define a function to generate a synthetic dataset for classification\n",
    "def get_dataset():\n",
    "    # 'make_classification' generates a random classification problem dataset\n",
    "    # Parameters:\n",
    "    # - n_samples=1000: 1000 samples (data points) will be generated\n",
    "    # - n_features=10: 10 features (columns) will be created\n",
    "    # - n_informative=5: 5 of the features will be informative (useful for predicting the target)\n",
    "    # - n_redundant=5: 5 of the features will be redundant (correlated with the informative features)\n",
    "    # - random_state=1: ensures reproducibility by fixing the random seed\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1\n",
    "    )\n",
    "    \n",
    "    # Return the generated feature matrix (X) and the target vector (y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "# Function to create a list of models for evaluation using Recursive Feature Elimination (RFE) and Decision Tree Classifier\n",
    "def get_models():\n",
    "    models = {}  # Initialize an empty dictionary to store models\n",
    "    \n",
    "    # Loop through the range 2 to 9 to create models with different numbers of selected features\n",
    "    for i in range(2, 10):\n",
    "        # Create an RFE (Recursive Feature Elimination) instance with a DecisionTreeClassifier as the estimator\n",
    "        # RFE will select 'i' number of features\n",
    "        rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
    "        \n",
    "        # Initialize a DecisionTreeClassifier model\n",
    "        model = DecisionTreeClassifier()\n",
    "        \n",
    "        # Store the model in the dictionary, with the number of features as the key\n",
    "        # Use a Pipeline to chain the RFE and the DecisionTreeClassifier together\n",
    "        models[str(i)] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "    \n",
    "    # Return the dictionary containing all the models\n",
    "    return models\n",
    "\n",
    "\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    return scores# Function to evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # Initialize a RepeatedStratifiedKFold cross-validation strategy\n",
    "    # n_splits=10: 10 folds (subsets of data)\n",
    "    # n_repeats=3: repeat the cross-validation process 3 times for more reliable results\n",
    "    # random_state=1: ensures reproducibility of results\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    \n",
    "    # Perform cross-validation using the model, input features (X), and target labels (y)\n",
    "    # scoring=\"accuracy\": evaluate performance based on accuracy\n",
    "    # cv=cv: use the defined cross-validation strategy\n",
    "    # n_jobs=-1: use all available CPU cores to speed up the process\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # Return the accuracy scores from the cross-validation process\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# Initialize two empty lists to store the results and model names\n",
    "results, names = [], []\n",
    "\n",
    "# Iterate over each model in the 'models' dictionary\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Evaluate the current model using the 'evaluate_model' function\n",
    "    # 'X' is the feature set, and 'y' is the target labels\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    \n",
    "    # Append the model's evaluation scores to the 'results' list\n",
    "    results.append(scores)\n",
    "    \n",
    "    # Append the model's name to the 'names' list\n",
    "    names.append(name)\n",
    "    \n",
    "    # Print the model's name, mean, and standard deviation of its evaluation scores\n",
    "    # 'mean(scores)' calculates the average score for the model, 'std(scores)' gives the standard deviation\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that performance improves as the number of features increase and\n",
    "perhaps peaks around 4-to-7 as we might expect, given that only  five features are relevant to\n",
    "the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box and whisker plot is created for the distribution of accuracy scores for each con gured\n",
    "number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary plotting module (assuming it's already imported)\n",
    "\n",
    "# Plotting a boxplot to compare the performance of different models\n",
    "# 'results' contains the performance data (e.g., accuracy scores, etc.) for each model\n",
    "# 'names' is a list of the model names corresponding to the performance results\n",
    "# The 'boxplot' function is used to generate a boxplot to compare the performance visually\n",
    "pyplot.boxplot(results, showmeans=True)  # Create the boxplot and display means on each box\n",
    "\n",
    "# Set the x-axis labels using 'xticklabels' (for model names corresponding to each boxplot)\n",
    "pyplot.xticks(ticks=range(1, len(names) + 1), labels=names)  # Set model names as x-axis labels\n",
    "\n",
    "# Display the plot to the user\n",
    "pyplot.show()  # Show the generated plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically Select the Number of Features\n",
    "\n",
    "It is also possible to automatically select the number of features chosen by RFE. This can be\n",
    "achieved by performing cross-validation evaluation of different numbers of features as we did in\n",
    "the previous section and automatically selecting the number of features that resulted in the\n",
    "best mean score. The **RFECV** class implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically select the number of features for Recursive Feature Elimination (RFE) using cross-validation\n",
    "\n",
    "# Define the dataset for classification\n",
    "# 'X' are the features, 'y' is the target variable\n",
    "# make_classification generates a synthetic classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,  # Number of samples in the dataset\n",
    "    n_features=10,   # Total number of features\n",
    "    n_informative=5, # Number of informative features (actually useful for prediction)\n",
    "    n_redundant=5,   # Number of redundant features (linear combinations of the informative ones)\n",
    "    random_state=1   # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Create a pipeline that combines feature selection and model training\n",
    "# Use Recursive Feature Elimination with Cross-Validation (RFECV) to automatically select the optimal number of features\n",
    "rfe = RFECV(estimator=DecisionTreeClassifier())  # RFE uses a DecisionTreeClassifier as the estimator\n",
    "model = DecisionTreeClassifier()  # The model to be used after feature selection\n",
    "pipeline = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])  # Pipeline applies RFE and then the classifier\n",
    "\n",
    "# Evaluate the model performance using cross-validation\n",
    "# RepeatedStratifiedKFold splits the dataset into 10 folds and repeats 3 times for more reliable results\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Perform cross-validation using the pipeline, scoring by accuracy, and running in parallel with n_jobs=-1\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report the mean and standard deviation of accuracy from the cross-validation results\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))  # Display average accuracy and its variation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see the RFE that uses a decision tree and automatically selects a number\n",
    "of features and then fits a decision tree on the selected features achieves a classification accuracy\n",
    "of about 88 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Features Were Selected\n",
    "\n",
    "When using RFE, we may be interested to know which features were selected and which were\n",
    "removed. This can be achieved by reviewing the attributes of the fit **RFE** object (or fit **RFECV**\n",
    "object). The support attribute reports true or false as to which features in order of column\n",
    "index were included and the ranking attribute reports the relative ranking of features in the\n",
    "same order. The example below fits an RFE model on the whole dataset and selects five features,\n",
    "then reports each feature column index (0 to 9), whether it was selected or not (True or False),\n",
    "and the relative feature ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report which features were selected by RFE (Recursive Feature Elimination)\n",
    "\n",
    "# Define the dataset\n",
    "# 'X' is the feature matrix, and 'y' is the target variable\n",
    "# Using 'make_classification' to generate a synthetic dataset with 1000 samples and 10 features\n",
    "# 5 informative features and 5 redundant features are used\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,  # number of samples\n",
    "    n_features=10,   # total number of features\n",
    "    n_informative=5, # number of informative features\n",
    "    n_redundant=5,   # number of redundant features\n",
    "    random_state=1   # for reproducibility\n",
    ")\n",
    "\n",
    "# Define the RFE (Recursive Feature Elimination) model\n",
    "# 'estimator' is the model used to evaluate the feature importance (here a DecisionTreeClassifier)\n",
    "# 'n_features_to_select' specifies how many features should be selected\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "\n",
    "# Fit RFE on the dataset\n",
    "# This will eliminate irrelevant features and rank the remaining features\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Summarize the selected features and their ranks\n",
    "# 'rfe.support_' indicates which features are selected (True/False)\n",
    "# 'rfe.ranking_' shows the ranking of all features (1 means the most important feature)\n",
    "for i in range(X.shape[1]):\n",
    "    print(\"Column: %d, Selected=%s, Rank: %d\" % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    # Print the index of the feature, whether it's selected (True/False), and its ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Estimator Algorithm\n",
    "\n",
    "There are many algorithms that can be used in the core RFE, as long as they provide some\n",
    "indication of variable importance. Most decision tree algorithms are likely to report the same\n",
    "general trends in feature importance, but this is not guaranteed. It might be helpful to explore\n",
    "the use of different algorithms wrapped by RFE. The example below demonstrates how you\n",
    "might explore this configuration option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the algorithm wrapped by RFE\n",
    "\n",
    "# Function to generate a dataset for classification\n",
    "def get_dataset():\n",
    "    # Create a synthetic classification dataset with 1000 samples, 10 features, 5 informative and 5 redundant features\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1\n",
    "    )\n",
    "    return X, y  # Return the features (X) and labels (y)\n",
    "\n",
    "\n",
    "# Function to create a dictionary of models wrapped with RFE (Recursive Feature Elimination)\n",
    "def get_models():\n",
    "    models = {}  # Initialize an empty dictionary to store models\n",
    "\n",
    "    # Logistic Regression (lr)\n",
    "    # RFE is wrapped around LogisticRegression to select the top 5 features\n",
    "    rfe = RFE(estimator=LogisticRegression(), n_features_to_select=5)\n",
    "    model = DecisionTreeClassifier()  # Use DecisionTreeClassifier as the final model\n",
    "    models[\"lr\"] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])  # Create a pipeline with RFE and model\n",
    "\n",
    "    # Perceptron (per)\n",
    "    # Wrap RFE around Perceptron estimator to select 5 features\n",
    "    rfe = RFE(estimator=Perceptron(), n_features_to_select=5)\n",
    "    model = DecisionTreeClassifier()  # Use DecisionTreeClassifier as the final model\n",
    "    models[\"per\"] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])  # Add to the models dictionary\n",
    "\n",
    "    # Decision Tree Classifier (dtc)\n",
    "    # Wrap RFE around DecisionTreeClassifier estimator\n",
    "    rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "    model = DecisionTreeClassifier()  # Use DecisionTreeClassifier as the final model\n",
    "    models[\"dtc\"] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "    # Random Forest Classifier (rf)\n",
    "    # Wrap RFE around RandomForestClassifier estimator\n",
    "    rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=5)\n",
    "    model = DecisionTreeClassifier()  # Use DecisionTreeClassifier as the final model\n",
    "    models[\"rf\"] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "    # Gradient Boosting Classifier (gbm)\n",
    "    # Wrap RFE around GradientBoostingClassifier estimator\n",
    "    rfe = RFE(estimator=GradientBoostingClassifier(), n_features_to_select=5)\n",
    "    model = DecisionTreeClassifier()  # Use DecisionTreeClassifier as the final model\n",
    "    models[\"gbm\"] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "    return models  # Return the dictionary containing models with RFE\n",
    "\n",
    "\n",
    "# Function to evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # Define cross-validation strategy: StratifiedKFold with 10 splits and 3 repeats\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # Perform cross-validation and calculate accuracy scores\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    return scores  # Return the accuracy scores\n",
    "\n",
    "\n",
    "# Define the dataset by calling the function get_dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# Get the models to evaluate by calling get_models\n",
    "models = get_models()\n",
    "\n",
    "# Initialize lists to store results and model names\n",
    "results, names = [], []\n",
    "\n",
    "# Loop through each model in the models dictionary\n",
    "for name, model in models.items():\n",
    "    # Evaluate each model using cross-validation and store the results\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)  # Append the scores for this model\n",
    "    names.append(name)  # Append the model name\n",
    "    # Print the model name, mean accuracy, and standard deviation of the accuracy scores\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))  # Print the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the results suggest that linear algorithms like logistic regression might select better features more reliably than the chosen decision tree and ensemble\n",
    "of decision tree algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary plotting module (assuming it's already imported)\n",
    "\n",
    "# Plotting a boxplot to compare the performance of different models\n",
    "# 'results' contains the performance data (e.g., accuracy scores, etc.) for each model\n",
    "# 'names' is a list of the model names corresponding to the performance results\n",
    "# The 'boxplot' function is used to generate a boxplot to compare the performance visually\n",
    "pyplot.boxplot(results, showmeans=True)  # Create the boxplot and display means on each box\n",
    "\n",
    "# Set the x-axis labels using 'xticklabels' (for model names corresponding to each boxplot)\n",
    "pyplot.xticks(ticks=range(1, len(names) + 1), labels=names)  # Set model names as x-axis labels\n",
    "\n",
    "# Display the plot to the user\n",
    "pyplot.show()  # Show the generated plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box and whisker plot is created for the distribution of accuracy scores for each configured\n",
    "wrapped algorithm. We can see the general trend of good performance with logistic regression,\n",
    "DTC and perhaps GBM. The model used within RFE can make an important\n",
    "difference to which features are selected and in turn the performance on the prediction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this tutorial, we learned how to effectively use Recursive Feature Elimination for feature selection in both classification and regression problems. We explored key hyperparameters like the number of features to select and the choice of wrapped algorithm. The techniques covered demonstrate how RFE can be used to identify the most relevant features for predictive modeling tasks.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
