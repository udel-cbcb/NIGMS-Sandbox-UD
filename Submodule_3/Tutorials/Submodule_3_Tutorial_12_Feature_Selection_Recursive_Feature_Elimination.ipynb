{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection:  Recursive Feature Elimination\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Recursive Feature Elimination (RFE)\n",
    "\n",
    "**Recursive Feature Elimination (RFE)** is a widely-used feature selection algorithm. It is popular due to its simplicity, ease of configuration, and effectiveness in identifying the most relevant features (columns) in a training dataset for predicting the target variable.\n",
    "\n",
    "When using RFE, there are two important hyperparameters to consider:\n",
    "\n",
    "1. **Number of Features to Select**: This determines how many features the algorithm will retain.\n",
    "2. **Algorithm for Feature Selection**: The choice of algorithm (e.g., linear models, decision trees) used to evaluate feature importance.\n",
    "\n",
    "While these hyperparameters can be tuned, the performance of RFE is generally robust and not highly sensitive to their configuration. This makes RFE a versatile and reliable tool for feature selection in machine learning workflows.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn how RFE is an efficient approach for eliminating features from a training dataset for feature selection\n",
    "- Learn how to use RFE for feature selection for classification and regression predictive modeling problems\n",
    "- Learn how to explore the number of selected features and wrapped algorithm used by the RFE procedure\n",
    "- Understand how to evaluate different algorithms wrapped by RFE for optimal feature selection\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Implement RFE for classification problems\n",
    "- Implement RFE for regression problems \n",
    "- Explore RFE hyperparameters\n",
    "- Evaluate different estimator algorithms for RFE\n",
    "- Analyze selected features\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages and import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary Python packages using pip\n",
    "# 'matplotlib' for plotting\n",
    "# 'numpy' for numerical operations\n",
    "# 'scikit-learn' for machine learning tools\n",
    "\n",
    "%pip install matplotlib numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules for data processing, model building, and evaluation\n",
    "\n",
    "# Import pyplot from matplotlib for plotting graphs\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Import mean and std from numpy to calculate statistical measures (mean and standard deviation)\n",
    "from numpy import mean, std\n",
    "\n",
    "# Import datasets for generating synthetic data\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "\n",
    "# Import ensemble classifiers for building gradient boosting and random forest models\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# Import Recursive Feature Elimination (RFE) and its cross-validation version (RFECV) for feature selection\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "# Importing LogisticRegression and Perceptron from scikit-learn's linear_model module\n",
    "# - LogisticRegression: A linear model used for binary or multi-class classification tasks.\n",
    "# - Perceptron: A simple linear classifier that is a precursor to neural networks.\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "# Import model selection techniques for cross-validation\n",
    "from sklearn.model_selection import (\n",
    "    RepeatedKFold,  # Repeated k-fold cross-validation\n",
    "    RepeatedStratifiedKFold,  # Repeated Stratified k-fold cross-validation for classification tasks\n",
    "    cross_val_score,  # Function for performing cross-validation\n",
    ")\n",
    "\n",
    "# Import Pipeline for building a sequence of processing steps including preprocessing and model fitting\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import decision tree classifiers and regressors for building decision tree-based models\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE for Classification\n",
    "\n",
    "We begin by generating a synthetic binary classification dataset using scikit-learn's **make_classification()** function. This simulated dataset contains 1,000 samples with 10 input features, carefully structured to include five informative predictors that directly influence the target variable and five redundant features that are linear combinations of the informative ones. This controlled configuration allows us to evaluate feature selection methods while knowing ground truth feature importance, with the redundant features serving as an internal control to test a method's ability to filter out uninformative variables.\n",
    "\n",
    "We then implement Recursive Feature Elimination (RFE) with a DecisionTreeClassifier as the estimator, configured to select the top five most predictive features. Following feature selection, we train a new **DecisionTreeClassifier** exclusively on these optimized features and evaluate performance using rigorous repeated stratified k-fold cross-validation (10 folds with 3 repeats). This evaluation protocol yields robust performance estimates, which we summarize through both the mean classification accuracy and standard deviation across all repetitions, providing insights into both model accuracy and stability. The entire pipeline - from feature selection to final evaluation - is designed to assess whether feature optimization can enhance model performance while maintaining interpretability through a reduced feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.888 (0.033)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Recursive Feature Elimination (RFE) for classification\n",
    "\n",
    "# Define dataset\n",
    "# Generate a synthetic classification dataset using make_classification\n",
    "# - n_samples: Number of samples (rows) in the dataset (1000 in this case).\n",
    "# - n_features: Total number of features (columns) in the dataset (10 in this case).\n",
    "# - n_informative: Number of informative (relevant) features that contribute to the target variable (5 in this case).\n",
    "# - n_redundant: Number of redundant features that are linear combinations of the informative features (5 in this case).\n",
    "# - random_state: Seed for the random number generator to ensure reproducibility (set to 1).\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "# Initialize RFE with a DecisionTreeClassifier as the estimator\n",
    "# Set n_features_to_select=5 to keep 5 most important features\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(random_state=42), n_features_to_select=5)\n",
    "\n",
    "# Create the classification model using DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create a pipeline that first applies RFE to select important features and then trains the model\n",
    "pipeline = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "# Evaluate model\n",
    "# Use RepeatedStratifiedKFold for cross-validation, which ensures the distribution of the target variable is maintained across folds.\n",
    "# 10 splits and 3 repeats will give more reliable results by testing multiple splits of the data.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Perform cross-validation to evaluate the pipeline's performance, using accuracy as the evaluation metric\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report performance\n",
    "# Calculate and print the mean and standard deviation of the cross-validation accuracy scores\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recursive feature elimination (RFE) implementation utilizing a decision tree classifier demonstrates strong performance, achieving 89% classification accuracy when configured to select the top five predictive features. This result suggests that the feature subset identified by the tree-based RFE effectively captures the most discriminative patterns in the data while eliminating redundant or noisy variables. Notably, the decision tree's inherent feature selection capability—combined with RFE's wrapper method—yields a parsimonious model that maintains high predictive power. The 89% accuracy benchmark provides a compelling baseline for comparing alternative feature selection approaches or more complex ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFE-model pipeline can serve as a finalized prediction system once feature selection is complete. After fitting the RFE selector and model on the entire training dataset, the pipeline becomes capable of making predictions on new, unseen data via the **predict()** method. This end-to-end approach ensures that the same feature selection process applied during training is automatically applied to future data, maintaining consistency in the model's input structure. For classification tasks, the pipeline outputs class labels directly, while **predict_proba()** can be used if probability estimates are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with an RFE (Recursive Feature Elimination) pipeline\n",
    "\n",
    "# Define dataset\n",
    "# Generate a synthetic classification dataset using make_classification\n",
    "# - n_samples: Number of samples (rows) in the dataset (1000 in this case).\n",
    "# - n_features: Total number of features (columns) in the dataset (10 in this case).\n",
    "# - n_informative: Number of informative (relevant) features that contribute to the target variable (5 in this case).\n",
    "# - n_redundant: Number of redundant features that are linear combinations of the informative features (5 in this case).\n",
    "# - random_state: Seed for the random number generator to ensure reproducibility (set to 1).\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "# Set up an RFE (Recursive Feature Elimination) model to select the top 5 features based on a DecisionTreeClassifier\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(random_state=42), n_features_to_select=5)\n",
    "\n",
    "# Create a basic DecisionTreeClassifier as the final model in the pipeline\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create a pipeline that first applies RFE for feature selection, then uses the DecisionTreeClassifier for prediction\n",
    "pipeline = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "# Fit the model on all available data\n",
    "# The pipeline is fit using all the data, where RFE first selects important features and then the DecisionTreeClassifier is trained\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Make a prediction for one example\n",
    "# Define a new data point with 10 feature values for prediction\n",
    "data = [\n",
    "    [\n",
    "        2.56999479,\n",
    "        -0.13019997,\n",
    "        3.16075093,\n",
    "        -4.35936352,\n",
    "        -1.61271951,\n",
    "        -1.39352057,\n",
    "        -2.48924933,\n",
    "        -1.93094078,\n",
    "        3.26130366,\n",
    "        2.05692145,\n",
    "    ]\n",
    "]\n",
    "# Make a prediction using the fitted pipeline\n",
    "yhat = pipeline.predict(data)\n",
    "\n",
    "# Print the predicted class\n",
    "# Output the predicted class label for the input data point\n",
    "# Ensure you're extracting the correct element from the array\n",
    "if yhat.ndim > 0:\n",
    "    print(\"Predicted: %.3f\" % (yhat.item()))  # Use .item() to get a single element\n",
    "else:\n",
    "    print(\"Predicted: %.3f\" % (yhat))  # If it's already a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's prediction of class 1 for the input data point suggests a strong alignment between the observed feature values and the distinguishing patterns characteristic of this class. This classification outcome implies that the input data's attributes—whether in isolation or through their multivariate interactions—resonate with the learned decision boundaries that define class 1 membership in our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE for Regression\n",
    "\n",
    "To demonstrate Recursive Feature Elimination (RFE) for regression tasks, we begin by generating a synthetic dataset using scikit-learn’s **make_regression()** function. This dataset will simulate a realistic scenario with 1,000 observations and 10 input features, where only 5 features are truly informative while the remaining 5 serve as redundant variables. This controlled environment allows us to:\n",
    "* Validate RFE’s ability to identify truly predictive features\n",
    "* Assess its performance in eliminating noise variables\n",
    "* Establish a benchmark for feature selection efficacy\n",
    "\n",
    "The synthetic data’s known ground truth (5 important vs. 5 redundant features) provides an objective evaluation framework before applying RFE to real-world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -26.642 (2.564)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RFE for regression\n",
    "\n",
    "# Define dataset\n",
    "# Generate a random regression problem.\n",
    "# 'n_samples=1000' specifies the number of data points.\n",
    "# 'n_features=10' defines the total number of features.\n",
    "# 'n_informative=5' sets how many features are informative for the model (the rest are noise).\n",
    "# 'random_state=1' ensures reproducibility by fixing the random number generator seed.\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "\n",
    "# Create pipeline to combine feature selection and model fitting\n",
    "# 'RFE' (Recursive Feature Elimination) selects the top 'n_features_to_select' features using an estimator.\n",
    "# 'estimator=DecisionTreeRegressor()' uses a Decision Tree Regressor as the model for feature importance.\n",
    "# 'n_features_to_select=5' specifies that 5 features should be selected after performing RFE.\n",
    "rfe = RFE(estimator=DecisionTreeRegressor(random_state=42), n_features_to_select=5)\n",
    "\n",
    "# Define the model to use after feature selection\n",
    "# 'DecisionTreeRegressor' is chosen to build the regression model.\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Create a pipeline that combines RFE and the DecisionTreeRegressor model\n",
    "# The pipeline consists of two steps: first 'RFE' for feature selection ('s'), then 'DecisionTreeRegressor' ('m').\n",
    "pipeline = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "# Initialize RepeatedKFold for cross-validation\n",
    "# - n_splits: Number of folds (splits) for cross-validation (10 in this case).\n",
    "# - n_repeats: Number of times the cross-validation process is repeated (3 in this case).\n",
    "# - random_state: Seed for the random number generator to ensure reproducibility (set to 1).\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# All scorer objects follow the convention that higher return values are better than lower return values.\n",
    "# Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error,\n",
    "# are available as neg_mean_squared_error which return the negated value of the metric.\n",
    "n_scores = cross_val_score(\n",
    "    pipeline, X, y, scoring=\"neg_mean_absolute_error\", cv=cv, n_jobs=-1\n",
    ")\n",
    "\n",
    "# report performance\n",
    "print(\"MAE: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recursive feature elimination (RFE) pipeline utilizing a decision tree model demonstrates a mean absolute error (MAE) of -26.642 ± 2.564. This result indicates that, on average, the model's predictions deviate from actual values by approximately 27 units (with a standard error of 2.56). The negative sign preceding the MAE value suggests potential systematic underestimation in the model's predictions, which warrants further investigation into bias correction techniques or feature engineering to improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination (RFE) can be seamlessly integrated into a production modeling pipeline for regression tasks. By fitting the Pipeline object (containing both the RFE selector and regression model) on the complete training dataset, we create an end-to-end system that automatically handles feature selection during both training and prediction phases. Once trained, this unified pipeline enables efficient deployment - simply calling **predict()** on new data will first apply the learned feature selection rules before generating predictions, ensuring consistent preprocessing across all environments. This approach maintains the benefits of RFE's feature optimization while simplifying the operational workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: -84.288\n"
     ]
    }
   ],
   "source": [
    "# Make a regression prediction with an RFE pipeline\n",
    "# - make_regression: used to generate synthetic regression data\n",
    "# - RFE: Recursive Feature Elimination for feature selection\n",
    "# - DecisionTreeRegressor: decision tree model used for regression\n",
    "# - Pipeline: used to streamline multiple steps (feature selection and modeling) into a single workflow\n",
    "\n",
    "# Generate a synthetic regression dataset\n",
    "# n_samples: Number of samples (1000)\n",
    "# n_features: Number of features (10)\n",
    "# n_informative: Number of informative features (5)\n",
    "# random_state: Seed for reproducibility\n",
    "# X = features, y = target variable\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "\n",
    "\n",
    "# Create a pipeline for feature selection and model fitting\n",
    "# Initialize Recursive Feature Elimination (RFE) with DecisionTreeRegressor as the estimator\n",
    "# RFE will select the 5 most important features from the dataset\n",
    "rfe = RFE(estimator=DecisionTreeRegressor(random_state=42), n_features_to_select=5)\n",
    "\n",
    "# Initialize a DecisionTreeRegressor model (to be used after feature selection)\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Combine the feature selection (RFE) and model (DecisionTreeRegressor) into a single pipeline\n",
    "# 'steps' defines the sequence of operations: first RFE for feature selection, then DecisionTreeRegressor for prediction\n",
    "pipeline = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "# Fit the pipeline on the entire dataset (X = features, y = target)\n",
    "# The pipeline will first perform RFE to select features, then train the DecisionTreeRegressor model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Make a prediction using the trained pipeline on a single example (data)\n",
    "# 'data' represents a single input sample to predict the target variable (y)\n",
    "data = [\n",
    "    [\n",
    "        -2.02220122,\n",
    "        0.31563495,\n",
    "        0.82797464,\n",
    "        -0.30620401,\n",
    "        0.16003707,\n",
    "        -1.44411381,\n",
    "        0.87616892,\n",
    "        -0.50446586,\n",
    "        0.23009474,\n",
    "        0.76201118,\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Use the fitted pipeline to make a prediction for the input 'data'\n",
    "yhat = pipeline.predict(data)\n",
    "\n",
    "# Print the predicted value, formatted to 3 decimal places\n",
    "# Ensure you're extracting the correct element from the array\n",
    "if yhat.ndim > 0:\n",
    "    print(\"Predicted: %.3f\" % (yhat.item()))  # Use .item() to get a single element\n",
    "else:\n",
    "    print(\"Predicted: %.3f\" % (yhat))  # If it's already a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model generated a predicted output value of -84.288, which suggests several possible interpretations depending on the context. For regression tasks, this substantial negative value may indicate either: (1) a legitimate prediction within the expected range of the target variable's distribution, (2) an extreme outlier prediction warranting investigation, or (3) potential model artifacts requiring validation. The negative sign specifically implies an inverse relationship with the input features, where increasing values of key predictors correspond to decreasing target values. To properly assess this result, we should compare it against the training data distribution (minimum/maximum observed values) and consider running diagnostic checks on both the input features and model assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE Hyperparameters\n",
    "\n",
    "This section provides a comprehensive examination of key hyperparameters in Recursive Feature Elimination (RFE) and their impact on model efficacy. Through systematic evaluation, we identify optimal configurations that balance model simplicity with accuracy, while addressing common pitfalls such as over-aggressive feature elimination or estimator bias. These insights enable more informed implementation of RFE across different dataset characteristics and modeling objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Number of Features\n",
    "\n",
    "The performance of Recursive Feature Elimination (RFE) critically depends on selecting an appropriate number of features, a key hyperparameter that requires careful tuning. While our previous analysis arbitrarily chose five features—aligning with the known structure of the synthetic dataset—real-world applications demand systematic evaluation. Since the optimal feature count is rarely known a priori, best practices recommend testing a range of values through cross-validation. This empirical approach not only identifies the most predictive subset but also helps balance the trade-off between model complexity (fewer features) and predictive power (more features). For robust results, consider evaluating feature counts across a geometrically spaced sequence (e.g., 5, 10, 20, 40) to efficiently explore the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2 0.849 (0.031)\n",
      ">3 0.858 (0.033)\n",
      ">4 0.867 (0.033)\n",
      ">5 0.874 (0.029)\n",
      ">6 0.881 (0.032)\n",
      ">7 0.890 (0.021)\n",
      ">8 0.888 (0.023)\n",
      ">9 0.884 (0.034)\n"
     ]
    }
   ],
   "source": [
    "# Explore the number of selected features for RFE\n",
    "\n",
    "# Define a function to generate a synthetic dataset for classification\n",
    "def get_dataset():\n",
    "    # 'make_classification' generates a random classification problem dataset\n",
    "    # Parameters:\n",
    "    # - n_samples=1000: 1000 samples (data points) will be generated\n",
    "    # - n_features=10: 10 features (columns) will be created\n",
    "    # - n_informative=5: 5 of the features will be informative (useful for predicting the target)\n",
    "    # - n_redundant=5: 5 of the features will be redundant (correlated with the informative features)\n",
    "    # - random_state=42: ensures reproducibility by fixing the random seed\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Return the generated feature matrix (X) and the target vector (y)\n",
    "    return X, y\n",
    "\n",
    "# Function to create a list of models for evaluation using Recursive Feature Elimination (RFE) and Decision Tree Classifier\n",
    "def get_models():\n",
    "    models = {}  # Initialize an empty dictionary to store models\n",
    "    \n",
    "    # Loop through the range 2 to 9 to create models with different numbers of selected features\n",
    "    for i in range(2, 10):\n",
    "        # Create an RFE (Recursive Feature Elimination) instance with a DecisionTreeClassifier as the estimator\n",
    "        # RFE will select 'i' number of features\n",
    "        rfe = RFE(estimator=DecisionTreeClassifier(random_state=42), n_features_to_select=i)\n",
    "        \n",
    "        # Initialize a DecisionTreeClassifier model\n",
    "        model = DecisionTreeClassifier(random_state=42)\n",
    "        \n",
    "        # Store the model in the dictionary, with the number of features as the key\n",
    "        # Use a Pipeline to chain the RFE and the DecisionTreeClassifier together\n",
    "        models[str(i)] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "    \n",
    "    # Return the dictionary containing all the models\n",
    "    return models\n",
    "\n",
    "# Evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # Initialize a RepeatedStratifiedKFold cross-validation strategy\n",
    "    # n_splits=10: 10 folds (subsets of data)\n",
    "    # n_repeats=3: repeat the cross-validation process 3 times for more reliable results\n",
    "    # random_state=1: ensures reproducibility of results\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    \n",
    "    # Perform cross-validation using the model, input features (X), and target labels (y)\n",
    "    # scoring=\"accuracy\": evaluate performance based on accuracy\n",
    "    # cv=cv: use the defined cross-validation strategy\n",
    "    # n_jobs=-1: use all available CPU cores to speed up the process\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # Return the accuracy scores from the cross-validation process\n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# Initialize two empty lists to store the results and model names\n",
    "results, names = [], []\n",
    "\n",
    "# Iterate over each model in the 'models' dictionary\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Evaluate the current model using the 'evaluate_model' function\n",
    "    # 'X' is the feature set, and 'y' is the target labels\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    \n",
    "    # Append the model's evaluation scores to the 'results' list\n",
    "    results.append(scores)\n",
    "    \n",
    "    # Append the model's name to the 'names' list\n",
    "    names.append(name)\n",
    "    \n",
    "    # Print the model's name, mean, and standard deviation of its evaluation scores\n",
    "    # 'mean(scores)' calculates the average score for the model, 'std(scores)' gives the standard deviation\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the impact of feature selection on model performance, we generate a box-and-whisker plot visualizing the distribution of accuracy scores across different feature subset sizes. The plot reveals key insights about the trade-off between dimensionality reduction and predictive power: narrower boxes indicate stable performance for a given feature count, while whisker length shows variance across cross-validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing the necessary plotting module (assuming it's already imported)\n",
    "\n",
    "# Plotting a boxplot to compare the performance of different models\n",
    "# 'results' contains the performance data (e.g., accuracy scores, etc.) for each model\n",
    "# 'names' is a list of the model names corresponding to the performance results\n",
    "# The 'boxplot' function is used to generate a boxplot to compare the performance visually\n",
    "pyplot.boxplot(results, showmeans=True)  # Create the boxplot and display means on each box\n",
    "\n",
    "# Add x-axis and y-axis labels\n",
    "pyplot.xlabel(\"Number of Features\")\n",
    "pyplot.ylabel(\"Accuracy\")\n",
    "\n",
    "# Set the x-axis labels using 'xticklabels' (for model names corresponding to each boxplot)\n",
    "pyplot.xticks(ticks=range(1, len(names) + 1), labels=names)  # Set model names as x-axis labels\n",
    "\n",
    "# Display the plot to the user\n",
    "pyplot.show()  # Show the generated plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This box plot illustrates the relationship between the number of features used in a model and the resulting accuracy scores. The x-axis represents the number of features selected, ranging from 2 to 9, while the y-axis shows the corresponding accuracy scores, which fall between approximately 0.775 and 0.950. For each feature count, a box plot summarizes the distribution of accuracy scores, providing insights into the model's performance variability.\n",
    "\n",
    "Each box represents the interquartile range (IQR), capturing the middle 50% of the accuracy values. The orange horizontal line inside each box marks the median accuracy, while the green triangle likely indicates the mean. The whiskers extend to the minimum and maximum values that are not considered outliers, and any outliers are shown as individual points outside the whiskers.\n",
    "\n",
    "From the plot, it is evident that as the number of features increases, both the median and mean accuracies tend to rise, particularly between 2 and 6 features. This trend suggests that including more features initially improves model performance. However, after about 6 features, the gains in accuracy become less consistent. For instance, although the median accuracy remains high from 6 to 9 features, the variability in performance also increases slightly in some cases, and the presence of outliers becomes more noticeable.\n",
    "\n",
    "Overall, this plot implies that model accuracy improves with the addition of features up to a certain point, beyond which the benefit plateaus or introduces greater variability. A feature count around 6 to 7 appears to offer a good trade-off between accuracy and stability, indicating a potentially optimal range for model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically Select the Number of Features\n",
    "\n",
    "To automate the selection of optimal features, we can leverage **RFECV (Recursive Feature Elimination with Cross-Validation)**, which systematically evaluates feature subsets of varying sizes through cross-validation. This method extends the standard RFE approach by dynamically identifying the number of features that yield the highest mean performance score across validation folds. By iteratively removing the least important features and assessing model accuracy at each step, RFECV eliminates the need for manual threshold-setting while ensuring robust feature selection. As demonstrated in our prior analysis, this data-driven approach balances model complexity and predictive power by selecting only the most contributive features, mitigating overfitting risks inherent in high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.889 (0.028)\n"
     ]
    }
   ],
   "source": [
    "# Automatically select the number of features for Recursive Feature Elimination (RFE) using cross-validation\n",
    "\n",
    "# Define the dataset for classification\n",
    "# 'X' are the features, 'y' is the target variable\n",
    "# make_classification generates a synthetic classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,  # Number of samples in the dataset\n",
    "    n_features=10,   # Total number of features\n",
    "    n_informative=5, # Number of informative features (actually useful for prediction)\n",
    "    n_redundant=5,   # Number of redundant features (linear combinations of the informative ones)\n",
    "    random_state=1   # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Initialize Recursive Feature Elimination with Cross-Validation (RFECV)\n",
    "# - estimator: The model used to evaluate feature importance (DecisionTreeClassifier in this case).\n",
    "# - RFECV automatically selects the optimal number of features by cross-validating different feature subsets.\n",
    "rfe = RFECV(estimator=DecisionTreeClassifier(random_state=42))\n",
    "\n",
    "# Initialize a DecisionTreeClassifier model\n",
    "# - This model will be used for classification after feature selection.\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create a pipeline to streamline the workflow\n",
    "# - steps: A list of tuples defining the sequence of operations:\n",
    "#   - \"s\": RFECV for feature selection.\n",
    "#   - \"m\": DecisionTreeClassifier for classification.\n",
    "pipeline = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "# Evaluate the model performance using cross-validation\n",
    "# RepeatedStratifiedKFold splits the dataset into 10 folds and repeats 3 times for more reliable results\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Perform cross-validation using the pipeline, scoring by accuracy, and running in parallel with n_jobs=-1\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report the mean and standard deviation of accuracy from the cross-validation results\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))  # Display average accuracy and its variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recursive Feature Elimination (RFE) pipeline, which leverages a decision tree model for both feature selection and final classification, demonstrates strong performance with an accuracy of 89%. This two-stage approach—first automatically identifying the optimal feature subset through RFE, then training a decision tree on these selected features—effectively balances dimensionality reduction with predictive power. The high accuracy suggests that the method successfully retains the most discriminative features while eliminating noise or redundancy in the input space. Notably, this performance is achieved without manual feature selection, highlighting the utility of automated wrapper methods for model optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Features Were Selected\n",
    "\n",
    "When performing Recursive Feature Elimination (RFE), it is critical to examine the final feature selection outcomes. The fitted RFE or RFECV object provides two key attributes for this analysis:\n",
    "* support_: A Boolean mask (True/False) indicating which features (ordered by column index) were retained.\n",
    "* ranking_: An array of integers representing the relative importance ranking of all features (where 1 denotes selected features and higher numbers indicate elimination order).\n",
    "\n",
    "For example, applying RFE to select the top 5 features from a 10-column dataset would yield:\n",
    "* Column indices (0 to 9) mapped to their selection status (True if among the top 5).\n",
    "* Rankings showing the elimination sequence (e.g., [3, 1, 2, 1, 1, 4, 1, 5, 1, 6] where 1 marks the 5 best features).\n",
    "\n",
    "This transparency enables precise interpretation of feature importance and supports validation of the selection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected=False, Rank: 4\n",
      "Column: 1, Selected=False, Rank: 6\n",
      "Column: 2, Selected=True, Rank: 1\n",
      "Column: 3, Selected=True, Rank: 1\n",
      "Column: 4, Selected=True, Rank: 1\n",
      "Column: 5, Selected=False, Rank: 5\n",
      "Column: 6, Selected=True, Rank: 1\n",
      "Column: 7, Selected=False, Rank: 3\n",
      "Column: 8, Selected=True, Rank: 1\n",
      "Column: 9, Selected=False, Rank: 2\n"
     ]
    }
   ],
   "source": [
    "# Report which features were selected by RFE (Recursive Feature Elimination)\n",
    "\n",
    "# Define the dataset\n",
    "# 'X' is the feature matrix, and 'y' is the target variable\n",
    "# Using 'make_classification' to generate a synthetic dataset with 1000 samples and 10 features\n",
    "# 5 informative features and 5 redundant features are used\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,  # number of samples\n",
    "    n_features=10,   # total number of features\n",
    "    n_informative=5, # number of informative features\n",
    "    n_redundant=5,   # number of redundant features\n",
    "    random_state=1   # for reproducibility\n",
    ")\n",
    "\n",
    "# Define the RFE (Recursive Feature Elimination) model\n",
    "# 'estimator' is the model used to evaluate the feature importance (here a DecisionTreeClassifier)\n",
    "# 'n_features_to_select' specifies how many features should be selected\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(random_state=42), n_features_to_select=5)\n",
    "\n",
    "# Fit RFE on the dataset\n",
    "# This will eliminate irrelevant features and rank the remaining features\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Summarize the selected features and their ranks\n",
    "# 'rfe.support_' indicates which features are selected (True/False)\n",
    "# 'rfe.ranking_' shows the ranking of all features (1 means the most important feature)\n",
    "for i in range(X.shape[1]):\n",
    "    print(\"Column: %d, Selected=%s, Rank: %d\" % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    # Print the index of the feature, whether it's selected (True/False), and its ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Estimator Algorithm\n",
    "\n",
    "The RFE algorithm's flexibility allows integration with any predictive model that generates feature importance scores, though the specific selection results may vary across algorithms. While most decision-tree-based methods (e.g., Random Forests, Gradient Boosted Trees) typically yield similar feature importance trends, subtle differences can emerge due to their distinct splitting criteria and regularization approaches. To ensure robust feature selection, practitioners should empirically evaluate multiple algorithms through systematic comparison - for instance, testing both linear models (like Logistic Regression with coefficient magnitudes) and non-linear tree-based approaches within the RFE framework. The following example illustrates this exploratory process by wrapping different estimators in RFE and comparing their selected feature subsets, which may reveal algorithm-dependent biases or reinforce consensus features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">lr 0.882 (0.035)\n",
      ">per 0.854 (0.040)\n",
      ">dtc 0.879 (0.036)\n",
      ">rf 0.859 (0.034)\n",
      ">gbm 0.879 (0.036)\n"
     ]
    }
   ],
   "source": [
    "# Explore the algorithm wrapped by RFE\n",
    "\n",
    "# Function to generate a dataset for classification\n",
    "def get_dataset():\n",
    "    # Create a synthetic classification dataset with 1000 samples, 10 features, 5 informative and 5 redundant features\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1\n",
    "    )\n",
    "    # Return the features (X) and labels (y)\n",
    "    return X, y  \n",
    "\n",
    "# Function to create a dictionary of models wrapped with RFE (Recursive Feature Elimination)\n",
    "def get_models():\n",
    "    # Initialize an empty dictionary to store models\n",
    "    models = {}  \n",
    "\n",
    "    # Logistic Regression (lr)\n",
    "    # Initialize Recursive Feature Elimination (RFE)\n",
    "    # - estimator: The model used to evaluate feature importance (LogisticRegression in this case).\n",
    "    # - n_features_to_select: The number of features to retain (5 in this case).\n",
    "    rfe = RFE(estimator=LogisticRegression(), n_features_to_select=5)\n",
    "    \n",
    "    # Initialize a DecisionTreeClassifier model\n",
    "    # - This model will be used for classification after feature selection.\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    \n",
    "    # Create a pipeline and store it in a dictionary\n",
    "    # - steps: A list of tuples defining the sequence of operations:\n",
    "    #   - \"s\": RFE for feature selection.\n",
    "    #   - \"m\": DecisionTreeClassifier for classification.\n",
    "    # - The pipeline is stored in the dictionary `models` with the key \"lr\".\n",
    "    models[\"lr\"] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "    # Perceptron (per)\n",
    "    # Initialize Recursive Feature Elimination (RFE)\n",
    "    # - estimator: The model used to evaluate feature importance (Perceptron in this case).\n",
    "    # - n_features_to_select: The number of features to retain (5 in this case).\n",
    "    rfe = RFE(estimator=Perceptron(), n_features_to_select=5)\n",
    "\n",
    "    # Initialize a DecisionTreeClassifier model\n",
    "    # - This model will be used for classification after feature selection.\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    # Create a pipeline and store it in a dictionary\n",
    "    # - steps: A list of tuples defining the sequence of operations:\n",
    "    #   - \"s\": RFE for feature selection.\n",
    "    #   - \"m\": DecisionTreeClassifier for classification.\n",
    "    # - The pipeline is stored in the dictionary `models` with the key \"per\".\n",
    "    models[\"per\"] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "    # Decision Tree Classifier (dtc)\n",
    "    # Initialize Recursive Feature Elimination (RFE)\n",
    "    # - estimator: The model used to evaluate feature importance (DecisionTreeClassifier in this case).\n",
    "    # - n_features_to_select: The number of features to retain (5 in this case).\n",
    "    rfe = RFE(estimator=DecisionTreeClassifier(random_state=42), n_features_to_select=5)\n",
    "\n",
    "    # Initialize a DecisionTreeClassifier model\n",
    "    # - This model will be used for classification after feature selection.\n",
    "    model = DecisionTreeClassifier(random_state=42)  \n",
    "\n",
    "    # Create a pipeline and store it in a dictionary\n",
    "    # - steps: A list of tuples defining the sequence of operations:\n",
    "    #   - \"s\": RFE for feature selection.\n",
    "    #   - \"m\": DecisionTreeClassifier for classification.\n",
    "    # - The pipeline is stored in the dictionary `models` with the key \"dtc\".\n",
    "    models[\"dtc\"] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "    # Random Forest Classifier (rf)\n",
    "    # Initialize Recursive Feature Elimination (RFE)\n",
    "    # - estimator: The model used to evaluate feature importance (RandomForestClassifier in this case).\n",
    "    # - n_features_to_select: The number of features to retain (5 in this case).\n",
    "    rfe = RFE(estimator=RandomForestClassifier(random_state=42), n_features_to_select=5)\n",
    "\n",
    "    # Initialize a DecisionTreeClassifier model\n",
    "    # - This model will be used for classification after feature selection.\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    # Create a pipeline and store it in a dictionary\n",
    "    # - steps: A list of tuples defining the sequence of operations:\n",
    "    #   - \"s\": RFE for feature selection.\n",
    "    #   - \"m\": DecisionTreeClassifier for classification.\n",
    "    # - The pipeline is stored in the dictionary `models` with the key \"rf\".\n",
    "    models[\"rf\"] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "    # Gradient Boosting Classifier (gbm)\n",
    "    # Initialize Recursive Feature Elimination (RFE)\n",
    "    # - estimator: The model used to evaluate feature importance (GradientBoostingClassifier in this case).\n",
    "    # - n_features_to_select: The number of features to retain (5 in this case).\n",
    "    rfe = RFE(estimator=GradientBoostingClassifier(random_state=42), n_features_to_select=5)\n",
    "\n",
    "    # Initialize a DecisionTreeClassifier model\n",
    "    # - This model will be used for classification after feature selection.\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    # Create a pipeline and store it in a dictionary\n",
    "    # - steps: A list of tuples defining the sequence of operations:\n",
    "    #   - \"s\": RFE for feature selection.\n",
    "    #   - \"m\": DecisionTreeClassifier for classification.\n",
    "    # - The pipeline is stored in the dictionary `models` with the key \"gbm\".\n",
    "    models[\"gbm\"] = Pipeline(steps=[(\"s\", rfe), (\"m\", model)])\n",
    "\n",
    "    # Return the dictionary containing models with RFE\n",
    "    return models  \n",
    "\n",
    "\n",
    "# Function to evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # Define cross-validation strategy: StratifiedKFold with 10 splits and 3 repeats\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation and calculate accuracy scores\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "    # Return the accuracy scores\n",
    "    return scores  \n",
    "\n",
    "\n",
    "# Define the dataset by calling the function get_dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# Get the models to evaluate by calling get_models\n",
    "models = get_models()\n",
    "\n",
    "# Initialize lists to store results and model names\n",
    "results, names = [], []\n",
    "\n",
    "# Loop through each model in the models dictionary\n",
    "for name, model in models.items():\n",
    "    # Evaluate each model using cross-validation and store the results\n",
    "    scores = evaluate_model(model, X, y)\n",
    "\n",
    "    # Append the scores for this model\n",
    "    results.append(scores)\n",
    "\n",
    "    # Append the model name\n",
    "    names.append(name)\n",
    "    \n",
    "    # Print the model name, mean accuracy, and standard deviation of the accuracy scores\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))  # Print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance metrics reveal that logistic regression (LR) achieved the highest mean accuracy of 0.882 (±0.035), followed closely by gradient boosting machine (GBM) at 0.879 (±0.036) and decision tree classifier (DTC) at 0.879 (±0.036). While all algorithms demonstrated strong performance, the superior and more consistent results from linear logistic regression (compared to the tree-based methods: random forest at 0.859±0.034 and perceptron at 0.854±0.040) suggest that its feature selection capabilities may be more reliable for this particular dataset. The narrow standard deviations across all models indicate stable performance, but logistic regression's combination of highest mean accuracy and competitive variability makes it particularly noteworthy. These results imply that linear methods may be better suited for feature selection tasks when working with data of this nature, though the strong showing from gradient boosting warrants further investigation into ensemble approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing the necessary plotting module (assuming it's already imported)\n",
    "\n",
    "# Plotting a boxplot to compare the performance of different models\n",
    "# 'results' contains the performance data (e.g., accuracy scores, etc.) for each model\n",
    "# 'names' is a list of the model names corresponding to the performance results\n",
    "# The 'boxplot' function is used to generate a boxplot to compare the performance visually\n",
    "pyplot.boxplot(results, showmeans=True)  # Create the boxplot and display means on each box\n",
    "\n",
    "# Add x-axis and y-axis labels\n",
    "pyplot.xlabel(\"Name of Model\")\n",
    "pyplot.ylabel(\"Accuracy\")\n",
    "\n",
    "# Set the x-axis labels using 'xticklabels' (for model names corresponding to each boxplot)\n",
    "pyplot.xticks(ticks=range(1, len(names) + 1), labels=names)  # Set model names as x-axis labels\n",
    "\n",
    "# Display the plot to the user\n",
    "pyplot.show()  # Show the generated plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This box plot compares the accuracy of different machine learning models, with the x-axis representing the names of the models and the y-axis showing their corresponding mean accuracy scores. The models being compared include Logistic Regression (lr), Perceptron (per), Decision Tree Classifier (dtc), Random Forest (rf), and Gradient Boosting Machine (gbm). Each box plot captures the distribution of mean accuracy values across multiple runs of the models, providing insights into their performance stability and effectiveness.\n",
    "\n",
    "The central orange line inside each box represents the median accuracy, while the green triangle indicates the mean accuracy. The boxes themselves depict the interquartile range (IQR), which encompasses the middle 50% of the accuracy values. The whiskers extend to the minimum and maximum values within a certain range, excluding any outliers, which appear as small circles beyond the whiskers.\n",
    "\n",
    "Logistic Regression and Gradient Boosting Machine demonstrate the highest median and mean accuracy among all the models, suggesting they consistently perform well. In contrast, the Perceptron model has the lowest median accuracy and exhibits a noticeable outlier, indicating that its performance is less stable compared to the others. The Decision Tree Classifier and Random Forest models show similar median accuracies, but Random Forest appears to be slightly more consistent with a more compact distribution.\n",
    "\n",
    "Another important observation is the variability in model performance. Logistic Regression and Gradient Boosting have higher upper whiskers, indicating that these models have the potential to achieve strong accuracy scores in some cases. The Perceptron model, on the other hand, has a wider spread and a lower outlier, reinforcing its inconsistency. Random Forest appears to have a relatively stable performance with fewer extreme values, making it a reliable option.\n",
    "\n",
    "In conclusion, Logistic Regression and Gradient Boosting seem to be the most effective models in terms of both median and mean accuracy. Perceptron performs the weakest, showing lower accuracy and higher variation across runs. If stability and high performance are the main objectives, Gradient Boosting or Random Forest would likely be the best choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this tutorial, we learned how to effectively use Recursive Feature Elimination for feature selection in both classification and regression problems. We explored key hyperparameters like the number of features to select and the choice of wrapped algorithm. The techniques covered demonstrate how RFE can be used to identify the most relevant features for predictive modeling tasks.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
