{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Numerical Data Distributions \n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Numerical input variables may have a highly skewed or non-standard distribution. This could be\n",
    "caused by outliers in the data, multi-modal distributions, highly exponential distributions, and\n",
    "more. Many machine learning algorithms prefer or perform better when numerical input variables\n",
    "and even output variables in the case of regression have a standard probability distribution,\n",
    "such as a Gaussian (normal) or a Uniform distribution.\n",
    "\n",
    "The quantile transform provides an automatic way to transform a numeric input variable to\n",
    "have a different data distribution, which in turn, can be used as input to a predictive model.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn why machine learning algorithms prefer numerical variables with Gaussian or standard probability distributions\n",
    "- Understand quantile transforms for transforming numerical variables to Gaussian or Uniform distributions \n",
    "- Learn how to use `QuantileTransformer` to change probability distributions of numeric variables to improve model performance\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Apply normal quantile transform\n",
    "- Apply uniform quantile transform  \n",
    "- Compare model performance with different transforms\n",
    "- Explore impact of quantile parameters\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python programming\n",
    "- Familiarity with NumPy libraries\n",
    "- Knowledge of basic statistical concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages, import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line uses the pip package installer to install the specified Python libraries if they are not already installed.\n",
    "# matplotlib: A comprehensive library for creating static, interactive, and animated visualizations in Python.\n",
    "# numpy: NumPy is the fundamental package for scientific computing in Python. It is used for working with arrays.\n",
    "# pandas: pandas is a powerful data manipulation and analysis library. It provides data structures like DataFrames.\n",
    "# scikit-learn: scikit-learn is a machine learning library that provides tools for classification, regression, clustering, dimensionality reduction, model selection, and preprocessing.\n",
    "%pip install matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyplot module from matplotlib library and alias it as plt for plotting functionalities.\n",
    "import matplotlib.pyplot as plt\n",
    "# Import specific functions (exp, mean, std) from the numpy library for numerical operations.\n",
    "from numpy import exp, mean, std\n",
    "# Import the randn function from numpy.random module for generating random numbers from a standard normal distribution.\n",
    "from numpy.random import randn\n",
    "# Import DataFrame and read_csv functions from the pandas library for data manipulation and reading CSV files.\n",
    "from pandas import DataFrame, read_csv\n",
    "# Import RepeatedStratifiedKFold and cross_val_score from sklearn.model_selection for model evaluation using cross-validation.\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "# Import KNeighborsClassifier from sklearn.neighbors for using the K-Nearest Neighbors classification algorithm.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Import Pipeline from sklearn.pipeline for creating a pipeline to chain preprocessing and model steps.\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Import LabelEncoder and QuantileTransformer from sklearn.preprocessing for data preprocessing tasks.\n",
    "from sklearn.preprocessing import LabelEncoder, QuantileTransformer\n",
    "\n",
    "# Define a variable 'pima_indians_diabetes_csv' that stores the file path to the Pima Indians Diabetes dataset CSV file.\n",
    "pima_indians_diabetes_csv = \"../../Data/pima-indians-diabetes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Quantile Transforms\n",
    "\n",
    "A quantile transform will map a variable's probability distribution to another probability\n",
    "distribution. The quantile function ranks or smooths out the relationship between observations and can be\n",
    "mapped onto other distributions, such as the uniform or normal distribution. The transformation\n",
    "can be applied to each numeric input variable in the training dataset and then provided as\n",
    "input to a machine learning model to learn a predictive modeling task. This quantile transform\n",
    "is available in the scikit-learn Python machine learning library via the **QuantileTransformer**\n",
    "class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first creates a sample of 1,000 random Gaussian values and adds a\n",
    "skew to the dataset. A histogram is created from the skewed dataset and clearly shows the\n",
    "distribution pushed to the far left. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of the quantile transform\n",
    "\n",
    "# generate gaussian data sample\n",
    "# Generate 1000 random numbers from a standard normal distribution (Gaussian distribution) using randn function.\n",
    "data = randn(1000)\n",
    "\n",
    "# add a skew to the data distribution\n",
    "# Apply the exponential function to each data point to introduce a right skew in the distribution.\n",
    "# Exponential transformation makes positive values larger and compresses negative values towards zero, creating skewness.\n",
    "data = exp(data)\n",
    "\n",
    "# histogram of the raw data with a skew\n",
    "# Create a histogram plot of the transformed 'data' to visualize its distribution.\n",
    "# 'bins=25' specifies that the histogram should have 25 bins.\n",
    "plt.hist(data, bins=25)\n",
    "# Display the histogram plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a QuantileTransformer is used to map the data to a Gaussian distribution and standardize the result, centering the values on the mean value of 0 and a standard deviation of\n",
    "1.0. A histogram of the transform data is created showing a Gaussian shaped data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the 'data' array to have rows and a single column.\n",
    "# This is often necessary for scikit-learn transformers that expect 2D input.\n",
    "data = data.reshape((len(data), 1))\n",
    "# Print the shape of the reshaped 'data' array.\n",
    "# This will output the new dimensions, confirming it's now a 2D array with one column.\n",
    "print(data.shape)\n",
    "\n",
    "# Initialize a QuantileTransformer object with the output distribution set to 'normal'.\n",
    "# 'output_distribution=\"normal\"' specifies that the transformed data should follow a normal (Gaussian) distribution as closely as possible.\n",
    "quantile = QuantileTransformer(output_distribution=\"normal\")\n",
    "# Fit the QuantileTransformer to the 'data' and then transform 'data'.\n",
    "# 'fit_transform' learns the quantile information from 'data' and applies the normal quantile transformation.\n",
    "data_trans = quantile.fit_transform(data)\n",
    "\n",
    "# Create a histogram of the transformed data 'data_trans'.\n",
    "# plt.hist(data_trans, bins=25) generates a histogram with 25 bins to visualize the distribution of the transformed data.\n",
    "plt.hist(data_trans, bins=25)\n",
    "# Display the histogram plot.\n",
    "# plt.show() command is used to open a window and display the generated plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient data as\n",
    "either an onset of diabetes within five years or not. \n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class \n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the variables from the pima-indians-diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the dataset\n",
    "\n",
    "# load dataset from a CSV file named 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' argument indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "# Print the first few rows of the DataFrame to get a glimpse of the data.\n",
    "# By default, head() displays the first 5 rows.\n",
    "print(dataset.head())\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "# Print the shape of the DataFrame, which includes the number of rows and columns.\n",
    "print(dataset.shape)\n",
    "\n",
    "# summarize each variable (column) in the dataset\n",
    "# Print descriptive statistics for each column in the DataFrame.\n",
    "# This includes count, mean, std, min, 25%, 50%, 75%, max for numerical columns.\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the 8\n",
    "input variables, one output variable, and 768 rows of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a histogram is created for each input variable. If we ignore the clutter of the plots and\n",
    "focus on the histograms themselves, we can see that many variables have a skewed distribution.\n",
    "The dataset provides a good candidate for using a quantile transform to make the variables\n",
    "more-Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for each variable in the 'dataset' DataFrame.\n",
    "# 'dataset.hist()' is a pandas function that generates histograms for all numerical columns in the DataFrame.\n",
    "# 'xlabelsize=4' and 'ylabelsize=4' arguments set the font size of the x and y axis labels to 4 points, making them smaller.\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "\n",
    "# Iterate over each subplot (histogram) in the figure returned by 'dataset.hist()'.\n",
    "# 'fig' is a NumPy array of matplotlib AxesSubplot objects, where each subplot is a histogram for a column.\n",
    "# 'fig.ravel()' flattens this array into a 1D array for easy iteration.\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "# For each subplot 'x', this line sets the title font size to 4 points, making the titles smaller.\n",
    "# 'x.title' accesses the title object of the subplot.\n",
    "# 'set_size(4)' sets the font size of the title.\n",
    "\n",
    "# Display the plot.\n",
    "# 'plt.show()' is a matplotlib function that opens a window and displays all generated figures.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's first and evaluate a machine learning model on the raw dataset. We will use\n",
    "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
    "stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate KNN classifier on the raw dataset\n",
    "\n",
    "# KFold is a cross-validator that divides the dataset into k folds.\n",
    "# Stratified is to ensure that each fold of dataset has the same proportion of\n",
    "# observations with a given label.\n",
    "# Repeated provides a way to improve the estimated performance of a machine\n",
    "# learning model.\n",
    "# This involves simply repeating the cross-validation procedure multiple times\n",
    "# and reporting the mean result across all folds from all runs. This mean result\n",
    "# is expected to be a more accurate estimate of the true unknown underlying mean\n",
    "# performance of the model on the dataset, as calculated using the standard\n",
    "# error.\n",
    "\n",
    "# load dataset\n",
    "# Load the dataset from the 'pima_indians_diabetes.csv' file using pandas' read_csv function.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "# Extract the values from the pandas DataFrame into a NumPy array.\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "# Separate the dataset into input features (X) and output labels (y).\n",
    "# X is assigned all columns except the last one ([:-1]).\n",
    "# y is assigned the last column ([-1]), which is assumed to be the target variable.\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "# Ensure that the input features (X) are of float32 data type.\n",
    "X = X.astype(\"float32\")\n",
    "# Encode the output labels (y) into integer labels using LabelEncoder.\n",
    "# fit_transform fits the LabelEncoder to y and then transforms y to numerical labels.\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define and configure the model using\n",
    "# Classifier implementing the k-nearest neighbors vote.\n",
    "# Define the KNN classifier model.\n",
    "# KNeighborsClassifier() creates a KNN classifier object with default parameters.\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# evaluate the model using RepeatedStratifiedKFold cross validator,\n",
    "# that repeats Stratified K-Fold n times with different randomization in each\n",
    "# repetition.\n",
    "# Define the cross-validation strategy using RepeatedStratifiedKFold.\n",
    "# n_splits=10:  Divides the dataset into 10 folds.\n",
    "# n_repeats=3: Repeats the cross-validation process 3 times.\n",
    "# random_state=1: Sets the random seed for reproducibility.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# Evaluate the model using cross-validation.\n",
    "# model: The KNN classifier model to evaluate.\n",
    "# X: The input features.\n",
    "# y: The output labels.\n",
    "# scoring='accuracy': The metric used to evaluate the model (accuracy in this case).\n",
    "# cv: The cross-validation strategy defined above (RepeatedStratifiedKFold).\n",
    "# n_jobs=-1: Use all available CPU cores for parallel execution.\n",
    "n_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "# Print the model's performance.\n",
    "# mean(n_scores): Calculates the mean accuracy across all folds and repeats.\n",
    "# std(n_scores): Calculates the standard deviation of the accuracy scores.\n",
    "# \"Accuracy: %.3f (%.3f)\": Formats the output string to display the mean and standard deviation rounded to 3 decimal places.\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the model achieved a mean classification accuracy of about 71.7\n",
    "percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Quantile Transform\n",
    "\n",
    "It is often desirable to transform an input variable to have a normal probability distribution to improve the modeling performance. We can apply the Quantile transform using the\n",
    "QuantileTransformer class and set the output distribution argument to `normal'. We\n",
    "must also set the n quantiles argument to a value less than the number of observations in the\n",
    "training dataset, in this case, 100. Once defined, we can call the fit transform() function and\n",
    "pass it to our dataset to create a quantile transformed version of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a normal quantile transform of the dataset\n",
    "\n",
    "# load dataset\n",
    "# Load the Pima Indians Diabetes dataset from the CSV file 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# retrieve just the numeric input values\n",
    "# Extract the numpy array of values from the DataFrame and select all rows (:) and all columns except the last one (:-1).\n",
    "# This assumes the last column is the target variable and the preceding columns are input features.\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a normal quantile transform of the dataset 'n_quantiles\" is the number\n",
    "# of quantiles to be computed. It corresponds to the number of landmarks used to\n",
    "# discretize the cumulative distribution function.  'output_distribution\" is the\n",
    "# marginal distribution for the transformed data. The choices are 'uniform'\n",
    "# (default) or 'normal'.\n",
    "# Initialize a QuantileTransformer object.\n",
    "# 'n_quantiles=100' sets the number of quantiles to 100, controlling the granularity of the transformation.\n",
    "# 'output_distribution=\"normal\"' specifies that the transformed data should follow a normal distribution.\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"normal\")\n",
    "# Fit the QuantileTransformer to the input data and then transform the data.\n",
    "# 'fit_transform' learns the quantile mapping from the data and applies the transformation.\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "# Convert the transformed numpy array 'data' back into a pandas DataFrame.\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "# Create histograms for each variable (column) in the DataFrame.\n",
    "# 'xlabelsize=4' and 'ylabelsize=4' set the font size of the x and y axis labels to 4 for better readability in the plot.\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "# Iterate over each histogram subplot in the figure and set the title font size to 4.\n",
    "# 'fig.ravel()' flattens the array of subplots into a 1D array.\n",
    "# 'x.title.set_size(4)' sets the font size of the title for each subplot.\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "# Display the generated histograms plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the shape of the histograms for each variable looks very Gaussian as compared\n",
    "to the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
    "normal quantile transform of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate KNN with normal quantile transform\n",
    "\n",
    "# load dataset from the specified CSV file 'pima_indians_diabetes_csv' without a header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "# Extract the values from the pandas DataFrame into a NumPy array.\n",
    "data = dataset.values\n",
    "\n",
    "# separate the data into input features (X) and output target (y) columns.\n",
    "# X gets all columns except the last one ([:-1]).\n",
    "# y gets only the last column ([:, -1]).\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs (X) are of float32 data type for numerical operations.\n",
    "X = X.astype(\"float32\")\n",
    "# encode the output labels (y) into integer labels using LabelEncoder.\n",
    "# fit_transform fits the LabelEncoder to y and then transforms y to encoded labels.\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define the quantile transformer to transform features to a normal distribution.\n",
    "# n_quantiles=100 specifies the number of quantiles to use in the transformation.\n",
    "# output_distribution=\"normal\" specifies that the output should be a normal distribution.\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"normal\")\n",
    "\n",
    "# define the model, using KNeighborsClassifier which is a k-nearest neighbors classifier.\n",
    "model = KNeighborsClassifier()\n",
    "# create a pipeline that first applies the quantile transformation (trans) and then the KNN model (model).\n",
    "# steps=[(\"t\", trans), (\"m\", model)] defines the steps in the pipeline with names 't' for transformer and 'm' for model.\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# define cross-validation configuration using RepeatedStratifiedKFold.\n",
    "# n_splits=10 means 10 folds for cross-validation.\n",
    "# n_repeats=3 means repeat cross-validation 3 times.\n",
    "# random_state=1 ensures reproducibility of the cross-validation splits.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the pipeline using cross_validation.\n",
    "# pipeline is the defined pipeline to evaluate.\n",
    "# X is the input data, y is the target data.\n",
    "# scoring=\"accuracy\" specifies that accuracy is the metric to evaluate.\n",
    "# cv=cv uses the defined RepeatedStratifiedKFold for cross-validation.\n",
    "# n_jobs=-1 uses all available CPU cores for parallel processing to speed up cross-validation.\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report the pipeline performance.\n",
    "# mean(n_scores) calculates the average accuracy across all cross-validation folds and repeats.\n",
    "# std(n_scores) calculates the standard deviation of the accuracy scores.\n",
    "# \"Accuracy: %.3f (%.3f)\" is a format string to print the mean and standard deviation, rounded to 3 decimal places.\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the normal quantile transform results in a lift in\n",
    "performance from 71.7 percent accuracy without the transform to about 73.4 percent with the\n",
    "transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Quantile Transform\n",
    "\n",
    "Sometimes it can be beneficial to transform a highly exponential or multi-modal distribution to\n",
    "have a uniform distribution. This is especially useful for data with a large and sparse range of\n",
    "values, e.g. outliers that are common rather than rare. We can apply the transform by defining\n",
    "a QuantileTransformer class and setting the output distribution argument to `uniform'\n",
    "(the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a uniform quantile transform of the dataset\n",
    "\n",
    "# load dataset\n",
    "# Load the Pima Indians Diabetes dataset from a CSV file named 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' indicates that the CSV file has no header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# retrieve just the numeric input values\n",
    "# Extract all rows and all columns except the last one (which is assumed to be the target variable).\n",
    "# ':,' selects all rows, and ':-1' selects all columns up to, but not including, the last column.\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a uniform quantile transform of the dataset\n",
    "# Initialize a QuantileTransformer object.\n",
    "# 'n_quantiles=100' specifies that the data should be mapped to 100 quantiles.\n",
    "# 'output_distribution=\"uniform\"' sets the output distribution to be uniform, meaning values will be spread evenly between 0 and 1.\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"uniform\")\n",
    "# Fit the QuantileTransformer to the data and then transform the data.\n",
    "# 'fit_transform' learns the quantile mapping from the input data and applies the transformation.\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "# Convert the NumPy array 'data' back into a pandas DataFrame.\n",
    "# This is done to easily use pandas' plotting capabilities for histograms.\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "# Create histograms for each column (variable) in the DataFrame.\n",
    "# 'xlabelsize=4' and 'ylabelsize=4' set the font size of the x and y axis labels to 4.\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "# Reduce the size of the titles of each histogram subplot.\n",
    "# 'fig.ravel()' flattens the array of subplots returned by 'dataset.hist()'.\n",
    "# '[x.title.set_size(4) for x in fig.ravel()]' iterates through each subplot and sets the title size to 4.\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "# Display the generated histograms plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the shape of the histograms for each variable looks very uniform compared to\n",
    "the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
    "uniform quantile transform of the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate KNN classifier on the dataset with uniform quantile transform\n",
    "\n",
    "# load dataset\n",
    "# Load the Pima Indians Diabetes dataset from a CSV file named 'pima_indians_diabetes_csv' using pandas.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "# Extract the values from the pandas DataFrame and store them in a NumPy array called 'data'.\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "# Separate the dataset into input features (X) and output labels (y).\n",
    "# X is assigned all columns except the last one ([:-1]).\n",
    "# y is assigned the last column ([-1]).\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "# Ensure that the input features (X) are of float32 data type.\n",
    "X = X.astype(\"float32\")\n",
    "# Encode the output labels (y) into integer labels using LabelEncoder.\n",
    "# 'astype(\"str\")' is used to handle potential mixed data types in the label column before encoding.\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define the pipeline\n",
    "# Define a QuantileTransformer for preprocessing.\n",
    "# 'n_quantiles=100' specifies the number of quantiles to use for transformation.\n",
    "# 'output_distribution=\"uniform\"' sets the output distribution to be uniform after transformation.\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"uniform\")\n",
    "# Define a KNeighborsClassifier model.\n",
    "model = KNeighborsClassifier()\n",
    "# Create a pipeline that first applies the QuantileTransformer (trans) and then the KNeighborsClassifier (model).\n",
    "# The steps are named 't' for transformer and 'm' for model.\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# evaluate the pipeline\n",
    "# Define a RepeatedStratifiedKFold cross-validation strategy.\n",
    "# 'n_splits=10' specifies 10 folds for each cross-validation iteration.\n",
    "# 'n_repeats=3' specifies to repeat the cross-validation 3 times.\n",
    "# 'random_state=1' ensures reproducibility of the shuffling in cross-validation.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# Evaluate the pipeline using cross-validation.\n",
    "# 'pipeline' is the estimator to evaluate.\n",
    "# 'X' is the input data.\n",
    "# 'y' is the target data.\n",
    "# 'scoring=\"accuracy\"' specifies that accuracy is the metric to evaluate.\n",
    "# 'cv=cv' uses the RepeatedStratifiedKFold cross-validation strategy defined earlier.\n",
    "# 'n_jobs=-1' uses all available CPU cores for parallel processing.\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report pipeline performance\n",
    "# Report the performance of the pipeline.\n",
    "# 'mean(n_scores)' calculates the average accuracy across all cross-validation folds and repeats.\n",
    "# 'std(n_scores)' calculates the standard deviation of the accuracy scores.\n",
    "# \"Accuracy: %.3f (%.3f)\" is a format string to print the mean and standard deviation, rounded to 3 decimal places.\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the uniform transform results in a lift in performance\n",
    "from 71.7 percent accuracy without the transform to about 73.4 percent with the normal transform, and achieved a score of 74.1 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the number of quantiles as an arbitrary number, in this case, 100. This hyperparameter can be tuned to explore the effect of the resolution of the transform on the resulting\n",
    "skill of the model. The example below performs this experiment and plots the mean accuracy\n",
    "for different n quantiles values from 1 to 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore number of quantiles on classification accuracy\n",
    "\n",
    "# Define a function named 'get_dataset' that takes a 'filename' as input.\n",
    "def get_dataset(filename):\n",
    "    # Load the dataset from a CSV file specified by 'filename' into a pandas DataFrame.\n",
    "    # 'header=None' argument indicates that the CSV file does not have a header row.\n",
    "    dataset = read_csv(filename, header=None)\n",
    "    \n",
    "    # Convert the pandas DataFrame 'dataset' into a NumPy array 'data'.\n",
    "    data = dataset.values\n",
    "    \n",
    "    # Split the 'data' array into input features (X) and output labels (y).\n",
    "    # '[:, :-1]' selects all rows and all columns except the last one for input features (X).\n",
    "    # '[:, -1]' selects all rows and only the last column for output labels (y).\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    \n",
    "    # Convert the input features 'X' to float32 data type.\n",
    "    X = X.astype(\"float32\")\n",
    "    \n",
    "    # Encode the output labels 'y' using LabelEncoder to convert string labels to numerical labels.\n",
    "    y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "    \n",
    "    # Return the processed input features 'X' and output labels 'y'.\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Define a function named 'get_models' to create and return a dictionary of machine learning models (pipelines).\n",
    "def get_models():\n",
    "    # Initialize an empty dictionary called 'models' to store the pipelines, where keys will be the number of quantiles and values will be the pipelines themselves.\n",
    "    models = dict()\n",
    "    \n",
    "    # Loop through numbers from 1 to 99 (inclusive), where each number 'i' represents the number of quantiles for the QuantileTransformer.\n",
    "    for i in range(1, 100):\n",
    "        # define the pipeline\n",
    "        # Create a QuantileTransformer object named 'trans' to transform the input features using quantiles.\n",
    "        trans = QuantileTransformer(n_quantiles=i, output_distribution=\"uniform\")\n",
    "        \n",
    "        # Create a KNeighborsClassifier object named 'model' which will be used as the classification model in the pipeline.\n",
    "        model = KNeighborsClassifier()\n",
    "        \n",
    "        # The purpose of the pipeline is to assemble several steps that can be\n",
    "        # cross-validated together while setting different parameters.\n",
    "        # Create a Pipeline object that chains together the QuantileTransformer ('trans') and KNeighborsClassifier ('model').\n",
    "        # The pipeline is stored in the 'models' dictionary with the number of quantiles 'i' as the key (converted to a string).\n",
    "        models[str(i)] = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "        \n",
    "    # Return the 'models' dictionary containing all the created pipelines.\n",
    "    return models\n",
    "\n",
    "\n",
    "# Define a function named 'evaluate_model' that takes a machine learning 'model', input features 'X', and output labels 'y' as input.\n",
    "def evaluate_model(model, X, y):\n",
    "    # Create a RepeatedStratifiedKFold cross-validation object named 'cv'.\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    \n",
    "    # Perform cross-validation using 'cross_val_score' to evaluate the 'model'.\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # Return the list of accuracy scores obtained from cross-validation.\n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "# Call the get_dataset function to load and prepare the dataset, and unpack the features (X) and labels (y)\n",
    "X, y = get_dataset(pima_indians_diabetes_csv)\n",
    "\n",
    "# get the models to evaluate\n",
    "# Call the function `get_models()` to retrieve a dictionary of machine learning models that will be evaluated.\n",
    "models = get_models()\n",
    "\n",
    "# Initialize an empty list called 'results' to store the mean accuracy scores for each model.\n",
    "results = list()\n",
    "# Start a loop that iterates through the 'models' dictionary. 'models.items()' returns key-value pairs (name, model).\n",
    "for name, model in models.items():\n",
    "    # Evaluate the current 'model' using the 'evaluate_model' function with the dataset (X, y).\n",
    "    # The 'evaluate_model' function performs cross-validation and returns a list of accuracy scores.\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    \n",
    "    # Calculate the mean of the 'scores' list (which represents the average accuracy from cross-validation) and append it to the 'results' list.\n",
    "    results.append(mean(scores))\n",
    "    \n",
    "    # Print the performance of the current model.\n",
    "    # '>%s %.3f (%.3f)' is a format string:\n",
    "    #   - '%s' will be replaced by the 'name' of the model (which is the number of quantiles in this case).\n",
    "    #   - '%.3f' will be replaced by the mean accuracy, formatted to 3 decimal places.\n",
    "    #   - '%.3f' will be replaced by the standard deviation of the accuracy scores, formatted to 3 decimal places.\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that surprisingly smaller values resulted in better accuracy, with\n",
    "values such as 9 achieving an accuracy of about 74.7 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance for comparison\n",
    "# Plot the 'results' data. Assuming 'results' is a list or array of performance metrics (e.g., accuracy scores) from different models or configurations.\n",
    "plt.plot(results)\n",
    "# Display the plot created by plt.plot(). This will open a window showing the graph.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line plot is created showing the number of quantiles used in the transform versus the\n",
    "classification accuracy of the resulting model. We can see a bump with values less than 10 and\n",
    "drop and \n",
    "at performance after that. The results highlight that there is likely some benefit in\n",
    "exploring different distributions and number of quantiles to see if better performance can be\n",
    "achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored the transformation of numerical input variables that exhibit skewed or non-standard distributions due to factors like outliers, multi-modal patterns, or exponential distributions. Then we explored the performance of machine learning algorithms both with and without transformation. \n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
