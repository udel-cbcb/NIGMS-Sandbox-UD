{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Numerical Data Distributions \n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Numerical input variables often exhibit **highly skewed** or **non-standard distributions**, which can arise due to outliers, multi-modal distributions, exponential patterns, or other factors. Many machine learning algorithms perform better when numerical input variables (and output variables in regression tasks) follow a **standard probability distribution**, such as Gaussian (normal) or Uniform.\n",
    "\n",
    "The **quantile transform** offers an automated approach to reshape the distribution of numerical input variables. By transforming the data into a desired distribution (e.g., Gaussian or Uniform), it can improve the suitability of the data for use as input to predictive models.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn why machine learning algorithms prefer numerical variables with Gaussian or standard probability distributions\n",
    "- Understand quantile transforms for transforming numerical variables to Gaussian or Uniform distributions \n",
    "- Learn how to use `QuantileTransformer` to change probability distributions of numeric variables to improve model performance\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Apply normal quantile transform\n",
    "- Apply uniform quantile transform  \n",
    "- Compare model performance with different transforms\n",
    "- Explore impact of quantile parameters\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python programming\n",
    "- Familiarity with NumPy libraries\n",
    "- Knowledge of basic statistical concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages, import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line uses the pip package installer to install the specified Python libraries if they are not already installed.\n",
    "# matplotlib: A comprehensive library for creating static, interactive, and animated visualizations in Python.\n",
    "# numpy: NumPy is the fundamental package for scientific computing in Python. It is used for working with arrays.\n",
    "# pandas: pandas is a powerful data manipulation and analysis library. It provides data structures like DataFrames.\n",
    "# scikit-learn: scikit-learn is a machine learning library that provides tools for classification, regression, clustering, dimensionality reduction, model selection, and preprocessing.\n",
    "%pip install matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyplot module from matplotlib library and alias it as plt for plotting functionalities.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import specific functions (exp, mean, std) from the numpy library for numerical operations.\n",
    "from numpy import exp, mean, std\n",
    "\n",
    "# Import the randn function from numpy.random module for generating random numbers from a standard normal distribution.\n",
    "from numpy.random import randn\n",
    "\n",
    "# Import DataFrame and read_csv functions from the pandas library for data manipulation and reading CSV files.\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "# Import RepeatedStratifiedKFold and cross_val_score from sklearn.model_selection for model evaluation using cross-validation.\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "\n",
    "# Import KNeighborsClassifier from sklearn.neighbors for using the K-Nearest Neighbors classification algorithm.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import Pipeline from sklearn.pipeline for creating a pipeline to chain preprocessing and model steps.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import LabelEncoder and QuantileTransformer from sklearn.preprocessing for data preprocessing tasks.\n",
    "from sklearn.preprocessing import LabelEncoder, QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Transforms\n",
    "\n",
    "### Overview\n",
    "A **quantile transform** is a non-linear transformation that maps a variable's probability distribution to a target distribution (typically uniform or Gaussian). This technique:\n",
    "- Preserves the rank-ordering of values\n",
    "- Reshapes the underlying distribution\n",
    "- Helps algorithms that assume normally distributed features\n",
    "\n",
    "### Key Characteristics\n",
    "- Applied independently to each numeric feature\n",
    "- Particularly effective for:\n",
    "  - Skewed distributions\n",
    "  - Outlier mitigation\n",
    "  - Non-normal data requiring Gaussian-like properties\n",
    "\n",
    "### Implementation\n",
    "```python\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Create transformer (output normal distribution by default)\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "# Fit and transform data\n",
    "X_transformed = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate a sample of 1,000 random values from a Gaussian (normal) distribution. We then intentionally introduce skewness to this dataset. When we plot the skewed data using a histogram, the visualization clearly reveals the distorted distribution, with most values clustered toward the left side of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of the quantile transform\n",
    "\n",
    "# Generate 1000 random numbers from a standard normal distribution (Gaussian distribution) using randn function.\n",
    "data = randn(1000)\n",
    "\n",
    "# Apply the exponential function to each data point to introduce a right skew in the distribution.\n",
    "# Exponential transformation makes positive values larger and compresses negative values towards zero, creating skewness.\n",
    "data = exp(data)\n",
    "\n",
    "# Create a histogram plot of the transformed 'data' to visualize its distribution.\n",
    "# 'bins=25' specifies that the histogram should have 25 bins.\n",
    "plt.hist(data, bins=25)\n",
    "\n",
    "# Display the histogram plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply a QuantileTransformer to redistribute the skewed data into a standard Gaussian (normal) distribution. This transformation process normalizes the values by centering them around zero and scaling to unit variance (mean=0, std=1). Visual examination of the transformed data's histogram confirms the successful conversion to a bell-shaped Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the 'data' array to have rows and a single column.\n",
    "# This is often necessary for scikit-learn transformers that expect 2D input.\n",
    "data = data.reshape((len(data), 1))\n",
    "\n",
    "# Print the shape of the reshaped 'data' array.\n",
    "# This will output the new dimensions, confirming it's now a 2D array with one column.\n",
    "print(data.shape)\n",
    "\n",
    "# Initialize a QuantileTransformer object with the output distribution set to 'normal'.\n",
    "# 'output_distribution=\"normal\"' specifies that the transformed data should follow a normal (Gaussian) distribution as closely as possible.\n",
    "quantile = QuantileTransformer(output_distribution=\"normal\")\n",
    "\n",
    "# Fit the QuantileTransformer to the 'data' and then transform 'data'.\n",
    "# 'fit_transform' learns the quantile information from 'data' and applies the normal quantile transformation.\n",
    "data_trans = quantile.fit_transform(data)\n",
    "\n",
    "# Create a histogram of the transformed data 'data_trans'.\n",
    "# plt.hist(data_trans, bins=25) generates a histogram with 25 bins to visualize the distribution of the transformed data.\n",
    "plt.hist(data_trans, bins=25)\n",
    "\n",
    "# Display the histogram plot.\n",
    "# plt.show() command is used to open a window and display the generated plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient data as\n",
    "either an onset of diabetes within five years or not. \n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class \n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the variables from the pima-indians-diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable 'pima_indians_diabetes_csv' that stores the file path to the Pima Indians Diabetes dataset CSV file.\n",
    "pima_indians_diabetes_csv = \"../../Data/pima-indians-diabetes.csv\"\n",
    "\n",
    "# load dataset from a CSV file defined by a variable named 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' argument indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Print the first few rows of the DataFrame to get a glimpse of the data.\n",
    "# By default, head() displays the first 5 rows.\n",
    "print(dataset.head())\n",
    "\n",
    "# Print the shape of the DataFrame, which includes the number of rows and columns.\n",
    "print(dataset.shape)\n",
    "\n",
    "# Print descriptive statistics for each column in the DataFrame.\n",
    "# This includes count, mean, std, min, 25%, 50%, 75%, max for numerical columns.\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comprises 768 observations with 8 predictive features and 1 target variable. A preliminary examination of variable distributions through histograms reveals significant skewness across multiple input features. This characteristic makes the data particularly suitable for quantile transformation, which can effectively reshape these variables to approximate a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for each variable in the 'dataset' DataFrame.\n",
    "# 'dataset.hist()' is a pandas function that generates histograms for all numerical columns in the DataFrame.\n",
    "# 'xlabelsize=4' and 'ylabelsize=4' arguments set the font size of the x and y axis labels to 4 points, making them smaller.\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "\n",
    "# Iterate over each subplot (histogram) in the figure returned by 'dataset.hist()'.\n",
    "# 'fig' is a NumPy array of matplotlib AxesSubplot objects, where each subplot is a histogram for a column.\n",
    "# 'fig.ravel()' flattens this array into a 1D array for easy iteration.\n",
    "# For each subplot 'x', this line sets the title font size to 4 points, making the titles smaller.\n",
    "# 'x.title' accesses the title object of the subplot.\n",
    "# 'set_size(4)' sets the font size of the title.\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# Display the plot.\n",
    "# 'plt.show()' is a matplotlib function that opens a window and displays all generated figures.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's **train and evaluate** a machine learning model on the raw dataset. We will use a **k-nearest neighbor (KNN) algorithm** with default hyperparameters and assess its performance using **repeated stratified k-fold cross-validation**. \n",
    "- KFold is a cross-validator that divides the dataset into k folds.\n",
    "- Stratified is to ensure that each fold of dataset has the same proportion of\n",
    " observations with a given label.\n",
    "- Repeated provides a way to improve the estimated performance of a machine learning model.\n",
    "\n",
    "This involves simply repeating the cross-validation procedure multiple times and reporting the mean result across all folds from all runs. This mean result is expected to be a more accurate estimate of the true unknown underlying mean performance of the model on the dataset, as calculated using the standard error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate KNN classifier on the raw dataset\n",
    "\n",
    "# Load the dataset from the 'pima_indians_diabetes.csv' file using pandas' read_csv function.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Extract the values from the pandas DataFrame into a NumPy array.\n",
    "data = dataset.values\n",
    "\n",
    "# Separate the dataset into input features (X) and output labels (y).\n",
    "# X is assigned all columns except the last one ([:-1]).\n",
    "# y is assigned the last column ([-1]), which is assumed to be the target variable.\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# Ensure that the input features (X) are of float32 data type.\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "# Encode the output labels (y) into integer labels using LabelEncoder.\n",
    "# fit_transform fits the LabelEncoder to y and then transforms y to numerical labels.\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# Define and configure the model using classifier implementing the k-nearest neighbors vote.\n",
    "# Define the KNN classifier model.\n",
    "# KNeighborsClassifier() creates a KNN classifier object with default parameters.\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Evaluate the model using RepeatedStratifiedKFold cross validator,\n",
    "# that repeats Stratified K-Fold n times with different randomization in each\n",
    "# repetition.\n",
    "# Define the cross-validation strategy using RepeatedStratifiedKFold.\n",
    "# n_splits=10:  Divides the dataset into 10 folds.\n",
    "# n_repeats=3: Repeats the cross-validation process 3 times.\n",
    "# random_state=1: Sets the random seed for reproducibility.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate the model using cross-validation.\n",
    "# model: The KNN classifier model to evaluate.\n",
    "# X: The input features.\n",
    "# y: The output labels.\n",
    "# scoring='accuracy': The metric used to evaluate the model (accuracy in this case).\n",
    "# cv: The cross-validation strategy defined above (RepeatedStratifiedKFold).\n",
    "# n_jobs=-1: Use all available CPU cores for parallel execution.\n",
    "n_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Print the model's performance.\n",
    "# mean(n_scores): Calculates the mean accuracy across all folds and repeats.\n",
    "# std(n_scores): Calculates the standard deviation of the accuracy scores.\n",
    "# \"Accuracy: %.3f (%.3f)\": Formats the output string to display the mean and standard deviation rounded to 3 decimal places.\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model demonstrated a mean classification accuracy of 71.7% across all test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Quantile Transform\n",
    "\n",
    "Transforming an input variable to have a **normal (Gaussian) probability distribution** can often improve modeling performance. To achieve this, we can apply the **Quantile Transform** using the `QuantileTransformer` class. \n",
    "\n",
    "When configuring the transformer:\n",
    "- Set the `output_distribution` argument to `'normal'` to ensure the data is mapped to a Gaussian distribution.\n",
    "- Set the `n_quantiles` argument to a value smaller than the number of observations in the training dataset (e.g., 100 in this case).\n",
    "\n",
    "Once configured, we can call the `fit_transform()` function and pass our dataset to it, creating a quantile-transformed version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a normal quantile transform of the dataset\n",
    "\n",
    "# Load the Pima Indians Diabetes dataset from the CSV file 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Extract the numpy array of values from the DataFrame and select all rows (:) and all columns except the last one (:-1).\n",
    "# This assumes the last column is the target variable and the preceding columns are input features.\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# Perform a normal quantile transform of the dataset 'n_quantiles\" is the number\n",
    "# of quantiles to be computed. It corresponds to the number of landmarks used to\n",
    "# discretize the cumulative distribution function.  'output_distribution\" is the\n",
    "# marginal distribution for the transformed data. The choices are 'uniform'\n",
    "# (default) or 'normal'.\n",
    "# Initialize a QuantileTransformer object.\n",
    "# 'n_quantiles=100' sets the number of quantiles to 100, controlling the granularity of the transformation.\n",
    "# 'output_distribution=\"normal\"' specifies that the transformed data should follow a normal distribution.\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"normal\")\n",
    "\n",
    "# Fit the QuantileTransformer to the input data and then transform the data.\n",
    "# 'fit_transform' learns the quantile mapping from the data and applies the transformation.\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# Convert the transformed numpy array 'data' back into a pandas DataFrame.\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# Create histograms for each variable (column) in the DataFrame.\n",
    "# 'xlabelsize=4' and 'ylabelsize=4' set the font size of the x and y axis labels to 4 for better readability in the plot.\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "\n",
    "# Iterate over each histogram subplot in the figure and set the title font size to 4.\n",
    "# 'fig.ravel()' flattens the array of subplots into a 1D array.\n",
    "# 'x.title.set_size(4)' sets the font size of the title for each subplot.\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "# Display the generated histograms plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the **histograms** for each variable now exhibit a **Gaussian-like shape**, a significant improvement compared to the raw data.\n",
    "\n",
    "Next, let's evaluate the same **k-nearest neighbor (KNN) model** as before, but this time using the **normal quantile-transformed version** of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate KNN with normal quantile transform\n",
    "\n",
    "# Load dataset from the specified CSV file 'pima_indians_diabetes_csv' without a header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Extract the values from the pandas DataFrame into a NumPy array.\n",
    "data = dataset.values\n",
    "\n",
    "# Separate the data into input features (X) and output target (y) columns.\n",
    "# X gets all columns except the last one ([:-1]).\n",
    "# y gets only the last column ([:, -1]).\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# Ensure inputs (X) are of float32 data type for numerical operations.\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "# Encode the output labels (y) into integer labels using LabelEncoder.\n",
    "# fit_transform fits the LabelEncoder to y and then transforms y to encoded labels.\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# Define the quantile transformer to transform features to a normal distribution.\n",
    "# n_quantiles=100 specifies the number of quantiles to use in the transformation.\n",
    "# output_distribution=\"normal\" specifies that the output should be a normal distribution.\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"normal\")\n",
    "\n",
    "# Define the model, using KNeighborsClassifier which is a k-nearest neighbors classifier.\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Create a pipeline that first applies the quantile transformation (trans) and then the KNN model (model).\n",
    "# steps=[(\"t\", trans), (\"m\", model)] defines the steps in the pipeline with names 't' for transformer and 'm' for model.\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# Define cross-validation configuration using RepeatedStratifiedKFold.\n",
    "# n_splits=10 means 10 folds for cross-validation.\n",
    "# n_repeats=3 means repeat cross-validation 3 times.\n",
    "# random_state=1 ensures reproducibility of the cross-validation splits.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate the pipeline using cross_validation.\n",
    "# pipeline is the defined pipeline to evaluate.\n",
    "# X is the input data, y is the target data.\n",
    "# scoring=\"accuracy\" specifies that accuracy is the metric to evaluate.\n",
    "# cv=cv uses the defined RepeatedStratifiedKFold for cross-validation.\n",
    "# n_jobs=-1 uses all available CPU cores for parallel processing to speed up cross-validation.\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report the pipeline performance.\n",
    "# mean(n_scores) calculates the average accuracy across all cross-validation folds and repeats.\n",
    "# std(n_scores) calculates the standard deviation of the accuracy scores.\n",
    "# \"Accuracy: %.3f (%.3f)\" is a format string to print the mean and standard deviation, rounded to 3 decimal places.\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that applying the **normal quantile transform** results in a **performance improvement**, increasing the accuracy from **71.7%** (without the transform) to approximately **73.4%** (with the transform)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Quantile Transform\n",
    "\n",
    "### When to Use\n",
    "This transformation is particularly valuable when working with:\n",
    "- Highly **exponential distributions** (e.g., power-law data)\n",
    "- **Multi-modal distributions** (data with multiple peaks)\n",
    "- Features with **sparse value ranges** and common outliers\n",
    "\n",
    "### Key Benefits\n",
    "- Normalizes the scale of extreme values without clipping\n",
    "- Preserves the relative ordering of observations\n",
    "- Handles common outliers more gracefully than Gaussian transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a uniform quantile transform of the dataset\n",
    "\n",
    "# Load the Pima Indians Diabetes dataset from a CSV file named 'pima_indians_diabetes_csv' into a pandas DataFrame.\n",
    "# 'header=None' indicates that the CSV file has no header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Extract all rows and all columns except the last one (which is assumed to be the target variable).\n",
    "# ':,' selects all rows, and ':-1' selects all columns up to, but not including, the last column.\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# Perform a uniform quantile transform of the dataset\n",
    "# Initialize a QuantileTransformer object.\n",
    "# 'n_quantiles=100' specifies that the data should be mapped to 100 quantiles.\n",
    "# 'output_distribution=\"uniform\"' sets the output distribution to be uniform, meaning values will be spread evenly between 0 and 1.\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"uniform\")\n",
    "\n",
    "# Fit the QuantileTransformer to the data and then transform the data.\n",
    "# 'fit_transform' learns the quantile mapping from the input data and applies the transformation.\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# Convert the NumPy array 'data' back into a pandas DataFrame.\n",
    "# This is done to easily use pandas' plotting capabilities for histograms.\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# Create histograms for each column (variable) in the DataFrame.\n",
    "# 'xlabelsize=4' and 'ylabelsize=4' set the font size of the x and y axis labels to 4.\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "\n",
    "# Reduce the size of the titles of each histogram subplot.\n",
    "# 'fig.ravel()' flattens the array of subplots returned by 'dataset.hist()'.\n",
    "# '[x.title.set_size(4) for x in fig.ravel()]' iterates through each subplot and sets the title size to 4.\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# Display the generated histograms plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable histograms now demonstrate a **perfect uniform distribution**, confirming the successful application of the quantile transform. This represents a substantial improvement over the original skewed distributions observed in the raw data.\n",
    "\n",
    "We will now assess model performance using:\n",
    "1. The **uniform-transformed dataset**\n",
    "2. The same **k-nearest neighbors (KNN) algorithm** as our baseline\n",
    "3. Identical evaluation metrics for direct comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate KNN classifier on the dataset with uniform quantile transform\n",
    "\n",
    "# Load the Pima Indians Diabetes dataset from a CSV file named 'pima_indians_diabetes_csv' using pandas.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# Extract the values from the pandas DataFrame and store them in a NumPy array called 'data'.\n",
    "data = dataset.values\n",
    "\n",
    "# Separate the dataset into input features (X) and output labels (y).\n",
    "# X is assigned all columns except the last one ([:-1]).\n",
    "# y is assigned the last column ([-1]).\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# Ensure that the input features (X) are of float32 data type.\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "# Encode the output labels (y) into integer labels using LabelEncoder.\n",
    "# 'astype(\"str\")' is used to handle potential mixed data types in the label column before encoding.\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# Define a QuantileTransformer for preprocessing.\n",
    "# 'n_quantiles=100' specifies the number of quantiles to use for transformation.\n",
    "# 'output_distribution=\"uniform\"' sets the output distribution to be uniform after transformation.\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"uniform\")\n",
    "\n",
    "# Define a KNeighborsClassifier model.\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Create a pipeline that first applies the QuantileTransformer (trans) and then the KNeighborsClassifier (model).\n",
    "# The steps are named 't' for transformer and 'm' for model.\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# Define a RepeatedStratifiedKFold cross-validation strategy.\n",
    "# 'n_splits=10' specifies 10 folds for each cross-validation iteration.\n",
    "# 'n_repeats=3' specifies to repeat the cross-validation 3 times.\n",
    "# 'random_state=1' ensures reproducibility of the shuffling in cross-validation.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate the pipeline using cross-validation.\n",
    "# 'pipeline' is the estimator to evaluate.\n",
    "# 'X' is the input data.\n",
    "# 'y' is the target data.\n",
    "# 'scoring=\"accuracy\"' specifies that accuracy is the metric to evaluate.\n",
    "# 'cv=cv' uses the RepeatedStratifiedKFold cross-validation strategy defined earlier.\n",
    "# 'n_jobs=-1' uses all available CPU cores for parallel processing.\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report the performance of the pipeline.\n",
    "# 'mean(n_scores)' calculates the average accuracy across all cross-validation folds and repeats.\n",
    "# 'std(n_scores)' calculates the standard deviation of the accuracy scores.\n",
    "# \"Accuracy: %.3f (%.3f)\" is a format string to print the mean and standard deviation, rounded to 3 decimal places.\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniform quantile transform demonstrates measurable performance gains:\n",
    "- **Baseline (no transform)**: 71.7% accuracy\n",
    "- **Normal quantile transform**: 73.4% (+1.7pp)\n",
    "- **Uniform quantile transform**: 74.1% (+2.4pp)\n",
    "\n",
    "The number of quantiles (set to **100** in this case) was chosen arbitrarily. This hyperparameter can be **tuned** to explore how the resolution of the transform affects model performance. The example below demonstrates this by evaluating the **mean accuracy** for different `n_quantiles` values ranging from **1 to 99** and plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore number of quantiles on classification accuracy\n",
    "\n",
    "# Define a function named 'get_dataset' that takes a 'filename' as input.\n",
    "def get_dataset(filename):\n",
    "    # Load the dataset from a CSV file specified by 'filename' into a pandas DataFrame.\n",
    "    # 'header=None' argument indicates that the CSV file does not have a header row.\n",
    "    dataset = read_csv(filename, header=None)\n",
    "    \n",
    "    # Convert the pandas DataFrame 'dataset' into a NumPy array 'data'.\n",
    "    data = dataset.values\n",
    "    \n",
    "    # Split the 'data' array into input features (X) and output labels (y).\n",
    "    # '[:, :-1]' selects all rows and all columns except the last one for input features (X).\n",
    "    # '[:, -1]' selects all rows and only the last column for output labels (y).\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    \n",
    "    # Convert the input features 'X' to float32 data type.\n",
    "    X = X.astype(\"float32\")\n",
    "    \n",
    "    # Encode the output labels 'y' using LabelEncoder to convert string labels to numerical labels.\n",
    "    y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "    \n",
    "    # Return the processed input features 'X' and output labels 'y'.\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Define a function named 'get_models' to create and return a dictionary of machine learning models (pipelines).\n",
    "def get_models():\n",
    "    # Initialize an empty dictionary called 'models' to store the pipelines, where keys will be the number of quantiles and values will be the pipelines themselves.\n",
    "    models = dict()\n",
    "    \n",
    "    # Loop through numbers from 1 to 99 (inclusive), where each number 'i' represents the number of quantiles for the QuantileTransformer.\n",
    "    for i in range(1, 100):\n",
    "        # define the pipeline\n",
    "        # Create a QuantileTransformer object named 'trans' to transform the input features using quantiles.\n",
    "        trans = QuantileTransformer(n_quantiles=i, output_distribution=\"uniform\")\n",
    "        \n",
    "        # Create a KNeighborsClassifier object named 'model' which will be used as the classification model in the pipeline.\n",
    "        model = KNeighborsClassifier()\n",
    "        \n",
    "        # The purpose of the pipeline is to assemble several steps that can be\n",
    "        # cross-validated together while setting different parameters.\n",
    "        # Create a Pipeline object that chains together the QuantileTransformer ('trans') and KNeighborsClassifier ('model').\n",
    "        # The pipeline is stored in the 'models' dictionary with the number of quantiles 'i' as the key (converted to a string).\n",
    "        models[str(i)] = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "        \n",
    "    # Return the 'models' dictionary containing all the created pipelines.\n",
    "    return models\n",
    "\n",
    "\n",
    "# Define a function named 'evaluate_model' that takes a machine learning 'model', input features 'X', and output labels 'y' as input.\n",
    "def evaluate_model(model, X, y):\n",
    "    # Create a RepeatedStratifiedKFold cross-validation object named 'cv'.\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    \n",
    "    # Perform cross-validation using 'cross_val_score' to evaluate the 'model'.\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # Return the list of accuracy scores obtained from cross-validation.\n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "# Call the get_dataset function to load and prepare the dataset, and unpack the features (X) and labels (y)\n",
    "X, y = get_dataset(pima_indians_diabetes_csv)\n",
    "\n",
    "# get the models to evaluate\n",
    "# Call the function `get_models()` to retrieve a dictionary of machine learning models that will be evaluated.\n",
    "models = get_models()\n",
    "\n",
    "# Initialize an empty list called 'results' to store the mean accuracy scores for each model.\n",
    "results = list()\n",
    "# Start a loop that iterates through the 'models' dictionary. 'models.items()' returns key-value pairs (name, model).\n",
    "for name, model in models.items():\n",
    "    # Evaluate the current 'model' using the 'evaluate_model' function with the dataset (X, y).\n",
    "    # The 'evaluate_model' function performs cross-validation and returns a list of accuracy scores.\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    \n",
    "    # Calculate the mean of the 'scores' list (which represents the average accuracy from cross-validation) and append it to the 'results' list.\n",
    "    results.append(mean(scores))\n",
    "    \n",
    "    # Print the performance of the current model.\n",
    "    # '>%s %.3f (%.3f)' is a format string:\n",
    "    #   - '%s' will be replaced by the 'name' of the model (which is the number of quantiles in this case).\n",
    "    #   - '%.3f' will be replaced by the mean accuracy, formatted to 3 decimal places.\n",
    "    #   - '%.3f' will be replaced by the standard deviation of the accuracy scores, formatted to 3 decimal places.\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we observe that **surprisingly smaller values** for `n_quantiles` resulted in better accuracy. For example, a value of **9** achieved an accuracy of approximately **74.7%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model performance for comparison\n",
    "# Plot the 'results' data. Assuming 'results' is a list or array of performance metrics (e.g., accuracy scores) from different models or configurations.\n",
    "plt.plot(results)\n",
    "# Display the plot created by plt.plot(). This will open a window showing the graph.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between quantile resolution and model accuracy is visualized through a line plot, revealing several key insights. Most notably, we observe a distinct performance peak when using fewer than 10 quantiles, with accuracy sharply declining as the number of quantiles increases beyond this threshold. This counterintuitive pattern suggests that excessive quantization may actually degrade model performance, possibly due to overfitting or loss of meaningful distribution characteristics. The findings strongly indicate that systematic exploration of both quantile counts (particularly in the 5-20 range) and alternative output distributions could yield additional performance gains, warranting further investigation as part of the model optimization process.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored the transformation of numerical input variables that exhibit skewed or non-standard distributions due to factors like outliers, multi-modal patterns, or exponential distributions. Then we explored the performance of machine learning algorithms both with and without transformation. \n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
