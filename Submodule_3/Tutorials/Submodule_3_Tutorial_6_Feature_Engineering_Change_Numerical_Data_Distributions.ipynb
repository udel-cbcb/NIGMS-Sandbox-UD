{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Numerical Data Distributions \n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Numerical input variables may have a highly skewed or non-standard distribution. This could be\n",
    "caused by outliers in the data, multi-modal distributions, highly exponential distributions, and\n",
    "more. Many machine learning algorithms prefer or perform better when numerical input variables\n",
    "and even output variables in the case of regression have a standard probability distribution,\n",
    "such as a Gaussian (normal) or a Uniform distribution.\n",
    "\n",
    "The quantile transform provides an automatic way to transform a numeric input variable to\n",
    "have a different data distribution, which in turn, can be used as input to a predictive model.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn why machine learning algorithms prefer numerical variables with Gaussian or standard probability distributions\n",
    "- Understand quantile transforms for transforming numerical variables to Gaussian or Uniform distributions \n",
    "- Learn how to use `QuantileTransformer` to change probability distributions of numeric variables to improve model performance\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Apply normal quantile transform\n",
    "- Apply uniform quantile transform  \n",
    "- Compare model performance with different transforms\n",
    "- Explore impact of quantile parameters\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python programming\n",
    "- Familiarity with NumPy libraries\n",
    "- Knowledge of basic statistical concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages, import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import exp, mean, std\n",
    "from numpy.random import randn\n",
    "from pandas import DataFrame, read_csv\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, QuantileTransformer\n",
    "\n",
    "# Pima Indians Diabetes data set file\n",
    "pima_indians_diabetes_csv = \"../../Data/pima-indians-diabetes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Quantile Transforms\n",
    "\n",
    "A quantile transform will map a variable's probability distribution to another probability\n",
    "distribution. The quantile function ranks or smooths out the relationship between observations and can be\n",
    "mapped onto other distributions, such as the uniform or normal distribution. The transformation\n",
    "can be applied to each numeric input variable in the training dataset and then provided as\n",
    "input to a machine learning model to learn a predictive modeling task. This quantile transform\n",
    "is available in the scikit-learn Python machine learning library via the **QuantileTransformer**\n",
    "class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first creates a sample of 1,000 random Gaussian values and adds a\n",
    "skew to the dataset. A histogram is created from the skewed dataset and clearly shows the\n",
    "distribution pushed to the far left. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of the quantile transform\n",
    "\n",
    "# generate gaussian data sample\n",
    "data = randn(1000)\n",
    "\n",
    "# add a skew to the data distribution\n",
    "data = exp(data)\n",
    "\n",
    "# histogram of the raw data with a skew\n",
    "plt.hist(data, bins=25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a QuantileTransformer is used to map the data to a Gaussian distribution and standardize the result, centering the values on the mean value of 0 and a standard deviation of\n",
    "1.0. A histogram of the transform data is created showing a Gaussian shaped data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data to have rows and columns\n",
    "data = data.reshape((len(data), 1))\n",
    "print(data.shape)\n",
    "\n",
    "# quantile transform the raw data perform a normal quantile transform of the\n",
    "# dataset 'output_distribution\" is the marginal distribution for the transformed\n",
    "# data. The choices are 'uniform' (default) or 'normal'.\n",
    "quantile = QuantileTransformer(output_distribution=\"normal\")\n",
    "data_trans = quantile.fit_transform(data)\n",
    "\n",
    "# histogram of the transformed data\n",
    "plt.hist(data_trans, bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient data as\n",
    "either an onset of diabetes within five years or not. \n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class \n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the variables from the pima-indians-diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the dataset\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "print(dataset.head())\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# summarize each variable\n",
    "print(dataset.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the 8\n",
    "input variables, one output variable, and 768 rows of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a histogram is created for each input variable. If we ignore the clutter of the plots and\n",
    "focus on the histograms themselves, we can see that many variables have a skewed distribution.\n",
    "The dataset provides a good candidate for using a quantile transform to make the variables\n",
    "more-Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's first and evaluate a machine learning model on the raw dataset. We will use\n",
    "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
    "stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate KNN classifier on the raw dataset\n",
    "\n",
    "# KFold is a cross-validator that divides the dataset into k folds.\n",
    "#\n",
    "# Stratified is to ensure that each fold of dataset has the same proportion of\n",
    "#   observations with a given label.\n",
    "#\n",
    "# Repeated provides a way to improve the estimated performance of a machine\n",
    "#   learning model.\n",
    "#\n",
    "# This involves simply repeating the cross-validation procedure multiple times\n",
    "# and reporting the mean result across all folds from all runs. This mean result\n",
    "# is expected to be a more accurate estimate of the true unknown underlying mean\n",
    "# performance of the model on the dataset, as calculated using the standard\n",
    "# error.\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype(\"float32\")\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define and configure the model using\n",
    "# Classifier implementing the k-nearest neighbors vote.\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# evaluate the model using RepeatedStratifiedKFold cross validator,\n",
    "# that repeats Stratified K-Fold n times with different randomization in each\n",
    "# repetition.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the model achieved a mean classification accuracy of about 71.7\n",
    "percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Quantile Transform\n",
    "\n",
    "It is often desirable to transform an input variable to have a normal probability distribution to improve the modeling performance. We can apply the Quantile transform using the\n",
    "QuantileTransformer class and set the output distribution argument to `normal'. We\n",
    "must also set the n quantiles argument to a value less than the number of observations in the\n",
    "training dataset, in this case, 100. Once defined, we can call the fit transform() function and\n",
    "pass it to our dataset to create a quantile transformed version of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a normal quantile transform of the dataset\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a normal quantile transform of the dataset 'n_quantiles\" is the number\n",
    "# of quantiles to be computed. It corresponds to the number of landmarks used to\n",
    "# discretize the cumulative distribution function.  'output_distribution\" is the\n",
    "# marginal distribution for the transformed data. The choices are 'uniform'\n",
    "# (default) or 'normal'.\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"normal\")\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the shape of the histograms for each variable looks very Gaussian as compared\n",
    "to the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
    "normal quantile transform of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate KNN with normal quantile transform\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype(\"float32\")\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define the pipeline\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"normal\")\n",
    "\n",
    "# Classifier implementing the k-nearest neighbors vote.\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report pipeline performance\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the normal quantile transform results in a lift in\n",
    "performance from 71.7 percent accuracy without the transform to about 73.4 percent with the\n",
    "transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Quantile Transform\n",
    "\n",
    "Sometimes it can be beneficial to transform a highly exponential or multi-modal distribution to\n",
    "have a uniform distribution. This is especially useful for data with a large and sparse range of\n",
    "values, e.g. outliers that are common rather than rare. We can apply the transform by defining\n",
    "a QuantileTransformer class and setting the output distribution argument to `uniform'\n",
    "(the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a uniform quantile transform of the dataset\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a uniform quantile transform of the dataset\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"uniform\")\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the shape of the histograms for each variable looks very uniform compared to\n",
    "the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
    "uniform quantile transform of the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate KNN classifier on the dataset with uniform quantile transform\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv(pima_indians_diabetes_csv, header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype(\"float32\")\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define the pipeline\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution=\"uniform\")\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report pipeline performance\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the uniform transform results in a lift in performance\n",
    "from 71.7 percent accuracy without the transform to about 73.4 percent with the normal transform, and achieved a score of 74.1 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the number of quantiles as an arbitrary number, in this case, 100. This hyperparameter can be tuned to explore the effect of the resolution of the transform on the resulting\n",
    "skill of the model. The example below performs this experiment and plots the mean accuracy\n",
    "for different n quantiles values from 1 to 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore number of quantiles on classification accuracy\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset(filename):\n",
    "    # load dataset\n",
    "    dataset = read_csv(filename, header=None)\n",
    "    data = dataset.values\n",
    "    # separate into input and output columns\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    # ensure inputs are floats and output is an integer label\n",
    "    X = X.astype(\"float32\")\n",
    "    y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for i in range(1, 100):\n",
    "        # define the pipeline\n",
    "        trans = QuantileTransformer(n_quantiles=i, output_distribution=\"uniform\")\n",
    "        model = KNeighborsClassifier()\n",
    "        # The purpose of the pipeline is to assemble several steps that can be\n",
    "        # cross-validated together while setting different parameters.\n",
    "        models[str(i)] = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "    return models\n",
    "\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # Repeats Stratified K-Fold n times with different randomization in each repetition.\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # Evaluate a score by cross-validation\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset(pima_indians_diabetes_csv)\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results = list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(mean(scores))\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that surprisingly smaller values resulted in better accuracy, with\n",
    "values such as 9 achieving an accuracy of about 74.7 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance for comparison\n",
    "plt.plot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line plot is created showing the number of quantiles used in the transform versus the\n",
    "classification accuracy of the resulting model. We can see a bump with values less than 10 and\n",
    "drop and \n",
    "at performance after that. The results highlight that there is likely some benefit in\n",
    "exploring different distributions and number of quantiles to see if better performance can be\n",
    "achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored the transformation of numerical input variables that exhibit skewed or non-standard distributions due to factors like outliers, multi-modal patterns, or exponential distributions. Then we explored the performance of machine learning algorithms both with and without transformation. \n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
