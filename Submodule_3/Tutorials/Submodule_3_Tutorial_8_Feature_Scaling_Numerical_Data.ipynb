{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling: Scale Numerical Data\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Many machine learning algorithms perform significantly better when numerical input variables are scaled to a standard range. This is because algorithms that rely on distance calculations (e.g., k-Nearest Neighbors, Support Vector Machines) or gradient-based optimization (e.g., linear regression, neural networks) are sensitive to the scale of the input features. Without scaling, features with larger magnitudes can dominate the model's behavior, leading to biased or suboptimal results.\n",
    "\n",
    "This tutorial provides a comprehensive guide to two widely used techniques for scaling numerical data: **normalization** and **standardization**. These preprocessing steps ensure that all input features contribute equally to the model's learning process, improving performance and convergence.\n",
    "\n",
    "#### Key Concepts:\n",
    "1. **Normalization**:\n",
    "   - Rescales numerical features to a fixed range, typically [0, 1].\n",
    "   - Useful for algorithms that require input data to be bounded, such as neural networks.\n",
    "\n",
    "2. **Standardization**:\n",
    "   - Rescales numerical features to have a mean of 0 and a standard deviation of 1.\n",
    "   - Suitable for algorithms that assume normally distributed data, such as linear regression and logistic regression.\n",
    "\n",
    "#### Why Scaling Matters:\n",
    "- **Improves Model Performance**: Ensures that no single feature dominates due to its scale.\n",
    "- **Speeds Up Convergence**: Helps gradient-based algorithms converge faster during training.\n",
    "- **Enhances Interpretability**: Makes it easier to compare the importance of features.\n",
    "\n",
    "By the end of this tutorial, you will understand how to apply normalization and standardization to your datasets, ensuring that your machine learning models perform at their best.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand why data scaling is a recommended pre-processing step for many machine learning algorithms\n",
    "- Learn how data scaling can be achieved through normalizing or standardizing real-valued input and output variables\n",
    "- Apply standardization and normalization techniques to improve predictive modeling algorithm performance\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Load and examine the diabetes dataset\n",
    "- Apply MinMaxScaler transformation\n",
    "- Apply StandardScaler transformation\n",
    "- Compare model performance with different scaling approaches\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python programming\n",
    "- Familiarity with NumPy and scikit-learn libraries\n",
    "- Knowledge of basic statistical and machine learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install the required packages and import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python libraries using the pip package manager.\n",
    "# These libraries are commonly used for data analysis, machine learning, and plotting.\n",
    "\n",
    "# %pip is a magic command in Jupyter Notebook to install Python packages directly in the notebook environment.\n",
    "# - matplotlib: A plotting library for creating static, animated, and interactive visualizations.\n",
    "# - numpy: A fundamental library for numerical computations, supporting arrays, matrices, and mathematical functions.\n",
    "# - pandas: A powerful library for data manipulation and analysis, providing data structures like DataFrames.\n",
    "# - scikit-learn: A comprehensive library for machine learning, offering tools for data preprocessing, modeling, and evaluation.\n",
    "%pip install matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyplot module from matplotlib for plotting functionalities and alias it as plt.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import the asarray, mean, and std functions from the numpy library for numerical operations.\n",
    "from numpy import asarray, mean, std\n",
    "\n",
    "# Import the DataFrame and read_csv classes from the pandas library for data manipulation and reading CSV files.\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "# Import RepeatedStratifiedKFold and cross_val_score from sklearn.model_selection for model evaluation using cross-validation.\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "\n",
    "# Import KNeighborsClassifier from sklearn.neighbors for K-Nearest Neighbors classification model.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import Pipeline from sklearn.pipeline to create composite estimators.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import LabelEncoder, MinMaxScaler, and StandardScaler from sklearn.preprocessing for data preprocessing tasks like label encoding and feature scaling.\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Data Scaling Methods\n",
    "\n",
    "- **Normalization**: Scales each input variable separately to the range of **0 to 1**. This range is ideal for floating-point values, as it ensures the highest precision for numerical computations.  \n",
    "  Formula:  \n",
    "  $$\n",
    "  X_{\\text{normalized}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "  $$\n",
    "\n",
    "- **Standardization**: Scales each input variable separately by **subtracting the mean** (centering) and **dividing by the standard deviation**. This transforms the distribution to have a **mean of zero** and a **standard deviation of one**.  \n",
    "  Formula:  \n",
    "  $$\n",
    "  X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}\n",
    "  $$  \n",
    "  where $\\mu$ is the mean and $\\sigma$ is the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "**Normalization** is a technique used to rescale data from its original range so that all values fall within a new range of **0 to 1**. To perform normalization, you need to know or accurately estimate the **minimum** and **maximum** observable values in the dataset. These values can often be estimated from the available data.\n",
    "\n",
    "- **Purpose**: Normalization ensures that all features contribute equally to the model's learning process, especially when features have different scales.\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  X_{\\text{normalized}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "  $$  \n",
    "  where:\n",
    "  - $X$ is the original value,\n",
    "  - $X_{\\text{min}}$ is the minimum value in the dataset,\n",
    "  - $X_{\\text{max}}$ is the maximum value in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a normalization\n",
    "\n",
    "# Define a NumPy array named 'data' containing sample data for normalization.\n",
    "data = asarray([[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]])\n",
    "# Print the original 'data' array to show the data before normalization.\n",
    "print(data)\n",
    "\n",
    "# Transform features by scaling each feature to a given range.\n",
    "# This estimator scales and translates each feature individually such\n",
    "# that it is in the given range on the training set, e.g. between\n",
    "# zero and one.\n",
    "# Create a MinMaxScaler object named 'scaler'.\n",
    "# MinMaxScaler scales features to a range between zero and one by default.\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the MinMaxScaler 'scaler' to the 'data' to calculate the min and max values for each feature.\n",
    "# Then, transform the 'data' using the fitted scaler to normalize it.\n",
    "scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Print the 'scaled' array to show the data after Min-Max normalization.\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standardization\n",
    "\n",
    "Standardizing a dataset involves rescaling the distribution of values so that:\n",
    "- The **mean** of the observed values becomes **0**.\n",
    "- The **standard deviation** becomes **1**.\n",
    "\n",
    "This process is equivalent to **subtracting the mean** (centering the data) and **dividing by the standard deviation**. Like normalization, standardization is often useful and sometimes required for machine learning algorithms, especially when input features have differing scales.\n",
    "\n",
    "#### Key Points:\n",
    "- **Assumption**: Standardization assumes that the data follows a **Gaussian distribution** (bell curve) with a well-defined mean and standard deviation.\n",
    "- **Flexibility**: You can still standardize data even if it doesn’t perfectly fit a Gaussian distribution, but the results may be less reliable.\n",
    "- **Use Cases**: Standardization is particularly important for algorithms that rely on distance calculations (e.g., k-Nearest Neighbors, Support Vector Machines) or gradient-based optimization (e.g., linear regression, neural networks).\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "where:\n",
    "- $X$ is the original value,\n",
    "- $\\mu$ is the mean of the dataset,\n",
    "- $\\sigma$ is the standard deviation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a standardization\n",
    "# This section demonstrates how to standardize data using scikit-learn's StandardScaler.\n",
    "\n",
    "# Define a NumPy array named 'data' representing the dataset to be standardized.\n",
    "data = asarray([[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]])\n",
    "\n",
    "# Print the original 'data' array to show the data before standardization.\n",
    "print(data)\n",
    "\n",
    "# Create a StandardScaler object named 'scaler'.\n",
    "# StandardScaler is used to standardize features by removing the mean and scaling to unit variance.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler to the 'data' array and then transform the data.\n",
    "# 'fit_transform' computes the mean and standard deviation from the data and then performs standardization.\n",
    "scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Print the 'scaled' array, which now contains the standardized data.\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient data as\n",
    "either an onset of diabetes within five years or not. \n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class \n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of the code is designed to load the pima-indians-diabetes dataset and provide a basic summary of its contents.\n",
    "\n",
    "# Load the dataset from a CSV file.\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "\n",
    "# Print the first few rows of the dataset.\n",
    "# This allows for a quick inspection of the data's structure and content.\n",
    "print(dataset.head())\n",
    "\n",
    "# Print the shape (number of rows and columns) of the dataset.\n",
    "print(dataset.shape)\n",
    "\n",
    "# Print descriptive statistics for each variable in the dataset.\n",
    "# This includes count, mean, std, min, max, and percentiles.\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Overview\n",
    "\n",
    "The dataset consists of:\n",
    "- **8 input variables**,\n",
    "- **1 output variable**, and\n",
    "- **768 rows of data**.\n",
    "\n",
    "A statistical summary of the input variables reveals that each variable has a **significantly different scale**. This characteristic makes the dataset an excellent candidate for exploring and applying **data scaling methods**.\n",
    "\n",
    "### Visualization of Input Variables\n",
    "\n",
    "To further understand the dataset, we can create a **histogram** for each input variable. These plots confirm the **differing scales** across the variables and highlight the variability in their distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate histograms for each column in the 'dataset' DataFrame.\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "\n",
    "# Iterate through each histogram subplot in the figure and set the title font size to 4.\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# Display the plot containing the histograms.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation on Raw Data\n",
    "\n",
    "Next, we will fit and evaluate a machine learning model using the **raw dataset**. For this task, we will employ a **k-nearest neighbor (KNN) algorithm** with its default hyperparameters. To ensure robust evaluation, we will use **repeated stratified k-fold cross-validation**, which provides a reliable estimate of the model's performance by maintaining the class distribution across folds and repeating the process multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of the code evaluates a K-Nearest Neighbors (KNN) classifier on the raw Pima Indians Diabetes dataset without any feature transformation.\n",
    "\n",
    "# Load the dataset from a CSV file named \"pima-indians-diabetes.csv\" located in the \"../../Data/\" directory into a pandas DataFrame.\n",
    "# 'header=None' argument indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "\n",
    "# Convert the pandas DataFrame 'dataset' into a NumPy array 'data'.\n",
    "# This is often done for compatibility with scikit-learn and for faster numerical computations.\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "# Split the 'data' array into input features (X) and output labels (y).\n",
    "# 'data[:, :-1]' selects all rows (:) and all columns except the last one (:-1) for input features (X).\n",
    "# 'data[:, -1]' selects all rows (:) and only the last column (-1) for output labels (y).\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "# Convert the input features 'X' to float32 data type.\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "# This ensures that the features are in a numerical format suitable for machine learning models.\n",
    "# Encode the output labels 'y' using LabelEncoder.\n",
    "# 'y.astype(\"str\")' first converts the labels to string type to handle potential mixed data types.\n",
    "# 'LabelEncoder().fit_transform()' then fits the LabelEncoder to the labels and transforms them into numerical labels (0, 1, 2, ...).\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define and configure the model\n",
    "# Create a KNeighborsClassifier object named 'model' with default parameters.\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# evaluate the model\n",
    "# Create a RepeatedStratifiedKFold cross-validation object named 'cv'.\n",
    "# 'n_splits=10' sets the number of folds for k-fold cross-validation to 10.\n",
    "# 'n_repeats=3' specifies that the cross-validation process should be repeated 3 times.\n",
    "# 'random_state=1' ensures that the data splitting is consistent across runs for reproducibility.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Perform cross-validation using 'cross_val_score' to evaluate the 'model'.\n",
    "# 'model' is the KNeighborsClassifier model to evaluate.\n",
    "# 'X' is the input features.\n",
    "# 'y' is the output labels.\n",
    "# 'scoring=\"accuracy\"' specifies that accuracy is the metric to evaluate.\n",
    "# 'cv=cv' uses the RepeatedStratifiedKFold object 'cv' for cross-validation splitting.\n",
    "# 'n_jobs=-1' utilizes all available CPU cores for parallel computation to speed up the process.\n",
    "n_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "# Print the performance of the KNN model.\n",
    "# \"Accuracy: %.3f (%.3f)\" is a format string.\n",
    "# '%.3f' is replaced by the mean accuracy calculated from 'n_scores', rounded to 3 decimal places.\n",
    "# '%.3f' is replaced by the standard deviation of the accuracy scores in 'n_scores', rounded to 3 decimal places.\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the model achieved a mean classification accuracy of about\n",
    "71.7 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MinMaxScaler` Transform\n",
    "\n",
    "We can apply the `MinMaxScaler` to the diabetes dataset to normalize the input variables. By default, the `MinMaxScaler` scales values to the range **0 to 1**. Here's how it works:\n",
    "\n",
    "1. **Define the Scaler**: Create an instance of `MinMaxScaler` with default hyperparameters.\n",
    "2. **Transform the Data**: Use the `fit_transform()` function to apply the scaling to the dataset, generating a transformed version where all input variables are normalized to the specified range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of each input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a minmax scaler transform of the diabetes dataset\n",
    "\n",
    "# Load the 'pima-indians-diabetes.csv' dataset from the specified path using pandas' read_csv function.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "\n",
    "# Extract all rows and all columns except the last one from the dataset's values (NumPy array).\n",
    "# This assumes the last column is the target variable and the rest are features.\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a min-max scaler transform of the dataset\n",
    "# Create a MinMaxScaler object to scale features to a range between 0 and 1.\n",
    "trans = MinMaxScaler()\n",
    "\n",
    "# Fit the MinMaxScaler to the data and then transform the data.\n",
    "# 'fit_transform' learns the min and max values from the data and applies the scaling.\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# Convert the NumPy array 'data' (which is now scaled) back into a pandas DataFrame.\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# Print descriptive statistics of the transformed DataFrame.\n",
    "# 'describe()' provides count, mean, std, min, 25%, 50%, 75%, max for each column.\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the transformation, we can observe that the distributions of the variables have been adjusted. The minimum and maximum values for each variable are now **0.0** and **1.0**, respectively, confirming that the data has been successfully normalized to the desired range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram plots of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for each variable in the 'dataset'. 'dataset.hist()' generates histograms for all numerical columns in the DataFrame. 'xlabelsize=4' and 'ylabelsize=4' set the font size of the x and y axis labels to 4.\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "\n",
    "# Iterate over each histogram subplot in the figure 'fig'. 'fig.ravel()' flattens the array of subplots into a 1D array, and '[x.title.set_size(4) for x in fig.ravel()]' sets the title size of each subplot to 4 using a list comprehension.\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# Display the generated histograms plot. 'plt.show()' is a function from matplotlib.pyplot that opens a window and displays the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram plots of the variables are generated to visualize their distributions. While the overall shapes of the distributions appear similar to their original forms (as seen in the previous section), we can confirm that the minimum and maximum values have been successfully scaled to **0** and **1**, respectively, as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will evaluate the same **k-nearest neighbor (KNN) model** as before, but this time using the dataset transformed by the **`MinMaxScaler`**. This will allow us to assess the impact of normalization on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script evaluates a K-Nearest Neighbors (KNN) classifier on the Pima Indians Diabetes dataset,\n",
    "# using a MinMaxScaler for feature scaling and Repeated Stratified K-Fold cross-validation for performance evaluation.\n",
    "\n",
    "# Read the CSV file 'pima-indians-diabetes.csv' into a pandas DataFrame.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "\n",
    "# Extract the values from the pandas DataFrame and store them in a NumPy array named 'data'.\n",
    "data = dataset.values\n",
    "\n",
    "# Separate the 'data' array into input features (X) and output labels (y).\n",
    "# 'data[:, :-1]' selects all rows and all columns except the last one for input features (X).\n",
    "# 'data[:, -1]' selects all rows and only the last column for output labels (y).\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# Ensure inputs are floats and output is an integer label\n",
    "# Convert the input features 'X' to float32 data type.\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "# Encode the output labels 'y' using LabelEncoder to convert them into integer labels.\n",
    "# 'y.astype(\"str\")' first converts the labels to string type to handle potential mixed data types before encoding.\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# Define the pipeline\n",
    "# Create a MinMaxScaler object named 'trans' to scale the input features.\n",
    "trans = MinMaxScaler()\n",
    "# Create a KNeighborsClassifier object named 'model' with default parameters.\n",
    "model = KNeighborsClassifier()\n",
    "# Create a Pipeline object named 'pipeline' that chains the MinMaxScaler and KNeighborsClassifier.\n",
    "# 'steps=[(\"t\", trans), (\"m\", model)]' defines the steps as a list of tuples, where 't' is the scaler and 'm' is the model.\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# Evaluate the pipeline\n",
    "# Create a RepeatedStratifiedKFold cross-validation object named 'cv'.\n",
    "# 'n_splits=10' sets the number of folds for k-fold cross-validation to 10.\n",
    "# 'n_repeats=3' specifies that the cross-validation process should be repeated 3 times.\n",
    "# 'random_state=1' ensures that the data splitting is consistent across runs for reproducibility.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate the 'pipeline' using cross-validation with RepeatedStratifiedKFold.\n",
    "# 'pipeline' is the model pipeline to evaluate.\n",
    "# 'X' is the input features.\n",
    "# 'y' is the output labels.\n",
    "# 'scoring=\"accuracy\"' specifies that accuracy is the metric to evaluate.\n",
    "# 'cv=cv' uses the 'cv' cross-validation object defined above.\n",
    "# 'n_jobs=-1' utilizes all available CPU cores for parallel computation to speed up the process.\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report pipeline performance\n",
    "# Print the mean and standard deviation of the cross-validation accuracy scores.\n",
    "# \"Accuracy: %.3f (%.3f)\" is a format string to print the accuracy and standard deviation, rounded to 3 decimal places.\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `MinMaxScaler` transform results in a lift in\n",
    "performance from 71.7 percent accuracy without the transform to about 73.9 percent with the\n",
    "transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `StandardScaler` Transform\n",
    "\n",
    "We can apply the `StandardScaler` to the diabetes dataset to standardize the input variables. By default, the `StandardScaler` performs the following operations:\n",
    "1. **Centering**: Subtracts the mean of each variable to center the data around **0.0**.\n",
    "2. **Scaling**: Divides by the standard deviation to ensure the data has a standard deviation of **1.0**.\n",
    "\n",
    "### Steps to Apply `StandardScaler`:\n",
    "1. **Define the Scaler**: Create an instance of `StandardScaler` with default hyperparameters.\n",
    "2. **Transform the Data**: Use the `fit_transform()` function to apply the standardization to the dataset, generating a transformed version where the input variables have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of each input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of the code demonstrates how to apply StandardScaler to the Pima Indians Diabetes dataset and then summarizes the transformed data.\n",
    "\n",
    "# Load the Pima Indians Diabetes dataset from the specified CSV file into a pandas DataFrame.\n",
    "# 'header=None' indicates that the CSV file does not have a header row.\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "\n",
    "# Extract the input features (all columns except the last one) from the dataset DataFrame and convert them into a NumPy array.\n",
    "# '[:, :-1]' selects all rows (:) and all columns up to, but not including, the last column (:-1).\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# Initialize a StandardScaler object named 'trans'.\n",
    "# StandardScaler standardizes features by removing the mean and scaling to unit variance.\n",
    "trans = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler 'trans' to the input data 'data' and then transform the data.\n",
    "# 'fit_transform' computes the mean and standard deviation on the data and then performs the standardization.\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# Convert the transformed NumPy array 'data' back into a pandas DataFrame named 'dataset'.\n",
    "# This is done to easily summarize the transformed data using DataFrame methods.\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# Print the descriptive statistics of the transformed DataFrame 'dataset'.\n",
    "# 'describe()' method provides summary statistics of the DataFrame, including count, mean, std, min, 25%, 50%, 75%, max for each column.\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the\n",
    "distributions have been adjusted and that the mean is a very small number close to zero and\n",
    "the standard deviation is very close to 1.0 for each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram plots of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for all variables in the dataset\n",
    "# The xlabelsize and ylabelsize parameters control the font size of axis labels\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "\n",
    "# Reduce the title size for each subplot in the figure for better readability\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# Display the histogram plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram plots of the variables are generated to visualize their distributions. While the overall shapes of the distributions appear similar to their original forms (as seen in the previous section), the **scale on the x-axis** has changed. Notably, the **center of mass** for each distribution is now centered around **0**, which is more apparent for some variables than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Next, we will evaluate the same **k-nearest neighbor (KNN) model** as before, but this time using the dataset transformed by the **`StandardScaler`**. This will allow us to assess the impact of standardization on the model's performance. The complete example is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 'pima-indians-diabetes.csv' dataset from the specified relative path using pandas' read_csv function, assuming no header row.\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "\n",
    "# Convert the pandas DataFrame 'dataset' into a NumPy array 'data' for easier numerical operations.\n",
    "data = dataset.values  # Convert DataFrame to NumPy array for easier manipulation\n",
    "\n",
    "# Separate features (X) and target labels (y)\n",
    "# Assign all columns except the last one from the 'data' array to 'X' as features.\n",
    "X, y = data[:, :-1], data[:, -1]  # Separate features (X) and target labels (y)\n",
    "\n",
    "# Ensure input features are float type and encode target labels as integers\n",
    "# Convert the feature matrix 'X' to float32 data type to ensure numerical stability and compatibility with scikit-learn models.\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "# Apply Label Encoding to the target labels 'y' to convert them into numerical labels (0, 1, 2, ...) if they are categorical.\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))  # Encode labels (if necessary)\n",
    "\n",
    "# Define a pipeline with standardization and KNN classifier\n",
    "# Initialize a StandardScaler object named 'trans' to perform feature scaling (standardization).\n",
    "trans = StandardScaler()  # StandardScaler normalizes feature values\n",
    "\n",
    "# Initialize a KNeighborsClassifier object named 'model' with default parameters.\n",
    "model = KNeighborsClassifier()  # Define KNN model\n",
    "\n",
    "# Construct a Pipeline named 'pipeline' that sequentially applies the StandardScaler ('trans') and then the KNeighborsClassifier ('model').\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])  # Create a pipeline with scaling + KNN\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "# Create a RepeatedStratifiedKFold cross-validation object named 'cv' for stratified k-fold cross-validation repeated multiple times.\n",
    "# This performs 10-fold cross-validation, repeated 3 times for robustness\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate the pipeline using cross-validation\n",
    "# Perform cross-validation using cross_val_score to evaluate the 'pipeline' on the dataset (X, y).\n",
    "# Computes accuracy scores for each fold and repetition\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report mean and standard deviation of accuracy across folds\n",
    "# Print the mean and standard deviation of the accuracy scores obtained from cross-validation to summarize the model's performance.\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "# Display plot (though nothing is plotted in this script)\n",
    "# Display any plots created using matplotlib (in this script, no plot is actually generated or displayed, so this line is effectively doing nothing).\n",
    "plt.show()  # This is unnecessary unless you plan to plot something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that applying the **`StandardScaler` transform** leads to a noticeable improvement in model performance. The accuracy increases from **71.7%** (without scaling) to approximately **74.1%** (with standardization). This performance is slightly higher than the **73.9%** accuracy achieved using the **`MinMaxScaler`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Data scaling techniques, such as **normalization** and **standardization**, can significantly enhance model performance. In this tutorial, we observed that:\n",
    "- The **`StandardScaler`** improved the KNN model's accuracy from **71.7%** to **74.1%**.\n",
    "- The **`MinMaxScaler`** achieved an accuracy of **73.9%**.\n",
    "\n",
    "The choice between these scaling methods depends on the specific characteristics of your data and the requirements of your machine learning model.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to:\n",
    "- Delete any downloaded datasets\n",
    "- Shut down your SageMaker notebook instance when finished\n",
    "- Remove any unnecessary resources\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
