{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling: Scale Numerical Data\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Many machine learning algorithms perform better when numerical input variables are scaled to a standard range. This tutorial covers normalization and standardization techniques for scaling numerical data prior to modeling.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand why data scaling is a recommended pre-processing step for many machine learning algorithms\n",
    "- Learn how data scaling can be achieved through normalizing or standardizing real-valued input and output variables\n",
    "- Apply standardization and normalization techniques to improve predictive modeling algorithm performance\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Load and examine the diabetes dataset\n",
    "- Apply MinMaxScaler transformation\n",
    "- Apply StandardScaler transformation\n",
    "- Compare model performance with different scaling approaches\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python programming\n",
    "- Familiarity with NumPy and scikit-learn libraries\n",
    "- Knowledge of basic statistical and machine learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install the required packages and import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from numpy import asarray, mean, std\n",
    "from pandas import DataFrame, read_csv\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Data Scaling Methods\n",
    "\n",
    "* **Normalization** scales each input variable separately to the range of 0-1, which is\n",
    "the range for\n",
    "floating-point values where we have the most precision.\n",
    "* **Standardization** scales\n",
    "each input variable separately by subtracting the mean (called centering) and dividing by the\n",
    "standard deviation to shift the distribution to have a mean of zero and a standard deviation of\n",
    "one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "Normalization is a rescaling of the data from the original range so that all values are within the\n",
    "new range of 0 and 1. Normalization requires that you know or are able to accurately estimate\n",
    "the minimum and maximum observable values. You may be able to estimate these values from\n",
    "your available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a normalization\n",
    "\n",
    "# define data\n",
    "data = asarray([[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]])\n",
    "print(data)\n",
    "\n",
    "# define min max scaler\n",
    "# Transform features by scaling each feature to a given range.\n",
    "# This estimator scales and translates each feature individually such\n",
    "# that it is in the given range on the training set, e.g. between\n",
    "# zero and one.\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit to data, then transform it.\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standardization\n",
    "\n",
    "Standardizing a dataset involves rescaling the distribution of values so that the mean of observed\n",
    "values is 0 and the standard deviation is 1. This can be thought of as subtracting the mean\n",
    "value or centering the data. Like normalization, standardization can be useful, and even\n",
    "required in some machine learning algorithms when your data has input values with differing\n",
    "scales. Standardization assumes that your observations fit a Gaussian distribution (bell curve)\n",
    "with a well-behaved mean and standard deviation. You can still standardize your data if this\n",
    "expectation is not met, but you may not get reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a standardization\n",
    "\n",
    "# define data\n",
    "data = asarray([[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]])\n",
    "print(data)\n",
    "\n",
    "# define standard scaler\n",
    "# Standardize features by removing the mean and scaling to unit variance.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit to data, then transform it.\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient data as\n",
    "either an onset of diabetes within five years or not. \n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class \n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the diabetes dataset\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "print(dataset.head())\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# summarize each variable\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the 8\n",
    "input variables, one output variable, and 768 rows of data. A statistical summary of the input\n",
    "variables is provided show that each variable has a very different scale. This makes it a good\n",
    "dataset for exploring data scaling methods.\n",
    "\n",
    "We can create a histogram for each input variable. The plots confirm the differing scale\n",
    "for each input variable and show that the variables have different scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit and evaluate a machine learning model on the raw dataset. We will use\n",
    "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
    "stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the raw diabetes dataset\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype(\"float32\")\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define and configure the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report model performance\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the model achieved a mean classification accuracy of about\n",
    "71.7 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MinMaxScaler` Transform\n",
    "\n",
    "We can apply the `MinMaxScaler` to the diabetes dataset directly to normalize the input variables.\n",
    "We will use the default configuration and scale values to the range 0 and 1. First, a `MinMaxScaler`\n",
    "instance is defined with default hyperparameters. Once defined, we can call the `fit.transform()`\n",
    "function and pass it to our dataset to create a transformed version of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of each input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a minmax scaler transform of the diabetes dataset\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a min-max scaler transform of the dataset\n",
    "trans = MinMaxScaler()\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# summarize\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the\n",
    "distributions have been adjusted and that the minimum and maximum values for each variable\n",
    "are now 0.0 and 1.0 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram plots of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram plots of the variables are created, although the distributions don't look much\n",
    "different from their original distributions seen in the previous section. We can confirm that the\n",
    "minimum and maximum values are zero and one respectively, as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case, on a\n",
    "MinMaxScaler transform of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the diabetes dataset with minmax scaler transform\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype(\"float32\")\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define the pipeline\n",
    "trans = MinMaxScaler()\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report pipeline performance\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `MinMaxScaler` transform results in a lift in\n",
    "performance from 71.7 percent accuracy without the transform to about 73.9 percent with the\n",
    "transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `StandardScaler` Transform\n",
    "\n",
    "We can apply the `StandardScaler` to the diabetes dataset directly to standardize the input\n",
    "variables. We will use the default configuration and scale values to subtract the mean to center\n",
    "them on 0.0 and divide by the standard deviation to give the standard deviation of 1.0. First, a\n",
    "`StandardScaler` instance is defined with default hyperparameters. Once defined, we can call\n",
    "the fit transform() function and pass it to our dataset to create a transformed version of our\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of each input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a standard scaler transform of the diabetes dataset\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a robust scaler transform of the dataset\n",
    "trans = StandardScaler()\n",
    "data = trans.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# summarize\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the\n",
    "distributions have been adjusted and that the mean is a very small number close to zero and\n",
    "the standard deviation is very close to 1.0 for each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram plots of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram plots of the variables are created, although the distributions don't look much\n",
    "different from their original distributions seen in the previous section other than their scale on\n",
    "the x-axis. We can see that the center of mass for each distribution is centered on zero, which is\n",
    "more obvious for some variables than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "Next, let's evaluate the same KNN model as the previous section, but in this case, on a\n",
    "StandardScaler transform of the dataset. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the diabetes dataset with standard scaler transform\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(\"../../Data/pima-indians-diabetes.csv\", header=None)\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype(\"float32\")\n",
    "y = LabelEncoder().fit_transform(y.astype(\"str\"))\n",
    "\n",
    "# define the pipeline\n",
    "trans = StandardScaler()\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[(\"t\", trans), (\"m\", model)])\n",
    "\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report pipeline performance\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the StandardScaler transform results in a lift in\n",
    "performance from 71.7 percent accuracy without the transform to about 74.1 percent with the\n",
    "transform, slightly higher than the result using the MinMaxScaler that achieved 73.9 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Data scaling techniques like normalization and standardization can significantly improve model performance. In this tutorial, we saw how StandardScaler improved KNN model accuracy from 71.7% to 74.1%, while MinMaxScaler achieved 73.9% accuracy. The choice between scaling methods depends on your specific data and model requirements.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to:\n",
    "- Delete any downloaded datasets\n",
    "- Shut down your SageMaker notebook instance when finished\n",
    "- Remove any unnecessary resources\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
