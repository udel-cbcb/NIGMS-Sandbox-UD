{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling Exercise\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Data processing is a crucial step in developing Machine Learning systems, combining domain expertise with mathematical transformations. Feature scaling plays a vital role in preventing model bias toward features with larger magnitude values, making it particularly important when experimenting with multiple Machine Learning algorithms.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the importance of feature scaling in machine learning\n",
    "- Learn three key feature scaling techniques:\n",
    "  - Standardized Scaling (Z-score)\n",
    "  - Min-Max Scaling\n",
    "  - Robust Scaling\n",
    "- Apply feature scaling methods using scikit-learn preprocessing tools\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Apply `StandardScaler` to normalize data using Z-score method\n",
    "- Implement Min-Max scaling to transform features to [0,1] range\n",
    "- Use `RobustScaler` for scaling data with outliers\n",
    "- Compare results across different scaling methods\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python programming environment\n",
    "- Basic understanding of statistical and machine learning concepts\n",
    "- Familiarity with common ML libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "- Please select kernel \"conda_python3\" from SageMaker notebook instance.\n",
    "### Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data preprocessing\n",
    "# Import StandardScaler, MinMaxScaler, and RobustScaler from scikit-learn's preprocessing module.\n",
    "# These are used for scaling and normalizing numerical features to improve model performance.\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler  # Scalers for feature normalization\n",
    "\n",
    "# Import the NumPy library and alias it as 'np'.\n",
    "# NumPy is fundamental for numerical computations in Python, especially for handling arrays and matrices.\n",
    "import numpy as np  # Numerical operations\n",
    "\n",
    "# Import the pandas library and alias it as 'pd'.\n",
    "# Pandas is used for data manipulation and analysis, providing data structures like DataFrames and Series.\n",
    "import pandas as pd  # Data manipulation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Using unscaled features can introduce bias in machine learning models, particularly for algorithms like linear and logistic regression that are sensitive to variable magnitudes. While tree-based models (e.g., random forests) are inherently scale-invariant, standardizing features remains a recommended practice. Normalization ensures fair feature comparison when evaluating multiple algorithms and helps optimization converge faster in gradient-based methods. A consistent preprocessing approach across all features enables more reliable model performance comparisons and often improves overall results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set NumPy print options to suppress scientific notation for small numbers.\n",
    "# This makes the output of NumPy arrays more readable by displaying small numbers in fixed-point notation instead of scientific notation (e.g., 0.0001 instead of 1e-4).\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample RNASeq data\n",
    "\n",
    "The RNA-seq data reveals substantial variation in gene expression levels across the five analyzed genes, as evidenced by their FPKM (Fragments Per Kilobase per Million) values. The expression magnitudes span multiple orders of scale, with certain genes demonstrating markedly higher transcriptional activity compared to others. This wide dynamic range in FPKM measurements highlights the inherent biological diversity in gene expression patterns and underscores the need for appropriate data normalization when conducting downstream comparative analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame named 'fpkms'.\n",
    "# The DataFrame is initialized with a list of numerical values.\n",
    "# Each value in the list represents an FPKM (Fragments Per Kilobase of transcript per Million mapped reads) value,\n",
    "# likely representing gene expression levels from RNA sequencing data.\n",
    "# The 'columns=[\"fpkms\"]' argument specifies that the DataFrame should have a single column named \"fpkms\".\n",
    "fpkms = pd.DataFrame([1295.0, 25.0, 19000.0, 5.0, 1.0, 300.0], columns=[\"fpkms\"])\n",
    "\n",
    "# Display the DataFrame 'fpkms'.\n",
    "# This line will output the DataFrame to the console, showing the FPKM values in a tabular format with the column name \"fpkms\".\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized Scaling\n",
    "\n",
    "Standard scaling (Z-score normalization) transforms feature values by:  \n",
    "1. **Centering**: Subtracting the mean ($\\mu$) from each value  \n",
    "2. **Scaling**: Dividing by the standard deviation ($\\sigma$)  \n",
    "\n",
    "Mathematically:  \n",
    "$$\n",
    "SS(x_i) = \\frac{x_i - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Key properties:  \n",
    "- Results in a distribution with $\\mu = 0$ and $\\sigma = 1$  \n",
    "- Preserves the shape of the original distribution  \n",
    "- Alternative scaling by variance ($\\sigma^2$) is possible but less common\n",
    "\n",
    "### `fit_transform()` Function in Scaling\n",
    "\n",
    "The `fit_transform()` function is a convenient and efficient method used in scaling and preprocessing data in machine learning. It combines two steps into one: **fitting** and **transforming**.\n",
    "\n",
    "1. **Fit**:  \n",
    "   The function first **fits** the scaler to the data, calculating the necessary parameters (e.g., mean, standard deviation, min, or max) based on the input dataset.\n",
    "\n",
    "2. **Transform**:  \n",
    "   It then **transforms** the data by applying the scaling operation (e.g., normalization or standardization) using the calculated parameters.\n",
    "\n",
    "#### Key Points:\n",
    "- **Use Case**:  \n",
    "  `fit_transform()` is particularly useful for preprocessing **training data**, as it ensures the scaling is consistent and based on the distribution of the training set.\n",
    "\n",
    "- **Test/Validation Data**:  \n",
    "  For test or validation data, only the `transform()` function should be used to apply the same scaling parameters learned from the training data. This prevents **data leakage** and ensures the model generalizes well to unseen data.\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler object.\n",
    "# StandardScaler is used for feature scaling to standardize the 'fpkms' column.\n",
    "\n",
    "ss = # Your code goes here\n",
    "\n",
    "# Apply StandardScaler to the 'fpkms' column of the DataFrame 'fpkms' and create a new column named 'zscore' to store the standardized values.\n",
    "# ss.fit_transform() calculates the mean and standard deviation of the 'fpkms' column and then standardizes the data.\n",
    "# The standardized values (z-scores) are added as a new column named 'zscore' to the 'fpkms' DataFrame.\n",
    "\n",
    "fpkms[\"zscore\"] = # Your code goes here\n",
    "\n",
    "# Display the DataFrame 'fpkms' with the newly added 'zscore' column.\n",
    "# This line shows the DataFrame after the standardization process, allowing you to inspect the results.\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can manually use the formula to compute the same result\n",
    "# Convert the 'fpkms' column from the 'fpkms' DataFrame into a NumPy array named 'fw'.\n",
    "# It's assumed that 'fpkms' is a Pandas DataFrame and it has a column named 'fpkms' containing gene expression values (FPKM - Fragments Per Kilobase of transcript per Million mapped reads).\n",
    "fw = np.array(fpkms[\"fpkms\"])\n",
    "\n",
    "# Calculate the z-score for the first element of the 'fw' array (which represents FPKM values).\n",
    "# fw[0] accesses the first FPKM value in the 'fw' array.\n",
    "# np.mean(fw) calculates the mean of all FPKM values in the 'fw' array.\n",
    "# np.std(fw) calculates the standard deviation of all FPKM values in the 'fw' array.\n",
    "# The formula (fw[0] - np.mean(fw)) / np.std(fw) standardizes the first FPKM value by subtracting the mean and dividing by the standard deviation, resulting in a z-score.\n",
    "(fw[0] - np.mean(fw)) / np.std(fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling\n",
    "\n",
    "Min-max scaling normalizes feature values to a fixed range [0, 1] using the transformation:  \n",
    "\n",
    "$$\n",
    "MMS(X_i) = \\frac{x_i - \\min(X)}{\\max(X) - \\min(X)}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $x_i$ = original feature value  \n",
    "- $\\min(X)$ = minimum value in feature $X$  \n",
    "- $\\max(X)$ = maximum value in feature $X$  \n",
    "\n",
    "This preserves the original distribution while ensuring consistent scale across features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the MinMaxScaler.\n",
    "# MinMaxScaler is used to scale and translate each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "\n",
    "mms = # Your code goes here\n",
    "\n",
    "# Apply MinMaxScaler to the 'fpkms' column of the 'fpkms' DataFrame.\n",
    "# .fit_transform() method first computes the minimum and maximum values of the 'fpkms' column and then scales the column to the range [0, 1].\n",
    "# The result is assigned to a new column named 'minmax' in the 'fpkms' DataFrame.\n",
    "\n",
    "fpkms[\"minmax\"] = # Your code goes here\n",
    "\n",
    "# Display the 'fpkms' DataFrame after applying MinMaxScaler.\n",
    "# This will show the original 'fpkms' column and the newly created 'minmax' column containing the scaled values.\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can manually use the formula to compute the same result\n",
    "# Normalizing the first element of the array `fw` using min-max scaling\n",
    "# Formula: (x - min) / (max - min)\n",
    "# This scales the value to a range between 0 and 1\n",
    "(fw[0] - np.min(fw)) / (np.max(fw) - np.min(fw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaling\n",
    "\n",
    "Min-max scaling suffers from outlier sensitivity, as extreme values compress the scaled range for all observations. Robust scaling addresses this by using median-centered, IQR-normalized transformation:  \n",
    "\n",
    "$$\n",
    "\\text{ScaledValue} = \\frac{x_i - \\text{median}(X)}{\\text{IQR}_{(1,3)}(X)}\n",
    "$$\n",
    "  \n",
    "where $\\text{IQR}_{(1,3)}(X) = Q_3(75^{\\text{th}}\\ \\text{percentile}) - Q_1(25^{\\text{th}}\\ \\text{percentile})$. This approach preserves the majority of the data's structure while minimizing outlier influence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RobustScaler instance to scale the 'fpkms' column\n",
    "\n",
    "rs = # Your code goes here\n",
    "\n",
    "# Apply RobustScaler transformation to the 'fpkms' column and store the result in a new column 'robust'\n",
    "# RobustScaler is useful for handling outliers since it scales data based on the interquartile range (IQR)\n",
    "\n",
    "fpkms[\"robust\"] = # Your code goes here\n",
    "\n",
    "# Display the modified DataFrame with the new 'robust' column\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Feature scaling is essential for many machine learning algorithms, particularly those sensitive to feature magnitudes like linear and logistic regression. We explored three powerful scaling techniques:\n",
    "\n",
    "- Z-score scaling for standardization\n",
    "- Min-Max scaling for bounded ranges\n",
    "- Robust scaling for handling outliers\n",
    "\n",
    "Each method serves specific purposes and should be chosen based on your data characteristics and model requirements.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
