{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling Exercise\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Data processing is a critical step in building Machine Learning systems, requiring both domain knowledge and mathematical transformations. Feature scaling helps prevent model bias toward features with high magnitude values and is especially important when trying multiple Machine Learning algorithms.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the importance of feature scaling in machine learning\n",
    "- Learn three key feature scaling techniques:\n",
    "  - Standardized Scaling (Z-score)\n",
    "  - Min-Max Scaling\n",
    "  - Robust Scaling\n",
    "- Apply feature scaling methods using scikit-learn preprocessing tools\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Apply `StandardScaler` to normalize data using Z-score method\n",
    "- Implement Min-Max scaling to transform features to [0,1] range\n",
    "- Use `RobustScaler` for scaling data with outliers\n",
    "- Compare results across different scaling methods\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python programming environment\n",
    "- Basic understanding of statistical and machine learning concepts\n",
    "- Familiarity with common ML libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "### Set up conda environment\n",
    "\n",
    "Ensure that you have created then conda environment using the `conda_env.yml` file included in this repository. E.g.,\n",
    "\n",
    "```\n",
    "# Create conda environment\n",
    "conda env create -f conda_env.yml\n",
    "\n",
    "# Register the kernel\n",
    "python -m ipykernel install --user \\\n",
    "    --name=nigms_sandbox_ud \\\n",
    "    --display-name \"Python (NIGMS Sandbox UD)\"\n",
    "```\n",
    "\n",
    "Then, when starting the notebook, select the `\"Python (NIGMS Sandbox UD)\"` kernel from the list.\n",
    "\n",
    "Note that you may need to restart Jupyter Lab for these changes to take effect.\n",
    "\n",
    "### Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data preprocessing\n",
    "# Import StandardScaler, MinMaxScaler, and RobustScaler from scikit-learn's preprocessing module.\n",
    "# These are used for scaling and normalizing numerical features to improve model performance.\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler  # Scalers for feature normalization\n",
    "# Import the NumPy library and alias it as 'np'.\n",
    "# NumPy is fundamental for numerical computations in Python, especially for handling arrays and matrices.\n",
    "import numpy as np  # Numerical operations\n",
    "# Import the pandas library and alias it as 'pd'.\n",
    "# Pandas is used for data manipulation and analysis, providing data structures like DataFrames and Series.\n",
    "import pandas as pd  # Data manipulation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Using the raw values as input features might make models biased\n",
    "toward features having really high magnitude values. These models are typically sensitive to the magnitude or\n",
    "scale of features like linear or logistic regression. Other models like tree based methods can still work without\n",
    "feature scaling. However it is still recommended to normalize and scale down the features with feature scaling,\n",
    "especially if you want to try out multiple Machine Learning algorithms on input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set NumPy print options to suppress scientific notation for small numbers.\n",
    "# This makes the output of NumPy arrays more readable by displaying small numbers in fixed-point notation instead of scientific notation (e.g., 0.0001 instead of 1e-4).\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample RNASeq data\n",
    "\n",
    "We have five genes with their RNASeq FPKMs (Fragments Per Kilobase of exon per Million reads). It is quite evident that some genes have\n",
    "been expressed a lot more than the others, giving a rise to values of high scale and magnitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame named 'fpkms'.\n",
    "# The DataFrame is initialized with a list of numerical values.\n",
    "# Each value in the list represents an FPKM (Fragments Per Kilobase of transcript per Million mapped reads) value,\n",
    "# likely representing gene expression levels from RNA sequencing data.\n",
    "# The 'columns=[\"fpkms\"]' argument specifies that the DataFrame should have a single column named \"fpkms\".\n",
    "fpkms = pd.DataFrame([1295.0, 25.0, 19000.0, 5.0, 1.0, 300.0], columns=[\"fpkms\"])\n",
    "# Display the DataFrame 'fpkms'.\n",
    "# This line will output the DataFrame to the console, showing the FPKM values in a tabular format with the column name \"fpkms\".\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized Scaling\n",
    "\n",
    "The standard scaler tries to standardize each value in a feature column by removing the mean and scaling\n",
    "the variance to be 1 from the values. This is also known as centering and scaling and can be denoted\n",
    "mathematically as $SS(X_i) = \\frac{x_i - \\mu}{\\sigma}$, where each value in feature $X$ is subtracted by the mean $\\mu_i$ and the resultant is divided by the standard deviation $\\sigma_x$. This is also known as $Z$-scsore scaling. We can aslo divide the resultant by the variance instead of the standard deviation if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler object.\n",
    "# StandardScaler is used for feature scaling to standardize the 'fpkms' column.\n",
    "\n",
    "ss = # Your code goes here\n",
    "\n",
    "# Apply StandardScaler to the 'fpkms' column of the DataFrame 'fpkms' and create a new column named 'zscore' to store the standardized values.\n",
    "# ss.fit_transform() calculates the mean and standard deviation of the 'fpkms' column and then standardizes the data.\n",
    "# The standardized values (z-scores) are added as a new column named 'zscore' to the 'fpkms' DataFrame.\n",
    "\n",
    "fpkms[\"zscore\"] = # Your code goes here\n",
    "\n",
    "# Display the DataFrame 'fpkms' with the newly added 'zscore' column.\n",
    "# This line shows the DataFrame after the standardization process, allowing you to inspect the results.\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can manually use the formula to compute the same result\n",
    "# Convert the 'fpkms' column from the 'fpkms' DataFrame into a NumPy array named 'fw'.\n",
    "# It's assumed that 'fpkms' is a Pandas DataFrame and it has a column named 'fpkms' containing gene expression values (FPKM - Fragments Per Kilobase of transcript per Million mapped reads).\n",
    "fw = np.array(fpkms[\"fpkms\"])\n",
    "# Calculate the z-score for the first element of the 'fw' array (which represents FPKM values).\n",
    "# fw[0] accesses the first FPKM value in the 'fw' array.\n",
    "# np.mean(fw) calculates the mean of all FPKM values in the 'fw' array.\n",
    "# np.std(fw) calculates the standard deviation of all FPKM values in the 'fw' array.\n",
    "# The formula (fw[0] - np.mean(fw)) / np.std(fw) standardizes the first FPKM value by subtracting the mean and dividing by the standard deviation, resulting in a z-score.\n",
    "(fw[0] - np.mean(fw)) / np.std(fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling\n",
    "\n",
    "With min-max scaling, we can transform and scale our feature values such that each value is within the\n",
    "range of [0, 1]. Min-Max Scaler can be represented as $MMS(X_i)=\\frac{x_i - min(x)}{max(x) - min(x)}$, where we scale aach value in the feature $X$ by substracting it from the minimum value in the feature $min(X)$ and dividing the resultant by the difference between the maximum and minimum values in the feature $max(X)-min(X)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the MinMaxScaler.\n",
    "# MinMaxScaler is used to scale and translate each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "\n",
    "mms = # Your code goes here\n",
    "\n",
    "# Apply MinMaxScaler to the 'fpkms' column of the 'fpkms' DataFrame.\n",
    "# .fit_transform() method first computes the minimum and maximum values of the 'fpkms' column and then scales the column to the range [0, 1].\n",
    "# The result is assigned to a new column named 'minmax' in the 'fpkms' DataFrame.\n",
    "\n",
    "fpkms[\"minmax\"] = # Your code goes here\n",
    "\n",
    "# Display the 'fpkms' DataFrame after applying MinMaxScaler.\n",
    "# This will show the original 'fpkms' column and the newly created 'minmax' column containing the scaled values.\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can manually use the formula to compute the same result\n",
    "# Normalizing the first element of the array `fw` using min-max scaling\n",
    "# Formula: (x - min) / (max - min)\n",
    "# This scales the value to a range between 0 and 1\n",
    "(fw[0] - np.min(fw)) / (np.max(fw) - np.min(fw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaling\n",
    "\n",
    "The disadvantage of min-max scaling is that often the presence of outliers affects the scaled values for any\n",
    "feature. Robust scaling tries to use specific statistical measures to scale features without being affected by\n",
    "outliers. Mathematically this scaler can be represented as $\\frac{x_i - median(x)}{IQR_{(1,3)}(x)}$, where we scale each value of feature $X$ by subtracting the median of $X$ and dividing the resultant by the IQR (Inter-Quartile Range) of $X$ which is the range (difference) between the first quartile (25th percentile) and the third quartile (75th percentile).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RobustScaler instance to scale the 'fpkms' column\n",
    "\n",
    "rs = # Your code goes here\n",
    "\n",
    "# Apply RobustScaler transformation to the 'fpkms' column and store the result in a new column 'robust'\n",
    "# RobustScaler is useful for handling outliers since it scales data based on the interquartile range (IQR)\n",
    "\n",
    "fpkms[\"robust\"] = # Your code goes here\n",
    "\n",
    "# Display the modified DataFrame with the new 'robust' column\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Feature scaling is essential for many machine learning algorithms, particularly those sensitive to feature magnitudes like linear and logistic regression. We explored three powerful scaling techniques:\n",
    "\n",
    "- Z-score scaling for standardization\n",
    "- Min-Max scaling for bounded ranges\n",
    "- Robust scaling for handling outliers\n",
    "\n",
    "Each method serves specific purposes and should be chosen based on your data characteristics and model requirements.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
