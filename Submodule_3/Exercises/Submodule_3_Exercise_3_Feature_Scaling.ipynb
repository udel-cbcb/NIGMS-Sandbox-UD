{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling Exercise Solution\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Data processing is a critical step in building Machine Learning systems, requiring both domain knowledge and mathematical transformations. Feature scaling helps prevent model bias toward features with high magnitude values and is especially important when trying multiple Machine Learning algorithms.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the importance of feature scaling in machine learning\n",
    "- Learn three key feature scaling techniques:\n",
    "  - Standardized Scaling (Z-score)\n",
    "  - Min-Max Scaling\n",
    "  - Robust Scaling\n",
    "- Apply feature scaling methods using scikit-learn preprocessing tools\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Apply `StandardScaler` to normalize data using Z-score method\n",
    "- Implement Min-Max scaling to transform features to [0,1] range\n",
    "- Use `RobustScaler` for scaling data with outliers\n",
    "- Compare results across different scaling methods\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python programming environment\n",
    "- Basic understanding of statistical and machine learning concepts\n",
    "- Familiarity with common ML libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "### Set up conda environment\n",
    "\n",
    "Ensure that you have created then conda environment using the `conda_env.yml` file included in this repository. E.g.,\n",
    "\n",
    "```\n",
    "# Create conda environment\n",
    "conda env create -f conda_env.yml\n",
    "\n",
    "# Register the kernel\n",
    "python -m ipykernel install --user \\\n",
    "    --name=nigms_sandbox_ud \\\n",
    "    --display-name \"Python (NIGMS Sandbox UD)\"\n",
    "```\n",
    "\n",
    "Then, when starting the notebook, select the `\"Python (NIGMS Sandbox UD)\"` kernel from the list.\n",
    "\n",
    "Note that you may need to restart Jupyter Lab for these changes to take effect.\n",
    "\n",
    "### Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Using the raw values as input features might make models biased\n",
    "toward features having really high magnitude values. These models are typically sensitive to the magnitude or\n",
    "scale of features like linear or logistic regression. Other models like tree based methods can still work without\n",
    "feature scaling. However it is still recommended to normalize and scale down the features with feature scaling,\n",
    "especially if you want to try out multiple Machine Learning algorithms on input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample RNASeq data\n",
    "\n",
    "We have five genes with their RNASeq FPKMs (Fragments Per Kilobase of exon per Million reads). It is quite evident that some genes have\n",
    "been expressed a lot more than the others, giving a rise to values of high scale and magnitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpkms = pd.DataFrame([1295.0, 25.0, 19000.0, 5.0, 1.0, 300.0], columns=[\"fpkms\"])\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized Scaling\n",
    "\n",
    "The standard scaler tries to standardize each value in a feature column by removing the mean and scaling\n",
    "the variance to be 1 from the values. This is also known as centering and scaling and can be denoted\n",
    "mathematically as $SS(X_i) = \\frac{x_i - \\mu}{\\sigma}$, where each value in feature $X$ is subtracted by the mean $\\mu_i$ and the resultant is divided by the standard deviation $\\sigma_x$. This is also known as $Z$-scsore scaling. We can aslo divide the resultant by the variance instead of the standard deviation if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler\n",
    "ss = ...  # Your code goes here\n",
    "# Add a 'zscore' column\n",
    "fpkms[\"zscore\"] = ...  # Your code goes here\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can manually use the formula to compute the same result\n",
    "fw = np.array(fpkms[\"fpkms\"])\n",
    "(fw[0] - np.mean(fw)) / np.std(fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling\n",
    "\n",
    "With min-max scaling, we can transform and scale our feature values such that each value is within the\n",
    "range of [0, 1]. Min-Max Scaler can be represented as $MMS(X_i)=\\frac{x_i - min(x)}{max(x) - min(x)}$, where we scale aach value in the feature $X$ by substracting it from the minimum value in the feature $min(X)$ and dividing the resultant by the difference between the maximum and minimum values in the feature $max(X)-min(X)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler\n",
    "mms = ...  # Your code goes here\n",
    "fpkms[\"minmax\"] = ...  # Your code goes here\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can manually use the formula to compute the same result\n",
    "(fw[0] - np.min(fw)) / (np.max(fw) - np.min(fw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaling\n",
    "\n",
    "The disadvantage of min-max scaling is that often the presence of outliers affects the scaled values for any\n",
    "feature. Robust scaling tries to use specific statistical measures to scale features without being affected by\n",
    "outliers. Mathematically this scaler can be represented as $\\frac{x_i - median(x)}{IQR_{(1,3)}(x)}$, where we scale each value of feature $X$ by subtracting the median of $X$ and dividing the resultant by the IQR (Inter-Quartile Range) of $X$ which is the range (difference) between the first quartile (25th percentile) and the third quartile (75th percentile).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RobustScaler\n",
    "rs = ...  # Your code goes here\n",
    "fpkms[\"robust\"] = ...  # Your code goes here\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Feature scaling is essential for many machine learning algorithms, particularly those sensitive to feature magnitudes like linear and logistic regression. We explored three powerful scaling techniques:\n",
    "\n",
    "- Z-score scaling for standardization\n",
    "- Min-Max scaling for bounded ranges\n",
    "- Robust scaling for handling outliers\n",
    "\n",
    "Each method serves specific purposes and should be chosen based on your data characteristics and model requirements.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
