{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Exercise\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module covers feature selection strategies in machine learning, focusing on the main approaches used to identify and select the most relevant features for model building.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the three main categories of feature selection methods\n",
    "  - Filter methods\n",
    "  - Wrapper methods\n",
    "  - Embedded methods\n",
    "- Learn how different feature selection strategies evaluate and select features\n",
    "- Recognize when to apply different feature selection approaches\n",
    "\n",
    "### Tasks to be completed\n",
    "\n",
    "- Review different feature selection methodologies\n",
    "- Compare filter, wrapper and embedded approaches\n",
    "- Understand the tradeoffs between selection methods\n",
    "- Practice implementing feature selection techniques\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python programming environment\n",
    "- Basic understanding of statistical and machine learning concepts\n",
    "- Familiarity with common ML libraries\n",
    "\n",
    "## Get Started\n",
    "\n",
    "### Set up conda environment\n",
    "\n",
    "### Set up conda environment\n",
    "\n",
    "Ensure that you have created then conda environment using the `conda_env.yml` file included in this repository. E.g.,\n",
    "\n",
    "```\n",
    "# Create conda environment\n",
    "conda env create -f conda_env.yml\n",
    "\n",
    "# Register the kernel\n",
    "python -m ipykernel install --user \\\n",
    "    --name=nigms_sandbox_ud \\\n",
    "    --display-name \"Python (NIGMS Sandbox UD)\"\n",
    "```\n",
    "\n",
    "Then, when starting the notebook, select the `\"Python (NIGMS Sandbox UD)\"` kernel from the list.\n",
    "\n",
    "Note that you may need to restart Jupyter Lab for these changes to take effect.\n",
    "\n",
    "### Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies and settings\n",
    "import warnings  # Import the warnings module to manage and control warning messages during script execution.\n",
    "\n",
    "import numpy as np  # Import the NumPy library and alias it as 'np' for numerical operations, especially for handling arrays and matrices.\n",
    "import pandas as pd  # Import the pandas library and alias it as 'pd' for data manipulation and analysis, particularly using DataFrames.\n",
    "from sklearn.datasets import load_breast_cancer  # Import the load_breast_cancer function from scikit-learn to load the breast cancer dataset for demonstration or testing.\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import the RandomForestClassifier class from scikit-learn for using Random Forest, an ensemble learning method for classification.\n",
    "from sklearn.feature_selection import RFE, SelectKBest, VarianceThreshold, chi2  # Import feature selection techniques from scikit-learn:\n",
    "#   - RFE (Recursive Feature Elimination) for feature ranking and selection by recursively fitting a model and removing the weakest features.\n",
    "#   - SelectKBest for feature selection based on univariate statistical tests, selecting the top k features.\n",
    "#   - VarianceThreshold for feature selection by removing features with low variance.\n",
    "#   - chi2 for chi-squared statistic, often used for feature selection in classification problems with non-negative features.\n",
    "from sklearn.linear_model import LogisticRegression  # Import the LogisticRegression class from scikit-learn for using logistic regression, a linear model for binary classification.\n",
    "from sklearn.model_selection import cross_val_score  # Import the cross_val_score function from scikit-learn for evaluating model performance using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import _E. coli_ Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the Ecoli dataset\n",
    "ecoli_data = \"../../Data/ecoli.csv\"\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv(ecoli_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecoli dataset is for predicting Protein Localization Sites in Ecoli.\n",
    "\n",
    "```\n",
    "Number of Instances:  336\n",
    "Number of Attributes: 8 ( 7 predictive, 1 name )\n",
    "Attribute Information.\n",
    "  1. Sequence Name: Accession number for the SWISS-PROT database\n",
    "  2. mcg: McGeoch's method for signal sequence recognition.\n",
    "  3. gvh: von Heijne's method for signal sequence recognition.\n",
    "  4. lip: von Heijne's Signal Peptidase II consensus sequence score (Binary attribute).\n",
    "  5. chg: Presence of charge on N-terminus of predicted lipoproteins (Binary attribute).\n",
    "  6. aac: score of discriminant analysis of the amino acid content of outer membrane and periplasmic proteins.\n",
    "  7. alm1: score of the ALOM membrane spanning region prediction program.\n",
    "  8. alm2: score of ALOM program after excluding putative cleavable signal regions from the sequence.\n",
    "Missing Attribute Values: None.\n",
    "Class Distribution. The class is the localization site.\n",
    "  cp  (cytoplasm)                                    143\n",
    "  im  (inner membrane without signal sequence)        77\n",
    "  pp  (perisplasm)                                    52\n",
    "  imU (inner membrane, uncleavable signal sequence)   35\n",
    "  om  (outer membrane)                                20\n",
    "  omL (outer membrane lipoprotein)                     5\n",
    "  imL (inner membrane lipoprotein)                     2\n",
    "  imS (inner membrane, cleavable signal sequence)      2\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "- Ecoli Dataset ([ecoli.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ecoli.data))\n",
    "- Ecoli Dataset Description ([ecoli.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ecoli.names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Feature selection strategies can be divided into three main areas based on the type of strategy and\n",
    "techniques employed:\n",
    "\n",
    "- **Filter methods**: select features purely based on metrics like\n",
    "  correlation, mutual information and so on. Popular methods include threshold based\n",
    "  methods and statistical tests.\n",
    "- **Wrapper methods**: capture interaction between multiple\n",
    "  features by using a recursive approach to build multiple models using feature\n",
    "  subsets and select the best subset of features giving us the best performing model.\n",
    "  Methods like backward selecting and forward elimination are popular wrapper\n",
    "  based methods.\n",
    "- **Embedded methods**: combine the benefits of the other\n",
    "  two methods by leveraging Machine Learning models themselves to rank and score\n",
    "  feature variables based on their importance. Tree based methods like decision trees\n",
    "  and ensemble methods like random forests are popular examples of embedded\n",
    "  methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure NumPy to suppress scientific notation when printing floating point numbers.\n",
    "# This ensures that numbers close to zero are displayed as 0 instead of in scientific notation.\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Retrieve the current print options and store the value of the \"threshold\" setting.\n",
    "# The threshold determines the number of array elements printed before summarization with \"...\".\n",
    "pt = np.get_printoptions()[\"threshold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold based methods\n",
    "\n",
    "This is a filter based feature selection strategy, where you can use some form of cut-off or thresholding for\n",
    "limiting the total number of features during feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance based thresholding\n",
    "\n",
    "Another way of using thresholds is to use variance based thresholding where features having low\n",
    "variance (below a user-specified threshold) are removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape (number of rows and columns) of the DataFrame\n",
    "df.shape  # Returns a tuple (num_rows, num_columns)\n",
    "\n",
    "# Convert the categorical variable \"site\" into dummy/indicator variables,\n",
    "# creating a separate binary column for each unique category in \"site\".\n",
    "ecoli_site = pd.get_dummies(df[\"site\"])\n",
    "\n",
    "# Display the first few rows of the newly created dummy variables\n",
    "ecoli_site.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VarianceThreshold object to filter out low-variance features  \n",
    "# Features with a variance lower than 0.15 will be removed  \n",
    "\n",
    "vt = # Your code goes here\n",
    "\n",
    "# Fit the VarianceThreshold object to the dataset (ecoli_site)  \n",
    "# This calculates the variance of each feature  \n",
    "vt.fit(ecoli_site)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display the variance of each feature and whether it was selected.  \n",
    "pd.DataFrame(\n",
    "    {   \n",
    "        \"variance\": vt.variances_,  # Store the variance values of the features.\n",
    "        \"select_feature\": vt.get_support()  # Boolean mask indicating selected features.\n",
    "    },  \n",
    "    index=ecoli_site.columns,  # Use feature names as index for better readability.\n",
    ").T  # Transpose the DataFrame to display features as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final subset of selected features  \n",
    "# `vt.get_support()` returns a boolean mask indicating which features were selected  \n",
    "# `iloc[:, vt.get_support()]` filters the dataset to keep only the selected features  \n",
    "# `head()` retrieves the first few rows for preview  \n",
    "ecoli_site_subset = ecoli_site.iloc[:, vt.get_support()].head()  \n",
    "ecoli_site_subset  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is known as the Wisconsin\n",
    "Diagnostic Breast Cancer dataset, which is also available in its native or raw format at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic), which is the UCI Machine Learning\n",
    "repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset from sklearn\n",
    "bc_data = load_breast_cancer()\n",
    "\n",
    "# Convert feature data into a Pandas DataFrame with appropriate column names\n",
    "bc_features = pd.DataFrame(bc_data.data, columns=bc_data.feature_names)\n",
    "\n",
    "# Convert target labels into a DataFrame with a descriptive column name\n",
    "bc_classes = pd.DataFrame(bc_data.target, columns=[\"IsMalignant\"])\n",
    "\n",
    "# Build the feature set (X) and response class labels (y)\n",
    "bc_X = np.array(bc_features)  # Convert features DataFrame to a NumPy array\n",
    "bc_y = np.array(bc_classes).T[0]  # Convert target labels to a NumPy array and flatten it\n",
    "\n",
    "# Print the shape of the feature set and response class labels for verification\n",
    "print(\"Feature set shape:\", bc_X.shape)\n",
    "print(\"Response class shape:\", bc_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the print threshold to limit the number of array elements displayed  \n",
    "np.set_printoptions(threshold=30)\n",
    "\n",
    "# Print the shape of the feature set  \n",
    "print(\"Feature set data [shape: \" + str(bc_X.shape) + \"]\")\n",
    "\n",
    "# Print the feature set with values rounded to 2 decimal places  \n",
    "print(np.round(bc_X, 2), \"\\n\")\n",
    "\n",
    "# Print the feature names  \n",
    "print(\"Feature names:\")\n",
    "print(np.array(bc_features.columns), \"\\n\")\n",
    "\n",
    "# Print the shape of the predictor class label data  \n",
    "print(\"Predictor Class label data [shape: \" + str(bc_y.shape) + \"]\")\n",
    "\n",
    "# Print the predictor class label data  \n",
    "print(bc_y, \"\\n\")\n",
    "\n",
    "# Print the predictor class name  \n",
    "print(\"Predictor name:\", np.array(bc_classes.columns))\n",
    "\n",
    "# Reset the print threshold to its previous value (stored in variable 'pt')  \n",
    "np.set_printoptions(threshold=pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response class variable is a binary\n",
    "class where 1 indicates the tumor detected was benign and 0 indicates it was malignant. We can also see\n",
    "the 30 features that are real valued numbers that describe characteristics of cell nuclei present in digitized\n",
    "images of breast mass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chi-square test to evaluate the importance of features  \n",
    "# and select the top 15 best features out of the 30 available features.\n",
    "\n",
    "skb = # Your code goes here\n",
    "\n",
    "# Fit the SelectKBest model to the feature matrix (bc_X) and target variable (bc_y)  \n",
    "# to determine the most relevant features based on the chi-square test.\n",
    "skb.fit(bc_X, bc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples where each tuple contains a feature name and its corresponding score\n",
    "feature_scores = [\n",
    "    (item, score) for item, score in zip(bc_data.feature_names, skb.scores_)\n",
    "]\n",
    "\n",
    "# Sort the list of feature scores in descending order based on the score value (x[1])\n",
    "# and return the top 10 most relevant features\n",
    "sorted(feature_scores, key=lambda x: -x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the selected features obtained from our original feature set \n",
    "# using the chi-square test (via SelectKBest)\n",
    "select_features_kbest = skb.get_support()  # Get the mask of selected features (True for selected)\n",
    "feature_names_kbest = bc_data.feature_names[select_features_kbest]  # Get feature names corresponding to selected features\n",
    "\n",
    "# Create a DataFrame with the selected features using the column names from the original feature set\n",
    "feature_subset_df = bc_features[feature_names_kbest]\n",
    "\n",
    "# Convert the DataFrame of selected features into a numpy array for further processing\n",
    "bc_SX = np.array(feature_subset_df)\n",
    "\n",
    "# Print the shape of the resulting feature subset array (rows, columns)\n",
    "print(bc_SX.shape)\n",
    "\n",
    "# Print the names of the selected features\n",
    "print(feature_names_kbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a feature subset of the Wisconsin Diagnostic Breast Cancer dataset using chi-square tests\n",
    "# The 'feature_subset_df' contains the result of feature selection based on chi-square tests\n",
    "# We are selecting a specific subset of rows (from row 20 to 25) and rounding the values to 2 decimal places\n",
    "np.round(feature_subset_df.iloc[20:25], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s now build a simple\n",
    "classification model using logistic regression on the original feature set of 30 features and compare the\n",
    "model accuracy performance with another model built using our selected 15 features. For model evaluation,\n",
    "we will use the accuracy metric (percent of correct predictions) and use a five-fold cross-validation scheme. The main idea here is to compare the model\n",
    "prediction performance between models trained on different feature sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings to keep the output clean (e.g., avoid displaying convergence warnings)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Build a Logistic Regression model with a maximum of 1000 iterations for convergence\n",
    "\n",
    "lr = # Your code goes here\n",
    "\n",
    "# Evaluate accuracy of the model built on the complete feature set using 5-fold cross-validation\n",
    "# 'cross_val_score' performs cross-validation and calculates accuracy for each fold\n",
    "full_feat_acc = np.average(cross_val_score(lr, bc_X, bc_y, scoring=\"accuracy\", cv=5))\n",
    "\n",
    "# Evaluate accuracy of the model built on the selected feature set using 5-fold cross-validation\n",
    "sel_feat_acc = np.average(cross_val_score(lr, bc_SX, bc_y, scoring=\"accuracy\", cv=5))\n",
    "\n",
    "# Print out the results\n",
    "print(\"Model accuracy statistics with 5-fold cross validation\")\n",
    "# Output the accuracy of the model using the full feature set, along with the shape of the input features\n",
    "print(\"Model accuracy with complete feature set\", bc_X.shape, \":\", full_feat_acc)\n",
    "# Output the accuracy of the model using the selected feature set, along with the shape of the input features\n",
    "print(\"Model accuracy with selected feature set\", bc_SX.shape, \":\", sel_feat_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metrics clearly show us that we actually built a better model\n",
    "when trained on the selected 15 feature subset as compared to the model built with the original 30 features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "Recursive Feature Elimination, also known as RFE, is a popular wrapper based feature selection technique,\n",
    "which allows you to recursively keep eliminating lower scored features till you arrive at the specific feature subset count. The basic idea is to start off with a specific Machine Learning estimator\n",
    "like the Logistic Regression algorithm we used for our classification needs. Next we take the entire feature set\n",
    "of 30 features and the corresponding response class variables. RFE aims to assign weights to these features\n",
    "based on the model fit. Features with the smallest weights are pruned out and then a model is fit again on the remaining features to obtain the new weights or scores. This process is recursively carried out multiple\n",
    "times and each time features with the lowest scores/weights are eliminated, until the pruned feature subset\n",
    "contains the desired number of features that the user wanted to select (this is taken as an input parameter at\n",
    "the start). This strategy is also popularly known as backward elimination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model to be used for feature selection\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Initialize RFE (Recursive Feature Elimination) with:\n",
    "# - the Logistic Regression model (lr) as the estimator\n",
    "# - n_features_to_select=15, meaning we want to select the top 15 features\n",
    "# - step=1, which means we remove one feature at a time in each iteration\n",
    "\n",
    "rfe = # Your code goes here\n",
    "\n",
    "# Fit the RFE model on the breast cancer dataset (X: features, y: target)\n",
    "rfe.fit(bc_X, bc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the final selected features from RFE (Recursive Feature Elimination)\n",
    "select_features_rfe = rfe.get_support()  # Get a boolean array of selected features (True for selected, False for non-selected)\n",
    "\n",
    "# Extract the names of the features that were selected by RFE\n",
    "feature_names_rfe = bc_data.feature_names[select_features_rfe]  # Filter the feature names based on the selected ones\n",
    "\n",
    "# Print the names of the selected features\n",
    "print(feature_names_rfe)\n",
    "\n",
    "# Fit the RFE model to the data (this step might be part of the RFE process, not needed if already fitted)\n",
    "rfe.fit(bc_X, bc_y)  # Fit the RFE model to the input data (bc_X) and target labels (bc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the feature subset selected using SelectKBest (feature_names_kbest) \n",
    "# with the feature subset selected using Recursive Feature Elimination (RFE) \n",
    "# (feature_names_rfe). The intersection will show the common features between \n",
    "# these two subsets.\n",
    "set(feature_names_kbest) & set(feature_names_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model based selection\n",
    "\n",
    "Tree based models like decision trees and ensemble models like random forests (ensemble of trees) can\n",
    "be utilized not just for modeling alone but for feature selection. These models can be used to compute\n",
    "feature importances when building the model that can in turn be used for selecting the best features and\n",
    "discarding irrelevant features with lower scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the RandomForestClassifier model from sklearn\n",
    "# Initialize a RandomForestClassifier object\n",
    "\n",
    "rfc = # Your code goes here  \n",
    "\n",
    "# Fitting the model on the training data (bc_X for features, bc_y for labels)\n",
    "rfc.fit(bc_X, bc_y)  # Train the random forest model on the given feature set (bc_X) and target labels (bc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random forest estimator to score the features based on their importance\n",
    "# The `rfc.feature_importances_` attribute provides the importance scores for each feature\n",
    "importance_scores = rfc.feature_importances_\n",
    "\n",
    "# Create a list of tuples where each tuple contains a feature name and its corresponding importance score\n",
    "feature_importances = [\n",
    "    (feature, score) for feature, score in zip(bc_data.feature_names, importance_scores)\n",
    "]\n",
    "\n",
    "# Sort the list of feature importances by the score in descending order and select the top 10 features\n",
    "# The `lambda x: -x[1]` ensures the list is sorted in descending order based on the score\n",
    "sorted(feature_importances, key=lambda x: -x[1])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use a threshold based parameter to filter out the top n features as needed or you can even\n",
    "make use of the SelectFromModel meta-transformer provided by scikit-learn by using it as a wrapper on\n",
    "top of this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Feature selection is a critical step in machine learning that helps identify the most relevant features for model building. We learned about three main approaches:\n",
    "\n",
    "- Filter methods that use metrics and statistical tests\n",
    "- Wrapper methods that evaluate feature subsets recursively\n",
    "- Embedded methods that combine benefits of other approaches using ML models\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
