{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Exercise (Solution)\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Feature engineering is a crucial step in developing effective Machine Learning systems, blending domain expertise with mathematical transformations. It focuses on processing diverse data types and variables, with each Machine Learning problem demanding tailored feature engineering strategies. This module explores techniques for engineering both **numeric** and **categorical** features.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Transform and engineer numeric features\n",
    "  - Apply raw measures and counts\n",
    "  - Implement binarization techniques\n",
    "  - Perform rounding operations\n",
    "  - Create feature interactions\n",
    "- Transform and engineer categorical features\n",
    "  - Convert nominal features to numeric representations\n",
    "  - Transform ordinal features with preserved ordering\n",
    "  - Apply encoding schemes for categorical data\n",
    "    - One Hot Encoding\n",
    "    - Dummy Coding\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Implement numeric feature engineering techniques\n",
    "- Transform categorical variables\n",
    "- Apply various encoding schemes\n",
    "- Analyze transformed features\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python programming environment\n",
    "- Basic understanding of statistical and machine learning concepts\n",
    "- Familiarity with common ML libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "- Please select kernel \"conda_python3\" from SageMaker notebook instance.\n",
    "\n",
    "### Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "\n",
    "# Matplotlib for plotting and visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy for numerical operations and array manipulations\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# SciPy statistical functions for advanced statistical analysis\n",
    "import scipy.stats as spstats\n",
    "\n",
    "# Scikit-learn preprocessing tools for data transformation\n",
    "from sklearn.preprocessing import (\n",
    "    Binarizer,           # Converts numerical values into binary (0 or 1) based on a threshold\n",
    "    LabelEncoder,        # Encodes categorical labels as integers (useful for classification tasks)\n",
    "    OneHotEncoder,       # Encodes categorical variables as one-hot (dummy) variables\n",
    "    PolynomialFeatures,  # Generates polynomial features for regression models\n",
    ")\n",
    "\n",
    "\n",
    "# Enable inline plotting in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Reload Matplotlib's style library to ensure the latest settings are applied\n",
    "mpl.style.reload_library()\n",
    "\n",
    "# Set the Matplotlib style to \"classic\" for a traditional look\n",
    "mpl.style.use(\"classic\")\n",
    "\n",
    "# Set the background color of figures to transparent (white with 0 alpha)\n",
    "mpl.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\n",
    "\n",
    "# Define the default figure size as 6 inches by 4 inches\n",
    "mpl.rcParams[\"figure.figsize\"] = [6.0, 4.0]\n",
    "\n",
    "# Set the figure resolution to 100 dots per inch (DPI) for better clarity\n",
    "mpl.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on Numeric Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While machine learning algorithms can process raw numerical data directly, effective modeling typically requires deliberate feature engineering to create meaningful representations aligned with the problem domain. For numerical features, two critical properties demand attention: scale (relative magnitude of values) and distribution (underlying statistical shape). Proper scaling ensures features contribute equally to distance-based calculations (e.g., k-NN, SVM), while distribution adjustments—such as normalizing skewed variables via log/power transforms—can improve performance for algorithms assuming Gaussian-like inputs (e.g., linear regression). These transformations are not merely algorithmic prerequisites but domain-specific design choices; for instance, financial models may intentionally preserve scale for interpretability, whereas image processing pipelines might aggressively normalize pixel intensities. The art of feature engineering lies in balancing mathematical soundness with problem context to extract maximal signal from numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Measures\n",
    "\n",
    "Raw measures constitute the most fundamental form of feature representation, where numeric variables are incorporated into machine learning models in their original, untransformed state. These features preserve the exact values as recorded in the source data - whether as continuous measurements, discrete counts, or absolute quantities - without undergoing normalization, scaling, or other engineering processes. While this approach maintains maximum fidelity to the initial data collection, it may introduce challenges when variables operate on vastly different scales or units, potentially biasing algorithms sensitive to feature magnitudes. The use of raw measures is particularly common in domains requiring strict interpretability of input variables, or when the native scales themselves carry meaningful information for the predictive task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Values\n",
    "\n",
    "Scalar values in their raw form represent individual measurements, metrics, or observations tied to specific variables, where the meaning and context of each value are typically inferred from the field name or, when available, through reference to a comprehensive data dictionary that provides formal definitions, units of measurement, and other relevant metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecoli Dataset\n",
    "\n",
    "Ecoli dataset is for predicting Protein Localization Sites in Ecoli.\n",
    "\n",
    "```\n",
    "Number of Instances:  336\n",
    "Number of Attributes: 8 ( 7 predictive, 1 name )\n",
    "Attribute Information.\n",
    "  1. Sequence Name: Accession number for the SWISS-PROT database\n",
    "  2. mcg: McGeoch's method for signal sequence recognition.\n",
    "  3. gvh: von Heijne's method for signal sequence recognition.\n",
    "  4. lip: von Heijne's Signal Peptidase II consensus sequence score (Binary attribute).\n",
    "  5. chg: Presence of charge on N-terminus of predicted lipoproteins (Binary attribute).\n",
    "  6. aac: score of discriminant analysis of the amino acid content of outer membrane and periplasmic proteins.\n",
    "  7. alm1: score of the ALOM membrane spanning region prediction program.\n",
    "  8. alm2: score of ALOM program after excluding putative cleavable signal regions from the sequence.\n",
    "Missing Attribute Values: None.\n",
    "Class Distribution. The class is the localization site.\n",
    "  cp  (cytoplasm)                                    143\n",
    "  im  (inner membrane without signal sequence)        77\n",
    "  pp  (perisplasm)                                    52\n",
    "  imU (inner membrane, uncleavable signal sequence)   35\n",
    "  om  (outer membrane)                                20\n",
    "  omL (outer membrane lipoprotein)                     5\n",
    "  imL (inner membrane lipoprotein)                     2\n",
    "  imS (inner membrane, cleavable signal sequence)      2\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "- Ecoli Dataset ([ecoli.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ecoli.data))\n",
    "- Ecoli Dataset Description ([ecoli.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ecoli.names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to the Ecoli dataset (relative path)\n",
    "ecoli_data = \"../../Data/ecoli.csv\"\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "ecoli_df = pd.read_csv(ecoli_data)\n",
    "\n",
    "# Display the first 10 rows of the dataset to inspect its structure\n",
    "ecoli_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the selected feature columns (\"mcg\", \"gvh\", \"chg\") \n",
    "# from the ecoli dataset\n",
    "ecoli_df[[\"mcg\", \"gvh\", \"chg\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute basic statistical measures (count, mean, std, min, max, and quartiles)\n",
    "# for the numerical columns 'mcg', 'gvh', and 'chg' in the DataFrame 'ecoli_df'\n",
    "ecoli_df[[\"mcg\", \"gvh\", \"chg\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts\n",
    "\n",
    "Numeric variables often directly encode quantitative measurements of events or characteristics, serving as fundamental representations of counts (e.g., customer transactions), frequencies (e.g., word occurrences in documents), or binary occurrences (e.g., presence/absence of symptoms). These raw numerical measures provide objective, machine-readable data that capture discrete phenomena without requiring transformation. However, their interpretation often requires contextual understanding - a count of '5' might represent 5 products purchased (a meaningful magnitude) or simply a binary 'yes' coded as 1 (where only presence matters). Proper documentation of what these numbers represent is essential for accurate analysis, as the same numerical format can convey fundamentally different types of information depending on the underlying measurement paradigm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes Dataset\n",
    "\n",
    "The dataset classifies patient data as\n",
    "either an onset of diabetes within five years or not.\n",
    "\n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class\n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "- Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "- Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Diabetes dataset from a CSV file\n",
    "diabetes_data = \"../../Data/pima-indians-diabetes.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame, specifying no header row (header=None)\n",
    "diabetes_df = pd.read_csv(diabetes_data, header=None)\n",
    "\n",
    "# Assign column names to the dataset for better readability\n",
    "diabetes_df.columns = [\n",
    "    \"pregnancy\",  # Number of times pregnant\n",
    "    \"glucose\",    # Plasma glucose concentration\n",
    "    \"bp\",         # Diastolic blood pressure (mm Hg)\n",
    "    \"triceps\",    # Triceps skinfold thickness (mm)\n",
    "    \"insulin\",    # 2-Hour serum insulin (mu U/ml)\n",
    "    \"bmi\",        # Body Mass Index (weight in kg/(height in m)^2)\n",
    "    \"pedigree\",   # Diabetes pedigree function (genetic risk factor)\n",
    "    \"age\",        # Age in years\n",
    "    \"diabetes\",   # Diabetes diagnosis (1 = positive, 0 = negative)\n",
    "]\n",
    "\n",
    "# Display the first 10 rows of the dataset\n",
    "diabetes_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization\n",
    "\n",
    "Binarization is a preprocessing technique that converts continuous numerical values into binary outputs (0 or 1) based on a specified threshold. In code implementation below:\n",
    "* The Binarizer from scikit-learn transforms the 'age' feature using 50 as the decision boundary\n",
    "* Ages ≤ 50 become 0 (representing \"not old\")\n",
    "* Ages > 50 become 1 (representing \"old\")\n",
    "\n",
    "The operation preserves the original data structure while creating a new binary column ('bn_old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'age' column to a NumPy array for easier manipulation\n",
    "age = np.array(diabetes_df[\"age\"])\n",
    "\n",
    "# Create a copy of the 'age' array to store the binarized values\n",
    "old = np.array(diabetes_df[\"age\"])\n",
    "\n",
    "# Assign 1 to individuals older than 50\n",
    "old[age > 50] = 1\n",
    "\n",
    "# Assign 0 to individuals aged 50 or younger\n",
    "old[age <= 50] = 0\n",
    "\n",
    "# Add the binarized 'old' column back to the DataFrame\n",
    "diabetes_df[\"old\"] = old\n",
    "\n",
    "# Display the first 10 rows of the updated DataFrame\n",
    "diabetes_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize 'age' field using Binarizer\n",
    "# This transformation converts numerical values into binary (0 or 1) based on a given threshold.\n",
    "\n",
    "# Initialize the Binarizer with a threshold of 50\n",
    "# Any age value greater than 50 will be mapped to 1, while 50 and below will be mapped to 0.\n",
    "bn = Binarizer(threshold=50)\n",
    "\n",
    "# Apply the transformation on the 'age' column of the diabetes dataset\n",
    "# Note: `Binarizer.transform()` expects a 2D array, so we wrap the column inside a list.\n",
    "bn_old = bn.transform([diabetes_df[\"age\"]])[0]  \n",
    "\n",
    "# Store the binarized values in a new column 'bn_old' in the DataFrame\n",
    "diabetes_df[\"bn_old\"] = bn_old\n",
    "\n",
    "# Display the first 10 rows of the updated DataFrame to verify the transformation\n",
    "diabetes_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rounding\n",
    "\n",
    "For numeric attributes representing proportions or percentages, excessive precision frequently offers diminishing returns. A pragmatic approach involves rounding these values to whole numbers, which serves dual purposes: the simplified integers can function either as (1) streamlined continuous variables that reduce computational noise or as (2) discrete categorical features that capture meaningful value bands. This transformation not only improves data manageability but may also enhance model interpretability without significant loss of predictive power, particularly when the original decimal precision exceeds measurement accuracy or business requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'pedigree_scale_10' by multiplying the 'pedigree' column by 10 \n",
    "# and rounding the values to the nearest integer, then converting them to integers\n",
    "diabetes_df[\"pedigree_scale_10\"] = np.array(\n",
    "    np.round((diabetes_df[\"pedigree\"] * 10)), dtype=\"int\"\n",
    ")\n",
    "\n",
    "# Create a new column 'pedigree_scale_100' by multiplying the 'pedigree' column by 100 \n",
    "# and rounding the values to the nearest integer, then converting them to integers\n",
    "diabetes_df[\"pedigree_scale_100\"] = np.array(\n",
    "    np.round((diabetes_df[\"pedigree\"] * 100)), dtype=\"int\"\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "diabetes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions\n",
    "\n",
    "In practical machine learning applications, explicitly creating interaction terms between features can significantly enhance model performance by capturing synergistic relationships that individual variables alone cannot express. These engineered features often reveal hidden patterns in real-world data where predictors combine non-additively, particularly in domains like healthcare (drug interactions), finance (portfolio effects), or engineering (system synergies). While some algorithms like deep neural networks can implicitly learn interactions, creating explicit cross-product features or using techniques like polynomial expansion often improves interpretability and boosts simpler models' predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the \"gvh\" (global protein localization) and \"lip\" (lipoprotein signal) columns from the ecoli_df DataFrame\n",
    "gvh_lip = ecoli_df[[\"gvh\", \"lip\"]]\n",
    "\n",
    "# Display the first 5 rows of the selected subset to inspect the data\n",
    "gvh_lip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of PolynomialFeatures with the following parameters:\n",
    "# degree=2: Generate features up to the second degree (squared features and interactions).\n",
    "# interaction_only=False: Allow both interaction terms (e.g., feature1*feature2) and polynomial terms (e.g., feature1^2).\n",
    "# include_bias=False: Exclude the bias column (column of ones) that represents the intercept term.\n",
    "pf = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "\n",
    "# Fit the PolynomialFeatures transformer on the dataset (gvh_lip) and then transform it.\n",
    "# This will generate new features that are combinations of the original features up to the specified degree.\n",
    "res = pf.fit_transform(gvh_lip)\n",
    "\n",
    "# Output the transformed feature set (the original features and their polynomial interactions).\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feature engineering, our final dataset comprises five predictive variables, including newly created interaction terms. These engineered features combine original variables to capture multiplicative relationships, expanding the model's capacity to identify non-linear patterns while maintaining a parsimonious feature set that balances predictive power and computational efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature degree matrix reveals the connectivity pattern of each variable, quantifying how strongly features interact within the modeled relationships. Higher-degree features indicate more numerous or influential connections in the underlying data structure, while lower-degree features may represent more isolated or niche predictors. This metric helps identify hub features that dominate relationships versus peripheral ones that contribute minimally to the system's topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pf.powers_ attribute to a Pandas DataFrame\n",
    "# 'pf.powers_' is assumed to be a NumPy array or similar structure containing power values.\n",
    "# The columns are labeled as 'gvh_degree' and 'lip_degree' for clarity.\n",
    "pd.DataFrame(pf.powers_, columns=[\"gvh_degree\", \"lip_degree\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the feature degrees now clearly mapped to their actual representations, we can assign meaningful names to each feature, transforming the generic dataset into an interpretable feature set. This naming convention enables clearer analysis and domain-specific insights while preserving all computational properties of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the list 'res' with specific column names\n",
    "intr_features = pd.DataFrame(res, columns=[\"gvh\", \"lip\", \"gvh^2\", \"gvh x lip\", \"lip^2\"])\n",
    "\n",
    "# Display the first 5 rows of the DataFrame to preview the data\n",
    "intr_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame containing sample observations for 'gvh' and 'lip' \n",
    "new_df = pd.DataFrame(\n",
    "    [[0.35, 0.49], [0.46, 0.38], [0.25, 0.48]], \n",
    "    columns=[\"gvh\", \"lip\"]  # Define column names as 'gvh' and 'lip' \n",
    ")\n",
    "\n",
    "# Display the newly created DataFrame\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pf object that we created earlier to transform the input features \n",
    "# and generate interaction features from the new data (new_df).\n",
    "new_res = pf.transform(new_df)\n",
    "\n",
    "# Convert the resulting interaction features (new_res) into a DataFrame \n",
    "# with column names representing the specific interaction terms and transformations.\n",
    "new_intr_features = pd.DataFrame(\n",
    "    new_res,  # The transformed features\n",
    "    columns=[\"gvh\", \"lip\", \"gvh^2\", \"gvh x lip\", \"lip^2\"]  # Assigning column names to the features\n",
    ")\n",
    "\n",
    "# Output the DataFrame containing the new interaction features\n",
    "new_intr_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on Categorical Data\n",
    "\n",
    "Categorical features represent discrete values from a finite set of possible categories, which may be expressed as either text labels or numeric codes. These variables are fundamentally classified into two types: **nominal** (unordered categories like colors or brands) and **ordinal** (ordered categories with inherent ranking like education levels or survey scales). The discrete nature of categorical data distinguishes it from continuous numerical values and requires specialized handling in statistical analysis and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Nominal Features\n",
    "\n",
    "Nominal features represent categorical variables with distinct, non-ordinal values (e.g., colors or cities). Since machine learning algorithms require numerical inputs, these string-based categories must be encoded into numeric representations. Common transformation techniques include one-hot encoding for low-cardinality features and target encoding for high-cardinality variables, each preserving different aspects of the categorical information while making it algorithm-compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 11 rows of the dataframe 'ecoli_df'\n",
    "ecoli_df.head(11)  # This will show the first 11 entries in the dataframe, including column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output displays the first 11 rows of the DataFrame ecoli_df, where each row represents a protein from E. coli, identified by its accession name (e.g., EMRA_ECOLI, AAT_ECOLI). The DataFrame includes 8 columns in total: 7 numerical features—mcg, gvh, lip, chg, aac, alm1, and alm2—which are used for classification, and 1 categorical target column, site, which indicates the predicted protein localization site (e.g., cp, im, om, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique values from the \"site\" column in the ecoli_df DataFrame\n",
    "sites = np.unique(ecoli_df[\"site\"])\n",
    "\n",
    "# Display the unique site values\n",
    "sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis reveals that the E.coli dataset contains samples from 8 unique site locations, as confirmed by distinct value counts in the site identifier feature. This categorical distribution is significant for ensuring representative sampling across different environmental contexts in subsequent modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder\n",
    "sle = LabelEncoder()\n",
    "\n",
    "# Encode the 'site' column in ecoli_df, converting categorical values into numeric labels\n",
    "site_labels = sle.fit_transform(ecoli_df[\"site\"])\n",
    "\n",
    "# Create a dictionary mapping each numeric label back to its original category\n",
    "site_mappings = {index: label for index, label in enumerate(sle.classes_)}\n",
    "\n",
    "# Display the mapping of encoded values to original categories\n",
    "site_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn's LabelEncoder (*sle*), we've encoded categorical site values into numerical representations, with the transformed integer labels stored in the site_labels array. This mapping scheme preserves the original categorical relationships while converting them to a format compatible with machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the list/array 'site_labels' to a new column 'siteLabel' in the dataframe 'ecoli_df'\n",
    "ecoli_df[\"siteLabel\"] = site_labels  \n",
    "\n",
    "# Display the first 11 rows of the dataframe to check the assigned labels\n",
    "ecoli_df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SiteLabel field contains the numerically encoded values corresponding to each site location, and our validation confirms these mappings precisely match the predefined label-to-integer assignments created during preprocessing. This consistency ensures accurate representation of categorical site data for downstream modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Ordinal Features\n",
    "\n",
    "Ordinal features share similarities with nominal features in their categorical nature, but differ critically through their meaningful value ordering. While both types often appear as text data requiring numerical encoding, ordinal features uniquely preserve this inherent order during transformation - a crucial distinction since the sequence itself carries predictive information that machine learning algorithms can leverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Generation based on 'age'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"age\" column of the diabetes dataframe to a NumPy array\n",
    "age = np.array(diabetes_df[\"age\"])\n",
    "\n",
    "# Create a new column \"Generation\" based on age groups using a lambda function\n",
    "diabetes_df[\"Generation\"] = diabetes_df[\"age\"].apply(\n",
    "    lambda value: (\n",
    "        \"Gen Z\" if value <= 25  # Ages 25 and below belong to Generation Z\n",
    "        else (\n",
    "            \"Millennials\" if value <= 41  # Ages 26-41 belong to Millennials\n",
    "            else (\n",
    "                \"Gen X\" if value <= 57  # Ages 42-57 belong to Generation X\n",
    "                else (\n",
    "                    \"Boomers II\" if value <= 67  # Ages 58-67 belong to Boomers II\n",
    "                    else (\n",
    "                        \"Boomers I\" if value <= 76  # Ages 68-76 belong to Boomers I\n",
    "                        else (\n",
    "                            \"Post WWII\" if value <= 94  # Ages 77-94 belong to Post-WWII generation\n",
    "                            else \"WWII\"  # Ages 95+ belong to WWII generation\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the first 10 rows of the \"age\" and \"Generation\" columns\n",
    "diabetes_df[[\"age\", \"Generation\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values from the \"Generation\" column of the diabetes_df DataFrame\n",
    "unique_generations = np.unique(diabetes_df[\"Generation\"])\n",
    "\n",
    "# Display the unique generation values\n",
    "print(unique_generations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains six distinct generations, representing an ordinal attribute with inherent sequential ordering. \n",
    "\n",
    "Since Python lacks native functionality for automated ordinal encoding of such features, we must manually implement the transformation. The following code demonstrates how to map these generational categories to their appropriate numerical representations while preserving the logical progression between values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map generation labels to ordinal values\n",
    "gen_ord_map = {\n",
    "    \"Gen Z\": 1,         # Youngest generation in the dataset\n",
    "    \"Millennials\": 2,   # Followed by Millennials\n",
    "    \"Gen X\": 3,         # Middle-aged generation\n",
    "    \"Boomers II\": 4,    # Late Boomers\n",
    "    \"Boomers I\": 5,     # Early Boomers\n",
    "    \"Post WWII\": 6,     # Oldest generation in the dataset\n",
    "}\n",
    "\n",
    "# Map the 'Generation' column in the diabetes dataset to corresponding ordinal values\n",
    "diabetes_df[\"GenerationLabel\"] = diabetes_df[\"Generation\"].map(gen_ord_map)\n",
    "\n",
    "# Display selected columns (age, original generation label, and mapped generation label)\n",
    "# for rows 4 to 9 (since slicing is exclusive of the end index)\n",
    "diabetes_df[[\"age\", \"Generation\", \"GenerationLabel\"]].iloc[4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BMI Class based on 'bmi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'bmi' column from the diabetes DataFrame and store it as a numpy array\n",
    "bmi = np.array(diabetes_df[\"bmi\"])\n",
    "\n",
    "# Create a new 'BMI' column in the dataframe by applying a function to the 'bmi' values\n",
    "diabetes_df[\"BMI\"] = diabetes_df[\"bmi\"].apply(\n",
    "    lambda value: (\n",
    "        \"Underweight\"  # If the BMI is less than or equal to 18.5, classify as Underweight\n",
    "        if value <= 18.5\n",
    "        else (\n",
    "            \"Normal\"  # If BMI is between 18.6 and 22.9, classify as Normal\n",
    "            if value <= 22.9\n",
    "            else (\n",
    "                \"Pre-obese\"  # If BMI is between 23 and 24.9, classify as Pre-obese\n",
    "                if value <= 24.9\n",
    "                else (\n",
    "                    \"Class I obesity\"  # If BMI is between 25 and 29.9, classify as Class I obesity\n",
    "                    if value <= 29.9\n",
    "                    else \"Class II obesity\"  # If BMI is between 30 and 34.9, classify as Class II obesity\n",
    "                    if value <= 34.9 \n",
    "                    else \"Class II obesity\"  # If BMI is greater than 35, classify as Class II obesity\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the first 10 rows of 'bmi' and the newly created 'BMI' column for review\n",
    "diabetes_df[[\"bmi\", \"BMI\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the 'BMI' column of the diabetes_df DataFrame\n",
    "# 'diabetes_df[\"BMI\"]' selects the BMI column from the dataframe\n",
    "# np.unique() returns the sorted unique values in the specified array\n",
    "unique_bmi_values = np.unique(diabetes_df[\"BMI\"])\n",
    "\n",
    "# Output the unique BMI values\n",
    "print(unique_bmi_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output reveals five naturally ordered BMI classes, confirming their ordinal nature. \n",
    "\n",
    "Since scikit-learn lacks built-in ordinal encoding functionality for such cases, we must manually implement the numeric transformation. The following code demonstrates how to create this custom mapping while preserving the class hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping BMI categories to ordinal values\n",
    "bmi_ord_map = {\n",
    "    \"Underweight\": 1,       # \"Underweight\" corresponds to 1\n",
    "    \"Normal\": 2,            # \"Normal\" corresponds to 2\n",
    "    \"Pre-obese\": 3,         # \"Pre-obese\" corresponds to 3\n",
    "    \"Class I obesity\": 4,   # \"Class I obesity\" corresponds to 4\n",
    "    \"Class II obesity\": 5,  # \"Class II obesity\" corresponds to 5\n",
    "}\n",
    "\n",
    "# Map the 'BMI' column in diabetes_df to its corresponding ordinal value using the bmi_ord_map dictionary\n",
    "diabetes_df[\"BMILabel\"] = diabetes_df[\"BMI\"].map(bmi_ord_map)\n",
    "\n",
    "# Display a subset of the dataframe (rows 4 to 9) showing 'bmi', 'BMI', and 'BMILabel' columns\n",
    "diabetes_df[[\"bmi\", \"BMI\", \"BMILabel\"]].iloc[4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show concerning patterns in BMI distribution and classification. Most entries (4/6) fall into Class II obesity (BMI ≥30), indicating a high-risk population sample. However, two critical issues emerge: First, the BMI value of 0.0 (ID 9) is physiologically impossible, suggesting either data entry error or missing values incorrectly coded as zero. Second, ID 5 (BMI=25.6) is classified as Class I obesity, which contradicts standard medical thresholds where obesity typically begins at BMI ≥30. The ordinal labels (BMILabel column) correctly reflect severity progression from Underweight (1) to Class II obesity (5), but the underlying classifications appear inconsistent with clinical standards. This output warrants verification of both the raw data quality (particularly the 0.0 value) and the classification thresholds being applied.\n",
    "\n",
    "Since no existing Python module automatically handles such ordinal conversions, we implement custom transformation logic to properly encode these generational categories into their corresponding numerical values while preserving this inherent sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Features\n",
    "\n",
    "If we directly feed these transformed numeric representations of categorical features into an algorithm, the model will interpret them as raw numeric features. This introduces an incorrect notion of magnitude, as the numeric values do not inherently carry meaningful order or scale.\n",
    "\n",
    "As a result, models built using these features directly would be suboptimal and inaccurate. To address this, several strategies exist for creating dummy features, where each unique value or label from the distinct categories is represented separately. In the following sections, we will explore some of these strategies, including **one-hot encoding**, **dummy coding**, **effect coding**, and **feature hashing schemes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding Scheme\n",
    "\n",
    "For a categorical feature with **m** unique labels, the one-hot encoding scheme transforms the feature into **m** binary features, each of which can only take a value of **1** or **0**. Each observation in the categorical feature is converted into a vector of size **m**, where only one element is **1** (indicating the active category) and the rest are **0**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the DataFrame 'diabetes_df' with specific columns\n",
    "# - 'diabetes': The target variable indicating if the person has diabetes (e.g., 1 or 0)\n",
    "# - 'Generation': A categorical variable representing the generation group (e.g., Gen X, Millennial)\n",
    "# - 'BMI': A numerical variable for Body Mass Index (BMI)\n",
    "\n",
    "# Use 'iloc' to filter rows between index 4 and 9 (remember, Python is 0-indexed, so row 4 is included, row 10 is excluded)\n",
    "diabetes_df[[\"diabetes\", \"Generation\", \"BMI\"]].iloc[4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder for \"Generation\"\n",
    "gen_le = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder on the \"Generation\" column and transform it into numerical labels\n",
    "gen_labels = gen_le.fit_transform(diabetes_df[\"Generation\"])\n",
    "\n",
    "# Add the transformed \"Generation\" labels as a new column in the dataframe\n",
    "diabetes_df[\"Gen_Label\"] = gen_labels\n",
    "\n",
    "# Initialize the LabelEncoder for \"BMI\"\n",
    "bmi_le = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder on the \"BMI\" column and transform it into numerical labels\n",
    "bmi_labels = bmi_le.fit_transform(diabetes_df[\"BMI\"])\n",
    "\n",
    "# Add the transformed \"BMI\" labels as a new column in the dataframe\n",
    "diabetes_df[\"BMI_Label\"] = bmi_labels\n",
    "\n",
    "# Create a new dataframe subset with only relevant columns: \n",
    "# \"diabetes\" (target variable), \"Generation\", \"Gen_Label\", \"BMI\", and \"BMI_Label\"\n",
    "diabetes_df_sub = diabetes_df[\n",
    "    [\"diabetes\", \"Generation\", \"Gen_Label\", \"BMI\", \"BMI_Label\"]\n",
    "]\n",
    "\n",
    "# Display rows 4 to 9 (5th to 10th) from the new dataframe subset\n",
    "diabetes_df_sub.iloc[4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode generation labels using one-hot encoding scheme\n",
    "gen_ohe = OneHotEncoder()  # Initialize the OneHotEncoder for 'Gen_Label'\n",
    "gen_feature_arr = gen_ohe.fit_transform(diabetes_df[[\"Gen_Label\"]]).toarray()  \n",
    "# Apply the encoder to the \"Gen_Label\" column and convert the result into an array\n",
    "gen_feature_labels = list(gen_ohe.categories_[0])  \n",
    "# Extract the unique categories from the 'Gen_Label' encoding and convert to a list\n",
    "gen_features = pd.DataFrame(gen_feature_arr, columns=gen_feature_labels)  \n",
    "# Create a DataFrame with the encoded features, with the appropriate column labels\n",
    "\n",
    "# Encode BMI labels using one-hot encoding scheme\n",
    "bmi_ohe = OneHotEncoder()  # Initialize the OneHotEncoder for 'BMI_Label'\n",
    "bmi_feature_arr = bmi_ohe.fit_transform(diabetes_df[[\"BMI_Label\"]]).toarray()  \n",
    "# Apply the encoder to the \"BMI_Label\" column and convert the result into an array\n",
    "bmi_feature_labels = [\"BMI_\" + str(cls_label) for cls_label in bmi_ohe.categories_[0]]  \n",
    "# Create BMI feature labels by prepending \"BMI_\" to the class labels of the BMI categories\n",
    "bmi_features = pd.DataFrame(bmi_feature_arr, columns=bmi_feature_labels)  \n",
    "# Create a DataFrame with the encoded BMI features, with the appropriate column labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes: diabetes_df_sub, gen_features, and bmi_features along columns (axis=1)\n",
    "# This will combine the features from these different sources into a single dataframe\n",
    "diabetes_df_ohe = pd.concat([diabetes_df_sub, gen_features, bmi_features], axis=1)\n",
    "\n",
    "# Create the column names list by combining predefined column labels and the feature labels\n",
    "# \"diabetes\", \"Generation\", and \"Gen_Label\" are predefined columns\n",
    "# gen_feature_labels and bmi_feature_labels are dynamically created lists based on features\n",
    "columns = sum(\n",
    "    [\n",
    "        [\"diabetes\", \"Generation\", \"Gen_Label\"],  # Predefined column names\n",
    "        gen_feature_labels,  # Feature labels for genetic data\n",
    "        [\"BMI\", \"BMI_Label\"],  # Predefined BMI-related column names\n",
    "        bmi_feature_labels,  # Feature labels for BMI-related data\n",
    "    ],\n",
    "    [],  # Flatten the list of lists into a single list\n",
    ")\n",
    "\n",
    "# Display a slice (rows 4 to 9) of the concatenated dataframe with the newly created columns\n",
    "diabetes_df_ohe[columns].iloc[4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output now includes one-hot encoded representations of both **Gen_Label** and **BMI_Label**, where each generated feature acts as a binary indicator. These dummy variables strictly assume values of 1 (when the category is present for a given observation) or 0 (when absent), creating mutually exclusive columns for every original categorical value. This transformation effectively converts the ordinal labels into a format suitable for machine learning algorithms while eliminating any artificial ordinal relationships that might bias model interpretation. For example, a '1' in the 'Class II obesity' column would indicate that particular BMI classification for the record while all other BMI category columns would show '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code creates a dummy DataFrame with two data points representing new diabetes cases.\n",
    "new_diabetes_df = pd.DataFrame(\n",
    "    # Data: A list of lists, where each list represents a data point\n",
    "    [[\"1\", \"Gen X\", \"Pre-obese\"],  # First data point: diabetes (1), Generation (Gen X), BMI (Pre-obese)\n",
    "     [\"0\", \"Boomers II\", \"Class I obesity\"]],  # Second data point: diabetes (0), Generation (Boomers II), BMI (Class I obesity)\n",
    "    \n",
    "    # Columns: Names of the columns for the DataFrame\n",
    "    columns=[\"diabetes\", \"Generation\", \"BMI\"],  # Define the names for each column (diabetes, Generation, BMI)\n",
    ")\n",
    "\n",
    "# Display the DataFrame to show the created data points\n",
    "new_diabetes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the text categories into numeric representations using our previously built LabelEncoder objects\n",
    "\n",
    "# Transforming the 'Generation' column values to numeric using the previously fitted LabelEncoder (gen_le)\n",
    "new_gen_labels = gen_le.transform(new_diabetes_df[\"Generation\"])\n",
    "\n",
    "# Adding the transformed numeric labels as a new column called \"Gen_Label\" in the DataFrame\n",
    "new_diabetes_df[\"Gen_Label\"] = new_gen_labels\n",
    "\n",
    "# Transforming the 'BMI' column values to numeric using the previously fitted LabelEncoder (bmi_le)\n",
    "new_bmi_labels = bmi_le.transform(new_diabetes_df[\"BMI\"])\n",
    "\n",
    "# Adding the transformed numeric labels as a new column called \"BMI_Label\" in the DataFrame\n",
    "new_diabetes_df[\"BMI_Label\"] = new_bmi_labels\n",
    "\n",
    "# Displaying the relevant columns to inspect the new encoded labels\n",
    "new_diabetes_df[[\"diabetes\", \"Generation\", \"Gen_Label\", \"BMI\", \"BMI_Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform 'Gen_Label' column using previously built LabelEncoder to one-hot encoded features\n",
    "# 'gen_ohe' is assumed to be a previously fitted OneHotEncoder for the 'Gen_Label' column.\n",
    "new_gen_feature_arr = gen_ohe.transform(new_diabetes_df[[\"Gen_Label\"]]).toarray()  \n",
    "# Convert the resulting array into a DataFrame with appropriate column names from 'gen_feature_labels'\n",
    "new_gen_features = pd.DataFrame(new_gen_feature_arr, columns=gen_feature_labels)  \n",
    "\n",
    "# Transform 'BMI_Label' column using previously built LabelEncoder to one-hot encoded features\n",
    "# 'bmi_ohe' is assumed to be a previously fitted OneHotEncoder for the 'BMI_Label' column.\n",
    "new_bmi_feature_arr = bmi_ohe.transform(new_diabetes_df[[\"BMI_Label\"]]).toarray()  \n",
    "# Convert the resulting array into a DataFrame with appropriate column names from 'bmi_feature_labels'\n",
    "new_bmi_features = pd.DataFrame(new_bmi_feature_arr, columns=bmi_feature_labels)  \n",
    "\n",
    "# Concatenate the original dataframe 'new_diabetes_df' with the newly generated one-hot encoded features\n",
    "# This will add the new columns from 'new_gen_features' and 'new_bmi_features' to the original data\n",
    "new_diabetes_ohe = pd.concat(\n",
    "    [new_diabetes_df, new_gen_features, new_bmi_features], axis=1\n",
    ")\n",
    "\n",
    "# Define the desired column order, starting with diabetes-related columns, then adding one-hot encoded features\n",
    "columns = sum(\n",
    "    [\n",
    "        [\"diabetes\", \"Generation\", \"Gen_Label\"],  # The original columns from the dataset\n",
    "        gen_feature_labels,  # Columns generated from one-hot encoding of 'Gen_Label'\n",
    "        [\"BMI\", \"BMI_Label\"],  # The BMI-related columns\n",
    "        bmi_feature_labels,  # Columns generated from one-hot encoding of 'BMI_Label'\n",
    "    ],\n",
    "    [],\n",
    ")\n",
    "\n",
    "# Display the dataframe with the new column order\n",
    "new_diabetes_ohe[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas provides the 'get_dummies()' function that can help us easily perform one-hot encoding.\n",
    "# It converts a categorical column into multiple binary columns, each representing a category in the original column.\n",
    "gen_onehot_features = pd.get_dummies(diabetes_df[\"Generation\"])\n",
    "\n",
    "# Concatenate the original dataframe with the one-hot encoded columns, while keeping the \"diabetes\" and \"Generation\" columns.\n",
    "# We use `axis=1` to concatenate along columns (horizontally).\n",
    "# The 'iloc[4:10]' selects rows 4 through 9 (i.e., 6 rows) from the resulting dataframe.\n",
    "# This gives a glimpse of the encoded features for a specific subset of rows.\n",
    "pd.concat([diabetes_df[[\"diabetes\", \"Generation\"]], gen_onehot_features], axis=1).iloc[\n",
    "    4:10\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Coding Scheme\n",
    "\n",
    "The dummy coding scheme is similar to one-hot encoding, with one key difference: when applied to a categorical feature with **m** distinct labels, it generates **m-1** binary features. As a result, each value of the categorical variable is converted into a vector of size **m-1**. The remaining feature is entirely omitted, and if the category values range from {**0, 1, ..., m-1**}, the 0th or (m-1)th feature is typically represented by a vector of all zeros (**0**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy (one-hot encoded) variables for the \"Generation\" column in the diabetes dataset.\n",
    "# The first category (Boomers I) is dropped to avoid the dummy variable trap (multicollinearity).\n",
    "gen_dummy_features = pd.get_dummies(diabetes_df[\"Generation\"], drop_first=True)\n",
    "\n",
    "# Concatenate the original \"diabetes\" and \"Generation\" columns with the newly created dummy variables.\n",
    "# Select and display rows 4 to 9 for inspection.\n",
    "pd.concat([diabetes_df[[\"diabetes\", \"Generation\"]], gen_dummy_features], axis=1).iloc[4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on the \"Generation\" column, creating binary features for each unique category.\n",
    "gen_onehot_features = pd.get_dummies(diabetes_df[\"Generation\"])\n",
    "\n",
    "# Drop the last column to avoid the dummy variable trap (multicollinearity issue).\n",
    "# This ensures that information is preserved without redundancy.\n",
    "gen_dummy_features = gen_onehot_features.iloc[:, :-1]\n",
    "\n",
    "# Concatenate the original \"diabetes\" and \"Generation\" columns with the encoded dummy variables.\n",
    "# Then, display rows 4 to 9 of the resulting DataFrame.\n",
    "pd.concat([diabetes_df[[\"diabetes\", \"Generation\"]], gen_dummy_features], axis=1).iloc[4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this module, we learned essential feature engineering techniques for both numeric and categorical data, including:\n",
    "\n",
    "- Converting raw data into machine learning-ready features\n",
    "- Applying appropriate transformations based on data type\n",
    "- Understanding and implementing different encoding schemes\n",
    "- Creating meaningful feature interactions\n",
    "- Handling both nominal and ordinal categorical variables\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
