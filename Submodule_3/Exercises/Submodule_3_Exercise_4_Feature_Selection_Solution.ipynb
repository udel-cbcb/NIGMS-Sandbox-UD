{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Exercise (Solution)\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module covers feature selection strategies in machine learning, focusing on the main approaches used to identify and select the most relevant features for model building.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the three main categories of feature selection methods\n",
    "  - Filter methods\n",
    "  - Wrapper methods\n",
    "  - Embedded methods\n",
    "- Learn how different feature selection strategies evaluate and select features\n",
    "- Recognize when to apply different feature selection approaches\n",
    "\n",
    "### Tasks to be completed\n",
    "\n",
    "- Review different feature selection methodologies\n",
    "- Compare filter, wrapper and embedded approaches\n",
    "- Understand the tradeoffs between selection methods\n",
    "- Practice implementing feature selection techniques\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python programming environment\n",
    "- Basic understanding of statistical and machine learning concepts\n",
    "- Familiarity with common ML libraries\n",
    "\n",
    "## Get Started\n",
    "\n",
    "- Please select kernel \"conda_python3\" from SageMaker notebook instance.\n",
    "\n",
    "### Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies and settings\n",
    "import warnings  # Import the warnings module to manage and control warning messages during script execution.\n",
    "\n",
    "import numpy as np  # Import the NumPy library and alias it as 'np' for numerical operations, especially for handling arrays and matrices.\n",
    "import pandas as pd  # Import the pandas library and alias it as 'pd' for data manipulation and analysis, particularly using DataFrames.\n",
    "from sklearn.datasets import load_breast_cancer  # Import the load_breast_cancer function from scikit-learn to load the breast cancer dataset for demonstration or testing.\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import the RandomForestClassifier class from scikit-learn for using Random Forest, an ensemble learning method for classification.\n",
    "from sklearn.feature_selection import RFE, SelectKBest, VarianceThreshold, chi2  # Import feature selection techniques from scikit-learn:\n",
    "#   - RFE (Recursive Feature Elimination) for feature ranking and selection by recursively fitting a model and removing the weakest features.\n",
    "#   - SelectKBest for feature selection based on univariate statistical tests, selecting the top k features.\n",
    "#   - VarianceThreshold for feature selection by removing features with low variance.\n",
    "#   - chi2 for chi-squared statistic, often used for feature selection in classification problems with non-negative features.\n",
    "from sklearn.linear_model import LogisticRegression  # Import the LogisticRegression class from scikit-learn for using logistic regression, a linear model for binary classification.\n",
    "from sklearn.model_selection import cross_val_score  # Import the cross_val_score function from scikit-learn for evaluating model performance using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import _E. coli_ Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the Ecoli dataset\n",
    "ecoli_data = \"../../Data/ecoli.csv\"\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv(ecoli_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecoli dataset is for predicting Protein Localization Sites in Ecoli.\n",
    "\n",
    "```\n",
    "Number of Instances:  336\n",
    "Number of Attributes: 8 ( 7 predictive, 1 name )\n",
    "Attribute Information.\n",
    "  1. Sequence Name: Accession number for the SWISS-PROT database\n",
    "  2. mcg: McGeoch's method for signal sequence recognition.\n",
    "  3. gvh: von Heijne's method for signal sequence recognition.\n",
    "  4. lip: von Heijne's Signal Peptidase II consensus sequence score (Binary attribute).\n",
    "  5. chg: Presence of charge on N-terminus of predicted lipoproteins (Binary attribute).\n",
    "  6. aac: score of discriminant analysis of the amino acid content of outer membrane and periplasmic proteins.\n",
    "  7. alm1: score of the ALOM membrane spanning region prediction program.\n",
    "  8. alm2: score of ALOM program after excluding putative cleavable signal regions from the sequence.\n",
    "Missing Attribute Values: None.\n",
    "Class Distribution. The class is the localization site.\n",
    "  cp  (cytoplasm)                                    143\n",
    "  im  (inner membrane without signal sequence)        77\n",
    "  pp  (perisplasm)                                    52\n",
    "  imU (inner membrane, uncleavable signal sequence)   35\n",
    "  om  (outer membrane)                                20\n",
    "  omL (outer membrane lipoprotein)                     5\n",
    "  imL (inner membrane lipoprotein)                     2\n",
    "  imS (inner membrane, cleavable signal sequence)      2\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "- Ecoli Dataset ([ecoli.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ecoli.data))\n",
    "- Ecoli Dataset Description ([ecoli.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ecoli.names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Feature Selection Strategies\n",
    "\n",
    "Feature selection methodologies can be categorized into three primary approaches, each with distinct characteristics:\n",
    "\n",
    "**1. Filter Methods**  \n",
    "These techniques evaluate features independently of any machine learning model, using statistical measures such as:\n",
    "- Correlation coefficients (Pearson, Spearman)  \n",
    "- Mutual information scores  \n",
    "- Chi-squared tests  \n",
    "- Variance thresholds  \n",
    "\n",
    "Filter methods are computationally efficient but may overlook feature interactions.\n",
    "\n",
    "**2. Wrapper Methods**  \n",
    "Wrapper approaches iteratively assess feature subsets by:\n",
    "- Building models with different feature combinations  \n",
    "- Evaluating performance metrics  \n",
    "- Selecting optimal subsets (e.g., via recursive feature elimination)  \n",
    "\n",
    "While more computationally intensive, they better capture feature interdependencies.\n",
    "\n",
    "**3. Embedded Methods**  \n",
    "These hybrid techniques integrate feature selection within model training:\n",
    "- Tree-based importance scores (Gini, entropy)  \n",
    "- Regularization parameters (L1/L2 penalties)  \n",
    "- Model-specific coefficients  \n",
    "\n",
    "Popular implementations include Random Forest feature importance and LASSO regression, offering a balance between efficiency and interaction awareness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure NumPy to suppress scientific notation when printing floating point numbers.\n",
    "# This ensures that numbers close to zero are displayed as 0 instead of in scientific notation.\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Retrieve the current print options and store the value of the \"threshold\" setting.\n",
    "# The threshold determines the number of array elements printed before summarization with \"...\".\n",
    "pt = np.get_printoptions()[\"threshold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold-Based Methods\n",
    "\n",
    "Threshold-based feature selection is a filter approach that retains features meeting specified statistical criteria while discarding others. Common thresholds include:  \n",
    "- **Variance thresholds** (removing low-variance features)  \n",
    "- **Score cutoffs** (e.g., keeping features above a percentile in chi-squared/MI tests)  \n",
    "- **Cardinality limits** (capping maximum features selected)  \n",
    "\n",
    "This method provides computational efficiency but requires careful threshold tuning to balance feature reduction with information retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance-Based Thresholding\n",
    "\n",
    "Variance-based feature selection eliminates low-variance features that provide minimal predictive value, as their near-constant values contribute little to model differentiation. By setting a user-defined threshold (e.g., `threshold=0.1`), features with variance below this cutoff are automatically removed. This approach is particularly effective for:  \n",
    "- **High-dimensional datasets** with many zero-variance features  \n",
    "- **Preprocessing pipelines** where scale-invariant filtering is needed  \n",
    "- **Noise reduction** by eliminating quasi-constant variables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape (number of rows and columns) of the DataFrame\n",
    "df.shape  # Returns a tuple (num_rows, num_columns)\n",
    "\n",
    "# Convert the categorical variable \"site\" into dummy/indicator variables,\n",
    "# creating a separate binary column for each unique category in \"site\".\n",
    "ecoli_site = pd.get_dummies(df[\"site\"])\n",
    "\n",
    "# Display the first few rows of the newly created dummy variables\n",
    "ecoli_site.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VarianceThreshold object to filter out low-variance features  \n",
    "# Features with a variance lower than 0.15 will be removed  \n",
    "vt = VarianceThreshold(threshold=0.15)  \n",
    "\n",
    "# Fit the VarianceThreshold object to the dataset (ecoli_site)  \n",
    "# This calculates the variance of each feature  \n",
    "vt.fit(ecoli_site)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display the variance of each feature and whether it was selected.  \n",
    "pd.DataFrame(\n",
    "    {   \n",
    "        \"variance\": vt.variances_,  # Store the variance values of the features.\n",
    "        \"select_feature\": vt.get_support()  # Boolean mask indicating selected features.\n",
    "    },  \n",
    "    index=ecoli_site.columns,  # Use feature names as index for better readability.\n",
    ").T  # Transpose the DataFrame to display features as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final subset of selected features  \n",
    "# `vt.get_support()` returns a boolean mask indicating which features were selected  \n",
    "# `iloc[:, vt.get_support()]` filters the dataset to keep only the selected features  \n",
    "# `head()` retrieves the first few rows for preview  \n",
    "ecoli_site_subset = ecoli_site.iloc[:, vt.get_support()].head()  \n",
    "ecoli_site_subset  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Breast Cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is known as the Wisconsin\n",
    "Diagnostic Breast Cancer dataset, which is also available in its native or raw format at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic), which is the UCI Machine Learning\n",
    "repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset from sklearn\n",
    "bc_data = load_breast_cancer()\n",
    "\n",
    "# Convert feature data into a Pandas DataFrame with appropriate column names\n",
    "bc_features = pd.DataFrame(bc_data.data, columns=bc_data.feature_names)\n",
    "\n",
    "# Convert target labels into a DataFrame with a descriptive column name\n",
    "bc_classes = pd.DataFrame(bc_data.target, columns=[\"IsMalignant\"])\n",
    "\n",
    "# Build the feature set (X) and response class labels (y)\n",
    "bc_X = np.array(bc_features)  # Convert features DataFrame to a NumPy array\n",
    "bc_y = np.array(bc_classes).T[0]  # Convert target labels to a NumPy array and flatten it\n",
    "\n",
    "# Print the shape of the feature set and response class labels for verification\n",
    "print(\"Feature set shape:\", bc_X.shape)\n",
    "print(\"Response class shape:\", bc_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the print threshold to limit the number of array elements displayed  \n",
    "np.set_printoptions(threshold=30)\n",
    "\n",
    "# Print the shape of the feature set  \n",
    "print(\"Feature set data [shape: \" + str(bc_X.shape) + \"]\")\n",
    "\n",
    "# Print the feature set with values rounded to 2 decimal places  \n",
    "print(np.round(bc_X, 2), \"\\n\")\n",
    "\n",
    "# Print the feature names  \n",
    "print(\"Feature names:\")\n",
    "print(np.array(bc_features.columns), \"\\n\")\n",
    "\n",
    "# Print the shape of the predictor class label data  \n",
    "print(\"Predictor Class label data [shape: \" + str(bc_y.shape) + \"]\")\n",
    "\n",
    "# Print the predictor class label data  \n",
    "print(bc_y, \"\\n\")\n",
    "\n",
    "# Print the predictor class name  \n",
    "print(\"Predictor name:\", np.array(bc_classes.columns))\n",
    "\n",
    "# Reset the print threshold to its previous value (stored in variable 'pt')  \n",
    "np.set_printoptions(threshold=pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a binary response variable where 1 represents benign tumors and 0 indicates malignant cases, providing clear clinical distinction for classification tasks. Accompanying this target variable are 30 continuous-valued features derived from digitized images of breast mass biopsies. These numerical features quantitatively characterize various morphological properties of cell nuclei, including size, texture, and structural characteristics, which are critical for pathological assessment. The real-valued nature of these predictors suggests they may benefit from standardization before modeling, while the binary classification task lends itself to evaluation using metrics like ROC-AUC and F1-score given the potential clinical consequences of misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chi-square test to evaluate the importance of features  \n",
    "# and select the top 15 best features out of the 30 available features.\n",
    "skb = SelectKBest(score_func=chi2, k=15)  \n",
    "\n",
    "# Fit the SelectKBest model to the feature matrix (bc_X) and target variable (bc_y)  \n",
    "# to determine the most relevant features based on the chi-square test.\n",
    "skb.fit(bc_X, bc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples where each tuple contains a feature name and its corresponding score\n",
    "feature_scores = [\n",
    "    (item, score) for item, score in zip(bc_data.feature_names, skb.scores_)\n",
    "]\n",
    "\n",
    "# Sort the list of feature scores in descending order based on the score value (x[1])\n",
    "# and return the top 10 most relevant features\n",
    "sorted(feature_scores, key=lambda x: -x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the selected features obtained from our original feature set \n",
    "# using the chi-square test (via SelectKBest)\n",
    "select_features_kbest = skb.get_support()  # Get the mask of selected features (True for selected)\n",
    "feature_names_kbest = bc_data.feature_names[select_features_kbest]  # Get feature names corresponding to selected features\n",
    "\n",
    "# Create a DataFrame with the selected features using the column names from the original feature set\n",
    "feature_subset_df = bc_features[feature_names_kbest]\n",
    "\n",
    "# Convert the DataFrame of selected features into a numpy array for further processing\n",
    "bc_SX = np.array(feature_subset_df)\n",
    "\n",
    "# Print the shape of the resulting feature subset array (rows, columns)\n",
    "print(bc_SX.shape)\n",
    "\n",
    "# Print the names of the selected features\n",
    "print(feature_names_kbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a feature subset of the Wisconsin Diagnostic Breast Cancer dataset using chi-square tests\n",
    "# The 'feature_subset_df' contains the result of feature selection based on chi-square tests\n",
    "# We are selecting a specific subset of rows (from row 20 to 25) and rounding the values to 2 decimal places\n",
    "np.round(feature_subset_df.iloc[20:25], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the impact of feature selection, we will train and compare two logistic regression models: one using the full set of 30 features and another with only the 15 selected features. Model performance will be assessed via accuracy (percentage of correct predictions) under a five-fold cross-validation scheme, ensuring robust evaluation while mitigating overfitting. This comparative analysis will isolate the effect of feature reduction, revealing whether the selected subset preserves predictive power or potentially improves model generalizability through noise reduction. The experiment’s design specifically controls for algorithmic bias by maintaining identical evaluation metrics and validation protocols across both feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings to keep the output clean (e.g., avoid displaying convergence warnings)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Build a Logistic Regression model with a maximum of 1000 iterations for convergence\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Evaluate accuracy of the model built on the complete feature set using 5-fold cross-validation\n",
    "# 'cross_val_score' performs cross-validation and calculates accuracy for each fold\n",
    "full_feat_acc = np.average(cross_val_score(lr, bc_X, bc_y, scoring=\"accuracy\", cv=5))\n",
    "\n",
    "# Evaluate accuracy of the model built on the selected feature set using 5-fold cross-validation\n",
    "sel_feat_acc = np.average(cross_val_score(lr, bc_SX, bc_y, scoring=\"accuracy\", cv=5))\n",
    "\n",
    "# Print out the results\n",
    "print(\"Model accuracy statistics with 5-fold cross validation\")\n",
    "# Output the accuracy of the model using the full feature set, along with the shape of the input features\n",
    "print(\"Model accuracy with complete feature set\", bc_X.shape, \":\", full_feat_acc)\n",
    "# Output the accuracy of the model using the selected feature set, along with the shape of the input features\n",
    "print(\"Model accuracy with selected feature set\", bc_SX.shape, \":\", sel_feat_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "Recursive Feature Elimination (RFE) is a powerful wrapper-based feature selection technique that systematically reduces features through an iterative backward elimination process. The method begins by training a specified machine learning model (such as logistic regression for classification tasks) on the complete set of features (e.g., 30 features in our case) and their corresponding target variables. RFE then evaluates and ranks features based on the model-derived importance weights (e.g., regression coefficients for linear models). In each iteration, the lowest-ranked features are pruned, and the model is retrained on the remaining subset. This recursive process continues until reaching a predetermined optimal number of features specified by the user. By progressively eliminating weaker features while retraining the model at each step, RFE effectively captures feature interactions and dependencies, often yielding more robust feature subsets than filter methods. This backward elimination approach is particularly valuable when working with high-dimensional datasets where identifying the most predictive feature combinations is crucial for model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model to be used for feature selection\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Initialize RFE (Recursive Feature Elimination) with:\n",
    "# - the Logistic Regression model (lr) as the estimator\n",
    "# - n_features_to_select=15, meaning we want to select the top 15 features\n",
    "# - step=1, which means we remove one feature at a time in each iteration\n",
    "rfe = RFE(estimator=lr, n_features_to_select=15, step=1)\n",
    "\n",
    "# Fit the RFE model on the breast cancer dataset (X: features, y: target)\n",
    "rfe.fit(bc_X, bc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the final selected features from RFE (Recursive Feature Elimination)\n",
    "select_features_rfe = rfe.get_support()  # Get a boolean array of selected features (True for selected, False for non-selected)\n",
    "\n",
    "# Extract the names of the features that were selected by RFE\n",
    "feature_names_rfe = bc_data.feature_names[select_features_rfe]  # Filter the feature names based on the selected ones\n",
    "\n",
    "# Print the names of the selected features\n",
    "print(feature_names_rfe)\n",
    "\n",
    "# Fit the RFE model to the data (this step might be part of the RFE process, not needed if already fitted)\n",
    "rfe.fit(bc_X, bc_y)  # Fit the RFE model to the input data (bc_X) and target labels (bc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the feature subset selected using SelectKBest (feature_names_kbest) \n",
    "# with the feature subset selected using Recursive Feature Elimination (RFE) \n",
    "# (feature_names_rfe). The intersection will show the common features between \n",
    "# these two subsets.\n",
    "set(feature_names_kbest) & set(feature_names_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model based selection\n",
    "\n",
    "Tree-based machine learning models, including individual decision trees and ensemble methods like random forests, serve dual purposes in predictive analytics: both as powerful modeling techniques and as effective feature selection tools. These models inherently calculate feature importance scores during their training process—through metrics like Gini impurity reduction or mean decrease in accuracy—which quantitatively measure each feature's contribution to prediction accuracy. By analyzing these importance scores, data scientists can objectively identify and retain the most predictive features while eliminating irrelevant or redundant ones. This embedded selection approach offers key advantages over standalone feature selection methods: it efficiently captures complex feature interactions, scales well to high-dimensional data, and produces importance rankings that are directly optimized for the specific modeling task. The selected features often yield more interpretable models while maintaining or even improving predictive performance compared to using the full feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the RandomForestClassifier model from sklearn\n",
    "rfc = RandomForestClassifier()  # Initialize a RandomForestClassifier object\n",
    "\n",
    "# Fitting the model on the training data (bc_X for features, bc_y for labels)\n",
    "rfc.fit(bc_X, bc_y)  # Train the random forest model on the given feature set (bc_X) and target labels (bc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random forest estimator to score the features based on their importance\n",
    "# The `rfc.feature_importances_` attribute provides the importance scores for each feature\n",
    "importance_scores = rfc.feature_importances_\n",
    "\n",
    "# Create a list of tuples where each tuple contains a feature name and its corresponding importance score\n",
    "feature_importances = [\n",
    "    (feature, score) for feature, score in zip(bc_data.feature_names, importance_scores)\n",
    "]\n",
    "\n",
    "# Sort the list of feature importances by the score in descending order and select the top 10 features\n",
    "# The `lambda x: -x[1]` ensures the list is sorted in descending order based on the score\n",
    "sorted(feature_importances, key=lambda x: -x[1])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use a threshold based parameter to filter out the top n features as needed or you can even\n",
    "make use of the SelectFromModel meta-transformer provided by scikit-learn by using it as a wrapper on\n",
    "top of this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Feature selection is a critical step in machine learning that helps identify the most relevant features for model building. We learned about three main approaches:\n",
    "\n",
    "- Filter methods that use metrics and statistical tests\n",
    "- Wrapper methods that evaluate feature subsets recursively\n",
    "- Embedded methods that combine benefits of other approaches using ML models\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
