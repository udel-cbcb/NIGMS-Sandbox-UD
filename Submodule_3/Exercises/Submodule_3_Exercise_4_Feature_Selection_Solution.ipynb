{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Exercise Solution\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module covers feature selection strategies in machine learning, focusing on the main approaches used to identify and select the most relevant features for model building.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the three main categories of feature selection methods\n",
    "  - Filter methods\n",
    "  - Wrapper methods\n",
    "  - Embedded methods\n",
    "- Learn how different feature selection strategies evaluate and select features\n",
    "- Recognize when to apply different feature selection approaches\n",
    "\n",
    "### Tasks to be completed\n",
    "\n",
    "- Review different feature selection methodologies\n",
    "- Compare filter, wrapper and embedded approaches\n",
    "- Understand the tradeoffs between selection methods\n",
    "- Practice implementing feature selection techniques\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python programming environment\n",
    "- Basic understanding of statistical and machine learning concepts\n",
    "- Familiarity with common ML libraries\n",
    "\n",
    "## Get Started\n",
    "\n",
    "### Set up conda environment\n",
    "\n",
    "### Set up conda environment\n",
    "\n",
    "Ensure that you have created then conda environment using the `conda_env.yml` file included in this repository. E.g.,\n",
    "\n",
    "```\n",
    "# Create conda environment\n",
    "conda env create -f conda_env.yml\n",
    "\n",
    "# Register the kernel\n",
    "python -m ipykernel install --user \\\n",
    "    --name=nigms_sandbox_ud \\\n",
    "    --display-name \"Python (NIGMS Sandbox UD)\"\n",
    "```\n",
    "\n",
    "Then, when starting the notebook, select the `\"Python (NIGMS Sandbox UD)\"` kernel from the list.\n",
    "\n",
    "Note that you may need to restart Jupyter Lab for these changes to take effect.\n",
    "\n",
    "### Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies and settings\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE, SelectKBest, VarianceThreshold, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import _E. coli_ Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ecoli dataset\n",
    "ecoli_data = \"../../Data/ecoli.csv\"\n",
    "df = pd.read_csv(ecoli_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecoli dataset is for predicting Protein Localization Sites in Ecoli.\n",
    "\n",
    "```\n",
    "Number of Instances:  336\n",
    "Number of Attributes: 8 ( 7 predictive, 1 name )\n",
    "Attribute Information.\n",
    "  1. Sequence Name: Accession number for the SWISS-PROT database\n",
    "  2. mcg: McGeoch's method for signal sequence recognition.\n",
    "  3. gvh: von Heijne's method for signal sequence recognition.\n",
    "  4. lip: von Heijne's Signal Peptidase II consensus sequence score (Binary attribute).\n",
    "  5. chg: Presence of charge on N-terminus of predicted lipoproteins (Binary attribute).\n",
    "  6. aac: score of discriminant analysis of the amino acid content of outer membrane and periplasmic proteins.\n",
    "  7. alm1: score of the ALOM membrane spanning region prediction program.\n",
    "  8. alm2: score of ALOM program after excluding putative cleavable signal regions from the sequence.\n",
    "Missing Attribute Values: None.\n",
    "Class Distribution. The class is the localization site.\n",
    "  cp  (cytoplasm)                                    143\n",
    "  im  (inner membrane without signal sequence)        77\n",
    "  pp  (perisplasm)                                    52\n",
    "  imU (inner membrane, uncleavable signal sequence)   35\n",
    "  om  (outer membrane)                                20\n",
    "  omL (outer membrane lipoprotein)                     5\n",
    "  imL (inner membrane lipoprotein)                     2\n",
    "  imS (inner membrane, cleavable signal sequence)      2\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "- Ecoli Dataset ([ecoli.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ecoli.data))\n",
    "- Ecoli Dataset Description ([ecoli.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ecoli.names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Feature selection strategies can be divided into three main areas based on the type of strategy and\n",
    "techniques employed:\n",
    "\n",
    "- **Filter methods**: select features purely based on metrics like\n",
    "  correlation, mutual information and so on. Popular methods include threshold based\n",
    "  methods and statistical tests.\n",
    "- **Wrapper methods**: capture interaction between multiple\n",
    "  features by using a recursive approach to build multiple models using feature\n",
    "  subsets and select the best subset of features giving us the best performing model.\n",
    "  Methods like backward selecting and forward elimination are popular wrapper\n",
    "  based methods.\n",
    "- **Embedded methods**: combine the benefits of the other\n",
    "  two methods by leveraging Machine Learning models themselves to rank and score\n",
    "  feature variables based on their importance. Tree based methods like decision trees\n",
    "  and ensemble methods like random forests are popular examples of embedded\n",
    "  methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print floating point numbers using fixed point notation, in which case numbers equal to zero in the current precision will print as zero.\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Return the current print options.\n",
    "pt = np.get_printoptions()[\"threshold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold based methods\n",
    "\n",
    "This is a filter based feature selection strategy, where you can use some form of cut-off or thresholding for\n",
    "limiting the total number of features during feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance based thresholding\n",
    "\n",
    "Another way of using thresholds is to use variance based thresholding where features having low\n",
    "variance (below a user-specified threshold) are removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variable into dummy/indicator variables.\n",
    "ecoli_site = pd.get_dummies(df[\"site\"])\n",
    "ecoli_site.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VarianceThreashold object to remove features from the one hot encoded\n",
    "# features where the variance is less than 0.15\n",
    "\n",
    "vt = VarianceThreshold(threshold=0.15)\n",
    "vt.fit(ecoli_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which features have been selected based on their True values and also their variance being above 0.15.\n",
    "pd.DataFrame(\n",
    "    {\"variance\": vt.variances_, \"select_feature\": vt.get_support()},\n",
    "    index=ecoli_site.columns,\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final subset of selected features\n",
    "ecoli_site_subset = ecoli_site.iloc[:, vt.get_support()].head()\n",
    "ecoli_site_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is known as the Wisconsin\n",
    "Diagnostic Breast Cancer dataset, which is also available in its native or raw format at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic), which is the UCI Machine Learning\n",
    "repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data = load_breast_cancer()\n",
    "bc_features = pd.DataFrame(bc_data.data, columns=bc_data.feature_names)\n",
    "bc_classes = pd.DataFrame(bc_data.target, columns=[\"IsMalignant\"])\n",
    "\n",
    "# build featureset and response class labels\n",
    "bc_X = np.array(bc_features)\n",
    "bc_y = np.array(bc_classes).T[0]\n",
    "print(\"Feature set shape:\", bc_X.shape)\n",
    "print(\"Response class shape:\", bc_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=30)\n",
    "print(\"Feature set data [shape: \" + str(bc_X.shape) + \"]\")\n",
    "print(np.round(bc_X, 2), \"\\n\")\n",
    "print(\"Feature names:\")\n",
    "print(np.array(bc_features.columns), \"\\n\")\n",
    "print(\"Predictor Class label data [shape: \" + str(bc_y.shape) + \"]\")\n",
    "print(bc_y, \"\\n\")\n",
    "print(\"Predictor name:\", np.array(bc_classes.columns))\n",
    "np.set_printoptions(threshold=pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response class variable is a binary\n",
    "class where 1 indicates the tumor detected was benign and 0 indicates it was malignant. We can also see\n",
    "the 30 features that are real valued numbers that describe characteristics of cell nuclei present in digitized\n",
    "images of breast mass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the chi-square test on this feature set and select the top 15 best features out of the 30 features.\n",
    "skb = SelectKBest(score_func=chi2, k=15)\n",
    "skb.fit(bc_X, bc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the scores to see the most relevant features\n",
    "feature_scores = [\n",
    "    (item, score) for item, score in zip(bc_data.feature_names, skb.scores_)\n",
    "]\n",
    "sorted(feature_scores, key=lambda x: -x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset of the selected features obtained from our original feature set of features with the help of the chi-square test\n",
    "select_features_kbest = skb.get_support()\n",
    "feature_names_kbest = bc_data.feature_names[select_features_kbest]\n",
    "feature_subset_df = bc_features[feature_names_kbest]\n",
    "bc_SX = np.array(feature_subset_df)\n",
    "print(bc_SX.shape)\n",
    "print(feature_names_kbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected feature subset of the Wisconsin Diagnostic Breast Cancer dataset using chi-square tests\n",
    "np.round(feature_subset_df.iloc[20:25], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now build a simple\n",
    "classification model using logistic regression on the original feature set of 30 features and compare the\n",
    "model accuracy performance with another model built using our selected 15 features. For model evaluation,\n",
    "we will use the accuracy metric (percent of correct predictions) and use a five-fold cross-validation scheme. The main idea here is to compare the model\n",
    "prediction performance between models trained on different feature sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# build logistic regression model with max_iter of 1000\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# evaluating accuracy for model built on full featureset\n",
    "full_feat_acc = np.average(cross_val_score(lr, bc_X, bc_y, scoring=\"accuracy\", cv=5))\n",
    "# evaluating accuracy for model built on selected featureset\n",
    "sel_feat_acc = np.average(cross_val_score(lr, bc_SX, bc_y, scoring=\"accuracy\", cv=5))\n",
    "\n",
    "print(\"Model accuracy statistics with 5-fold cross validation\")\n",
    "print(\"Model accuracy with complete feature set\", bc_X.shape, \":\", full_feat_acc)\n",
    "print(\"Model accuracy with selected feature set\", bc_SX.shape, \":\", sel_feat_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metrics clearly show us that we actually built a better model\n",
    "when trained on the selected 15 feature subset as compared to the model built with the original 30 features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "Recursive Feature Elimination, also known as RFE, is a popular wrapper based feature selection technique,\n",
    "which allows you to recursively keep eliminating lower scored features till you arrive at the specific feature subset count. The basic idea is to start off with a specific Machine Learning estimator\n",
    "like the Logistic Regression algorithm we used for our classification needs. Next we take the entire feature set\n",
    "of 30 features and the corresponding response class variables. RFE aims to assign weights to these features\n",
    "based on the model fit. Features with the smallest weights are pruned out and then a model is fit again on the remaining features to obtain the new weights or scores. This process is recursively carried out multiple\n",
    "times and each time features with the lowest scores/weights are eliminated, until the pruned feature subset\n",
    "contains the desired number of features that the user wanted to select (this is taken as an input parameter at\n",
    "the start). This strategy is also popularly known as backward elimination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "# select the top 15 features on our breast cancer dataset now using RFE.\n",
    "rfe = RFE(estimator=lr, n_features_to_select=15, step=1)\n",
    "rfe.fit(bc_X, bc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the final selected features\n",
    "select_features_rfe = rfe.get_support()\n",
    "feature_names_rfe = bc_data.feature_names[select_features_rfe]\n",
    "print(feature_names_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare this feature subset with the one we obtained using statistical tests\n",
    "# in the previous section and see which features are common among both these subsets\n",
    "set(feature_names_kbest) & set(feature_names_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model based selection\n",
    "\n",
    "Tree based models like decision trees and ensemble models like random forests (ensemble of trees) can\n",
    "be utilized not just for modeling alone but for feature selection. These models can be used to compute\n",
    "feature importances when building the model that can in turn be used for selecting the best features and\n",
    "discarding irrelevant features with lower scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the random forest model to score and rank features based on their importance.\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(bc_X, bc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random forest estimator to score the features based on their importance\n",
    "# and we display the top 10 most important features based on this score\n",
    "importance_scores = rfc.feature_importances_\n",
    "feature_importances = [\n",
    "    (feature, score) for feature, score in zip(bc_data.feature_names, importance_scores)\n",
    "]\n",
    "sorted(feature_importances, key=lambda x: -x[1])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use a threshold based parameter to filter out the top n features as needed or you can even\n",
    "make use of the SelectFromModel meta-transformer provided by scikit-learn by using it as a wrapper on\n",
    "top of this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Feature selection is a critical step in machine learning that helps identify the most relevant features for model building. We learned about three main approaches:\n",
    "\n",
    "- Filter methods that use metrics and statistical tests\n",
    "- Wrapper methods that evaluate feature subsets recursively\n",
    "- Embedded methods that combine benefits of other approaches using ML models\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
