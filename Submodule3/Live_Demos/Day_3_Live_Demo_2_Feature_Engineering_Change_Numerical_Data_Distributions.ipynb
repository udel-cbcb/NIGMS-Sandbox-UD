{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Change Numerical Data Distributions**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical input variables may have a highly skewed or non-standard distribution. This could be\n",
    "caused by outliers in the data, multi-modal distributions, highly exponential distributions, and\n",
    "more. Many machine learning algorithms prefer or perform better when numerical input variables\n",
    "and even output variables in the case of regression have a standard probability distribution,\n",
    "such as a Gaussian (normal) or a Uniform distribution.\n",
    "\n",
    "The quantile transform provides an automatic way to transform a numeric input variable to\n",
    "have a different data distribution, which in turn, can be used as input to a predictive model.\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "\n",
    "* Many machine learning algorithms prefer or perform better when numerical variables have\n",
    "a Gaussian or standard probability distribution.\n",
    "* Quantile transforms are techniques for transforming numerical input or output variables\n",
    "to have a Gaussian or Uniform probability distribution.\n",
    "* How to use the QuantileTransformer to change the probability distribution of numeric\n",
    "variables to improve the performance of predictive models.\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Quantile Transforms\n",
    "A quantile transform will map a variable's probability distribution to another probability\n",
    "distribution. The quantile function ranks or smooths out the relationship between observations and can be\n",
    "mapped onto other distributions, such as the uniform or normal distribution. The transformation\n",
    "can be applied to each numeric input variable in the training dataset and then provided as\n",
    "input to a machine learning model to learn a predictive modeling task. This quantile transform\n",
    "is available in the scikit-learn Python machine learning library via the **QuantileTransformer**\n",
    "class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first creates a sample of 1,000 random Gaussian values and adds a\n",
    "skew to the dataset. A histogram is created from the skewed dataset and clearly shows the\n",
    "distribution pushed to the far left. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of the quantile transform\n",
    "from numpy import exp\n",
    "from numpy.random import randn\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate gaussian data sample\n",
    "data = randn(1000)\n",
    "# add a skew to the data distribution\n",
    "data = exp(data)\n",
    "# histogram of the raw data with a skew\n",
    "pyplot.hist(data, bins=25)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a QuantileTransformer is used to map the data to a Gaussian distribution and standardize the result, centering the values on the mean value of 0 and a standard deviation of\n",
    "1.0. A histogram of the transform data is created showing a Gaussian shaped data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data to have rows and columns\n",
    "data = data.reshape((len(data),1))\n",
    "print(data.shape)\n",
    "# quantile transform the raw data\n",
    "# perform a normal quantile transform of the dataset\n",
    "# 'output_distribution\" is the marginal distribution for the transformed data. The choices are\n",
    "# 'uniform' (default) or 'normal'.\n",
    "quantile = QuantileTransformer(output_distribution='normal')\n",
    "data_trans = quantile.fit_transform(data)\n",
    "# histogram of the transformed data\n",
    "pyplot.hist(data_trans, bins=25)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Diabetes Dataset\n",
    "The dataset classifies patient data as\n",
    "either an onset of diabetes within five years or not. \n",
    "```\n",
    "Number of Instances: 768\n",
    "Number of Attributes: 8 plus class \n",
    "For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "Missing Attribute Values: Yes\n",
    "Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "   Class Value  Number of instances\n",
    "   0            500\n",
    "   1            268\n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "\n",
    "* Diabetes Dataset File ([pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv))\n",
    "* Diabetes Dataset Details ([pima-indians-diabetes.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Download Diabetes data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!python -m wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\" -o pima-indians-diabetes.csv\n",
    "!python -m wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\" -o pima-indians-diabetes.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Summarizing the variables from the pima-indians-diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the dataset\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "print(dataset.head())\n",
    "# summarize the shape of the dataset\n",
    "print(dataset.shape)\n",
    "# summarize each variable\n",
    "print(dataset.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the 8\n",
    "input variables, one output variable, and 768 rows of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a histogram is created for each input variable. If we ignore the clutter of the plots and\n",
    "focus on the histograms themselves, we can see that many variables have a skewed distribution.\n",
    "The dataset provides a good candidate for using a quantile transform to make the variables\n",
    "more-Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's first and evaluate a machine learning model on the raw dataset. We will use\n",
    "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
    "stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate KNN classifier on the raw dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# KFold \n",
    "#   is a cross-validator that divides the dataset into k folds.\n",
    "# Stratified\n",
    "#   is to ensure that each fold of dataset has the same proportion of observations with a given label.\n",
    "# Repeated \n",
    "#   provides a way to improve the estimated performance of a machine learning model. \n",
    "# This involves simply repeating the cross-validation procedure multiple times and reporting the mean \n",
    "# result across all folds from all runs. This mean result is expected to be a more accurate estimate \n",
    "# of the true unknown underlying mean performance of the model on the dataset, as calculated using the standard error.\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load dataset\n",
    "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "# define and configure the model using \n",
    "# Classifier implementing the k-nearest neighbors vote.\n",
    "model = KNeighborsClassifier()\n",
    "# evaluate the model using RepeatedStratifiedKFold cross validator, \n",
    "# that repeats Stratified K-Fold n times with different randomization in each\n",
    "# repetition.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report model performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the model achieved a mean classification accuracy of about 71.7\n",
    "percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Normal Quantile Transform\n",
    "It is often desirable to transform an input variable to have a normal probability distribution to improve the modeling performance. We can apply the Quantile transform using the\n",
    "QuantileTransformer class and set the output distribution argument to `normal'. We\n",
    "must also set the n quantiles argument to a value less than the number of observations in the\n",
    "training dataset, in this case, 100. Once defined, we can call the fit transform() function and\n",
    "pass it to our dataset to create a quantile transformed version of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a normal quantile transform of the dataset\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "# perform a normal quantile transform of the dataset\n",
    "# 'n_quantiles\" is the number of quantiles to be computed. It corresponds to the number\n",
    "# of landmarks used to discretize the cumulative distribution function.\n",
    "# 'output_distribution\" is the marginal distribution for the transformed data. The choices are\n",
    "# 'uniform' (default) or 'normal'.\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "data = trans.fit_transform(data)\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the shape of the histograms for each variable looks very Gaussian as compared\n",
    "to the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
    "normal quantile transform of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate KNN with normal quantile transform\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "# define the pipeline\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "# Classifier implementing the k-nearest neighbors vote.\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)])\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the normal quantile transform results in a lift in\n",
    "performance from 71.7 percent accuracy without the transform to about 73.4 percent with the\n",
    "transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Uniform Quantile Transform\n",
    "Sometimes it can be beneficial to transform a highly exponential or multi-modal distribution to\n",
    "have a uniform distribution. This is especially useful for data with a large and sparse range of\n",
    "values, e.g. outliers that are common rather than rare. We can apply the transform by defining\n",
    "a QuantileTransformer class and setting the output distribution argument to `uniform'\n",
    "(the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a uniform quantile transform of the dataset\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "# perform a uniform quantile transform of the dataset\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution='uniform')\n",
    "data = trans.fit_transform(data)\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the shape of the histograms for each variable looks very uniform compared to\n",
    "the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
    "uniform quantile transform of the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate KNN classifer on the dataset with uniform quantile transform\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "# define the pipeline\n",
    "trans = QuantileTransformer(n_quantiles=100, output_distribution='uniform')\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)])\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the uniform transform results in a lift in performance\n",
    "from 71.7 percent accuracy without the transform to about 73.4 percent with the normal transform, and achieved a score of 74.1 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the number of quantiles as an arbitrary number, in this case, 100. This hyperparameter can be tuned to explore the effect of the resolution of the transform on the resulting\n",
    "skill of the model. The example below performs this experiment and plots the mean accuracy\n",
    "for different n quantiles values from 1 to 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore number of quantiles on classification accuracy\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset(filename):\n",
    "  # load dataset\n",
    "  dataset = read_csv(filename, header=None)\n",
    "  data = dataset.values\n",
    "  # separate into input and output columns\n",
    "  X, y = data[:, :-1], data[:, -1]\n",
    "  # ensure inputs are floats and output is an integer label\n",
    "  X = X.astype('float32')\n",
    "  y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "  return X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "  models = dict()\n",
    "  for i in range(1,100):\n",
    "    # define the pipeline\n",
    "    trans = QuantileTransformer(n_quantiles=i, output_distribution='uniform')\n",
    "    model = KNeighborsClassifier()\n",
    "    # The purpose of the pipeline is to assemble several steps that can be\n",
    "    # cross-validated together while setting different parameters.\n",
    "    models[str(i)] = Pipeline(steps=[('t', trans), ('m', model)])\n",
    "  return models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "  # Repeats Stratified K-Fold n times with different randomization in each repetition.\n",
    "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "  # Evaluate a score by cross-validation\n",
    "  scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "  return scores\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset('pima-indians-diabetes.csv')\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results = list()\n",
    "for name, model in models.items():\n",
    "  scores = evaluate_model(model, X, y)\n",
    "  results.append(mean(scores))\n",
    "  print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that surprisingly smaller values resulted in better accuracy, with\n",
    "values such as 9 achieving an accuracy of about 74.7 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance for comparison\n",
    "pyplot.plot(results)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line plot is created showing the number of quantiles used in the transform versus the\n",
    "classification accuracy of the resulting model. We can see a bump with values less than 10 and\n",
    "drop and \n",
    "at performance after that. The results highlight that there is likely some benefit in\n",
    "exploring different distributions and number of quantiles to see if better performance can be\n",
    "achieved."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
