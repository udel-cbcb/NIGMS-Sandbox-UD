{"cells":[{"cell_type":"markdown","metadata":{"id":"sEvOjMDe0zHF"},"source":["#**Feature Selection Exercise** \n","\n","Feature selection strategies can be divided into three main areas based on the type of strategy and\n","techniques employed:\n","\n","* **Filter methods**: select features purely based on metrics like\n","correlation, mutual information and so on. Popular methods include threshold based\n","methods and statistical tests.\n","* **Wrapper methods**: capture interaction between multiple\n","features by using a recursive approach to build multiple models using feature\n","subsets and select the best subset of features giving us the best performing model.\n","Methods like backward selecting and forward elimination are popular wrapper\n","based methods.\n","* **Embedded methods**: combine the benefits of the other\n","two methods by leveraging Machine Learning models themselves to rank and score\n","feature variables based on their importance. Tree based methods like decision trees\n","and ensemble methods like random forests are popular examples of embedded\n","methods.\n","\n","Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1)."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6D1KKPX30zHJ","executionInfo":{"status":"ok","timestamp":1655166168414,"user_tz":240,"elapsed":966,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}}},"outputs":[],"source":["#Import necessary dependencies and settings\n","import numpy as np\n","import pandas as pd\n","\n","# print floating point numbers using fixed point notation, in which case numbers equal to zero in the current precision will print as zero.\n","np.set_printoptions(suppress=True)\n","\n","# Return the current print options.\n","pt = np.get_printoptions()['threshold']"]},{"cell_type":"markdown","metadata":{"id":"euuyHItf0zHK"},"source":["# Threshold based methods\n","This is a filter based feature selection strategy, where you can use some form of cut-off or thresholding for\n","limiting the total number of features during feature selection."]},{"cell_type":"markdown","metadata":{"id":"xMfISgq30zHM"},"source":["## Variance based thresholding\n","\n","Another way of using thresholds is to use variance based thresholding where features having low\n","variance (below a user-specified threshold) are removed.\n","\n"]},{"cell_type":"markdown","source":["###Ecoli Dataset\n","\n","Ecoli dataset is for predicting Protein Localization Sites in Ecoli. \n","```\n","Number of Instances:  336 \n","Number of Attributes: 8 ( 7 predictive, 1 name )\n","Attribute Information.\n","  1. Sequence Name: Accession number for the SWISS-PROT database\n","  2. mcg: McGeoch's method for signal sequence recognition.\n","  3. gvh: von Heijne's method for signal sequence recognition.\n","  4. lip: von Heijne's Signal Peptidase II consensus sequence score (Binary attribute).\n","  5. chg: Presence of charge on N-terminus of predicted lipoproteins (Binary attribute).\n","  6. aac: score of discriminant analysis of the amino acid content of outer membrane and periplasmic proteins.\n","  7. alm1: score of the ALOM membrane spanning region prediction program.\n","  8. alm2: score of ALOM program after excluding putative cleavable signal regions from the sequence.\n","Missing Attribute Values: None.\n","Class Distribution. The class is the localization site.\n","  cp  (cytoplasm)                                    143\n","  im  (inner membrane without signal sequence)        77               \n","  pp  (perisplasm)                                    52\n","  imU (inner membrane, uncleavable signal sequence)   35\n","  om  (outer membrane)                                20\n","  omL (outer membrane lipoprotein)                     5\n","  imL (inner membrane lipoprotein)                     2\n","  imS (inner membrane, cleavable signal sequence)      2\n","```\n","You can learn more about the dataset here:\n","* Ecoli Dataset ([ecoli.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ecoli.data))\n","* Ecoli Dataset Description ([ecoli.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ecoli.names))\n"],"metadata":{"id":"uDR7bh6pvlWc"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"M32d0aCY0zHM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655166176530,"user_tz":240,"elapsed":3445,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"b98eaca4-dd35-4102-bc12-dcdb3757ee28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n","\n","Saved under ecoli (1).csv\n"]}],"source":["# Download Ecoli dataset\n","!pip install wget\n","!python -m wget -o ecoli.csv \"https://raw.githubusercontent.com/udel-cbcb/al_ml_workshop/main/data/ecoli.csv\"\n"," \n","df = pd.read_csv('ecoli.csv')"]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"_fjIWzYw11ow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert categorical variable 'site' into dummy/indicator variables.\n","ecoli_site = # Your code goes here\n","ecoli_site.head()"],"metadata":{"id":"2KZsRQ0u1zVS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5k4y2jPK0zHN"},"outputs":[],"source":["from sklearn.feature_selection import VarianceThreshold\n","# Create a VarianceThreashold object to remove features from the one hot encoded \n","# features where the variance is less than 0.15\n","vt = # Your code goes here\n","vt.fit(ecoli_site)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMhyRpt60zHO"},"outputs":[],"source":["# Show which features have been selected based on their True values and also their variance being above 0.15.\n","pd.DataFrame({'variance': vt.variances_,\n","              'select_feature': vt.get_support()},\n","            index=ecoli_site.columns).T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ELyWcm10zHP"},"outputs":[],"source":["# Get the final subset of selected features\n","ecoli_site_subset = ecoli_site.iloc[:,vt.get_support()].head()\n","ecoli_site_subset"]},{"cell_type":"markdown","metadata":{"id":"H2gQrPlY0zHP"},"source":["# Statistical Methods"]},{"cell_type":"markdown","source":["This dataset is known as the Wisconsin\n","Diagnostic Breast Cancer dataset, which is also available in its native or raw format at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic), which is the UCI Machine Learning\n","repository."],"metadata":{"id":"j06jvsPbzmFA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bPONxe90zHQ"},"outputs":[],"source":["from sklearn.datasets import load_breast_cancer\n","\n","bc_data = load_breast_cancer()\n","bc_features = pd.DataFrame(bc_data.data, columns=bc_data.feature_names)\n","bc_classes = pd.DataFrame(bc_data.target, columns=['IsMalignant'])\n","\n","# build featureset and response class labels \n","bc_X = np.array(bc_features)\n","bc_y = np.array(bc_classes).T[0]\n","print('Feature set shape:', bc_X.shape)\n","print('Response class shape:', bc_y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05h0k4HA0zHQ"},"outputs":[],"source":["np.set_printoptions(threshold=30)\n","print('Feature set data [shape: '+str(bc_X.shape)+']')\n","print(np.round(bc_X, 2), '\\n')\n","print('Feature names:')\n","print(np.array(bc_features.columns), '\\n')\n","print('Predictor Class label data [shape: '+str(bc_y.shape)+']')\n","print(bc_y, '\\n')\n","print('Predictor name:', np.array(bc_classes.columns))\n","np.set_printoptions(threshold=pt)"]},{"cell_type":"markdown","source":["The response class variable is a binary\n","class where 1 indicates the tumor detected was benign and 0 indicates it was malignant. We can also see\n","the 30 features that are real valued numbers that describe characteristics of cell nuclei present in digitized\n","images of breast mass."],"metadata":{"id":"b30OhI-6z9o6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jgu8o5Qe0zHR"},"outputs":[],"source":["from sklearn.feature_selection import chi2, SelectKBest\n","\n","# use the chi-square test on this feature set and select the top 15 best features out of the 30 features.\n","skb = # Your code goes here\n","skb.fit(bc_X, bc_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qiKpeV4O0zHS"},"outputs":[],"source":["# sort the scores to see the most relevant features\n","feature_scores = [(item, score) for item, score in zip(bc_data.feature_names, skb.scores_)]\n","sorted(feature_scores, key=lambda x: -x[1])[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MynmGwFY0zHS"},"outputs":[],"source":["# create a subset of the selected features obtained from our original feature set of features with the help of the chi-square test\n","select_features_kbest = skb.get_support()\n","feature_names_kbest = bc_data.feature_names[select_features_kbest]\n","feature_subset_df = bc_features[feature_names_kbest]\n","bc_SX = np.array(feature_subset_df)\n","print(bc_SX.shape)\n","print(feature_names_kbest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fv9OqBq_0zHT"},"outputs":[],"source":["# Selected feature subset of the Wisconsin Diagnostic Breast Cancer dataset using chi-square tests\n","np.round(feature_subset_df.iloc[20:25], 2)"]},{"cell_type":"markdown","source":["Letâ€™s now build a simple\n","classification model using logistic regression on the original feature set of 30 features and compare the\n","model accuracy performance with another model built using our selected 15 features. For model evaluation,\n","we will use the accuracy metric (percent of correct predictions) and use a five-fold cross-validation scheme. The main idea here is to compare the model\n","prediction performance between models trained on different feature sets."],"metadata":{"id":"ZKXCoW2i1Beo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdMUX7D90zHT"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# build logistic regression model with max_iter of 1000\n","lr = # Your code goes here\n","\n","# evaluating accuracy for model built on full featureset\n","full_feat_acc = np.average(cross_val_score(lr, bc_X, bc_y, scoring='accuracy', cv=5))\n","# evaluating accuracy for model built on selected featureset\n","sel_feat_acc = np.average(cross_val_score(lr, bc_SX, bc_y, scoring='accuracy', cv=5))\n","\n","print('Model accuracy statistics with 5-fold cross validation')\n","print('Model accuracy with complete feature set', bc_X.shape, ':', full_feat_acc)\n","print('Model accuracy with selected feature set', bc_SX.shape, ':', sel_feat_acc)"]},{"cell_type":"markdown","source":["The accuracy metrics clearly show us that we actually built a better model \n","when trained on the selected 15 feature subset as compared to the model built with the original 30 features."],"metadata":{"id":"BCwlRf6z237u"}},{"cell_type":"markdown","metadata":{"id":"dS0YYANY0zHU"},"source":["# Recursive Feature Elimination\n","\n","Recursive Feature Elimination, also known as RFE, is a popular wrapper based feature selection technique,\n","which allows you to recursively keep eliminating lower scored features till you arrive at the specific feature subset count. The basic idea is to start off with a specific Machine Learning estimator\n","like the Logistic Regression algorithm we used for our classification needs. Next we take the entire feature set\n","of 30 features and the corresponding response class variables. RFE aims to assign weights to these features\n","based on the model fit. Features with the smallest weights are pruned out and then a model is fit again on the remaining features to obtain the new weights or scores. This process is recursively carried out multiple\n","times and each time features with the lowest scores/weights are eliminated, until the pruned feature subset\n","contains the desired number of features that the user wanted to select (this is taken as an input parameter at\n","the start). This strategy is also popularly known as backward elimination."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WDa4LzJp0zHU"},"outputs":[],"source":["from sklearn.feature_selection import RFE\n","\n","lr = LogisticRegression()\n","# select the top 15 features on our breast cancer dataset now using RFE.\n","rfe = # Your code goes here (hint: use RFE())\n","rfe.fit(bc_X, bc_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wzJIE9W0zHU"},"outputs":[],"source":["# obtain the final selected features\n","select_features_rfe = rfe.get_support()\n","feature_names_rfe = bc_data.feature_names[select_features_rfe]\n","print(feature_names_rfe)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hiyEU0YP0zHU"},"outputs":[],"source":["# compare this feature subset with the one we obtained using statistical tests \n","# in the previous section and see which features are common among both these subsets\n","set(feature_names_kbest) & set(feature_names_rfe)"]},{"cell_type":"markdown","metadata":{"id":"83fxrTpB0zHV"},"source":["# Model based selection\n","\n","Tree based models like decision trees and ensemble models like random forests (ensemble of trees) can\n","be utilized not just for modeling alone but for feature selection. These models can be used to compute\n","feature importances when building the model that can in turn be used for selecting the best features and\n","discarding irrelevant features with lower scores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uwd3OzqN0zHV"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# use the random forest model to score and rank features based on their importance.\n","rfc = # Your code goes here\n","rfc.fit(bc_X, bc_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PW9vKSTe0zHV"},"outputs":[],"source":["# Use random forest estimator to score the features based on their importance\n","# and we display the top 10 most important features based on this score\n","importance_scores = rfc.feature_importances_\n","feature_importances = [(feature, score) for feature, score in zip(bc_data.feature_names, importance_scores)]\n","sorted(feature_importances, key=lambda x: -x[1])[:10]"]},{"cell_type":"markdown","source":["You can now use a threshold based parameter to filter out the top n features as needed or you can even\n","make use of the SelectFromModel meta-transformer provided by scikit-learn by using it as a wrapper on\n","top of this model."],"metadata":{"id":"YaBCkhIv7kWN"}}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python [conda root]","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"Day_3_Exericse_Feature_Selection.ipynb","provenance":[{"file_id":"https://github.com/udel-cbcb/al_ml_workshop/blob/main/Day_3/Exercises/Day_3_Exericse_Feature_Selection_Solution.ipynb","timestamp":1652988720728},{"file_id":"https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch04_Feature_Engineering_and_Selection/Feature%20Selection.ipynb","timestamp":1647033217825}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}