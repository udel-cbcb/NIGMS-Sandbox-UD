{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Feature Scaling Exercise Solution**\n",
    "\n",
    "Data processing is often described to be the toughest task or step in building any Machine Learning system by\n",
    "data scientists with the need of both domain knowledge as well as mathematical transformations.\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Feature Scaling\n",
    "\n",
    "Using the raw values as input features might make models biased\n",
    "toward features having really high magnitude values. These models are typically sensitive to the magnitude or\n",
    "scale of features like linear or logistic regression. Other models like tree based methods can still work without\n",
    "feature scaling. However it is still recommended to normalize and scale down the features with feature scaling,\n",
    "especially if you want to try out multiple Machine Learning algorithms on input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary dependencies and settings\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample RNASeq data\n",
    "\n",
    "We have five genes with their RNASeq FPKMs (Fragments Per Kilobase of exon per Million reads). It is quite evident that some genes have\n",
    "been expressed a lot more than the others, giving a rise to values of high scale and magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpkms = pd.DataFrame([1295., 25., 19000., 5., 1., 300.], columns=['fpkms'])\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized Scaling\n",
    "\n",
    "The standard scaler tries to standardize each value in a feature column by removing the mean and scaling\n",
    "the variance to be 1 from the values. This is also known as centering and scaling and can be denoted\n",
    "mathematically as $SS(X_i) = \\frac{x_i - \\mu}{\\sigma}$, where each value in feature $X$ is subtracted by the mean $\\mu_i$ and the resultant is divided by the standard deviation $\\sigma_x$. This is also known as $Z$-scsore scaling. We can aslo divide the resultant by the variance instead of the standard deviation if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler\n",
    "ss = StandardScaler()\n",
    "# Add a 'zscore' column\n",
    "fpkms['zscore'] = ss.fit_transform(fpkms[['fpkms']])\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can manually use the formula to compute the same result\n",
    "fw = np.array(fpkms['fpkms'])\n",
    "(fw[0] - np.mean(fw)) / np.std(fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling\n",
    "With min-max scaling, we can transform and scale our feature values such that each value is within the\n",
    "range of [0, 1]. Min-Max Scaler can be represented as $MMS(X_i)=\\frac{x_i - min(x)}{max(x) - min(x)}$, where we scale aach value in the feature $X$ by substracting it from the minimum value in the feature $min(X)$ and dividing the resultant by the difference between the maximum and minimum values in the feature $max(X)-min(X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "fpkms['minmax'] = mms.fit_transform(fpkms[['fpkms']])\n",
    "fpkms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can manually use the formula to compute the same result\n",
    "(fw[0] - np.min(fw)) / (np.max(fw) - np.min(fw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaling\n",
    "The disadvantage of min-max scaling is that often the presence of outliers affects the scaled values for any\n",
    "feature. Robust scaling tries to use specific statistical measures to scale features without being affected by\n",
    "outliers. Mathematically this scaler can be represented as $\\frac{x_i - median(x)}{IQR_{(1,3)}(x)}$, where we scale each value of feature $X$ by subtracting the median of $X$ and dividing the resultant by the IQR (Inter-Quartile Range) of $X$ which is the range (difference) between the first quartile (25th percentile) and the third quartile (75th percentile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RobustScaler\n",
    "rs = RobustScaler()\n",
    "fpkms['robust'] = rs.fit_transform(fpkms[['fpkms']])\n",
    "fpkms"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
