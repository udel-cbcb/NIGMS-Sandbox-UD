{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Select Categorical Input Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#What is Feature Selection?\n",
    "Feature selection is the process of identifying and selecting a subset of input features that are\n",
    "most relevant to the target variable. Feature selection is often straightforward when working\n",
    "with real-valued data, such as using the Pearson's correlation coefficient, but can be challenging\n",
    "when working with categorical data. The two most commonly used feature selection methods for\n",
    "categorical input data when the target variable is also categorical (e.g. classification predictive\n",
    "modeling) are the **chi-squared statistic** and the **mutual information statistic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial,\n",
    "you will discover how to perform feature selection with categorical input data. After completing\n",
    "this tutorial, you will learn:\n",
    "* The breast cancer predictive modeling problem with categorical inputs and binary classification target variable.\n",
    "* How to evaluate the importance of categorical features using the chi-squared and mutual\n",
    "information statistics.\n",
    "* How to perform feature selection for categorical data when      fitting and evaluating a\n",
    "classification model.\n",
    "\n",
    "Adapted from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Breast Cancer Dataset\n",
    "Breast cancer dataset classifies breast cancer\n",
    "patient data as either a recurrence or no recurrence of cancer. \n",
    "\n",
    "```\n",
    "Number of Instances: 286\n",
    "Number of Attributes: 9 + the class attribute\n",
    "Attribute Information:\n",
    "   1. Class: no-recurrence-events, recurrence-events\n",
    "   2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "   3. menopause: lt40, ge40, premeno.\n",
    "   4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
    "   5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
    "   6. node-caps: yes, no.\n",
    "   7. deg-malig: 1, 2, 3.\n",
    "   8. breast: left, right.\n",
    "   9. breast-quad: left-up, left-low, right-up,\tright-low, central.\n",
    "  10. irradiat:\tyes, no.\n",
    "Missing Attribute Values: (denoted by \"?\")\n",
    "   Attribute #:  Number of instances with missing values:\n",
    "   6.             8\n",
    "   9.             1.\n",
    "Class Distribution:\n",
    "    1. no-recurrence-events: 201 instances\n",
    "    2. recurrence-events: 85 instances \n",
    "```\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "* Breast Cancer Dataset ([breast-cancer.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv))\n",
    "* Breast Cancer Dataset Description ([breast-cancer.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Download Breast Cancer data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!python -m wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\" -o breast-cancer.csv\n",
    "!python -m wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names\" -o breast-cancer.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Loading and encoding the categorical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of loading and preparing the breast cancer dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "  # load the dataset\n",
    "  dataset = read_csv(filename, header=None)\n",
    "  # retrieve array\n",
    "  data = dataset.values\n",
    "  # split into input and output variables\n",
    "  X = data[:, :-1]\n",
    "  y = data[:,-1]\n",
    "  # format all fields as string\n",
    "  X = X.astype(str)\n",
    "  return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "  oe = OrdinalEncoder() # encode each variable to integers\n",
    "  oe.fit(X_train)\n",
    "  X_train_enc = oe.transform(X_train)\n",
    "  X_test_enc = oe.transform(X_test)\n",
    "  return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "  le = LabelEncoder() # LabelEncoder is designed for encoding a single variable\n",
    "  le.fit(y_train)\n",
    "  y_train_enc = le.transform(y_train)\n",
    "  y_test_enc = le.transform(y_test)\n",
    "  return y_train_enc, y_test_enc\n",
    "  \n",
    "# load the dataset\n",
    "X, y = load_dataset('breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "# summarize\n",
    "print('Train', X_train_enc.shape, y_train_enc.shape)\n",
    "print('Test', X_test_enc.shape, y_test_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 191 examples for training and 95 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Categorical Feature Selection\n",
    "There are two popular feature selection techniques that can be used for categorical input data\n",
    "and a categorical (class) target variable. They are:\n",
    "* Chi-Squared Statistic.\n",
    "* Mutual Information Statistic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Chi-Squared Feature Selection**\n",
    "Pearson's chi-squared statistical hypothesis\n",
    "test is an example of a test for independence between categorical variables. The results of this\n",
    "test can be used for feature selection, where those features that are independent of the target\n",
    "variable can be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of chi squared feature selection for categorical data\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "  # load the dataset as a pandas DataFrame\n",
    "  dataset = read_csv(filename, header=None)\n",
    "  # retrieve numpy array\n",
    "  data = dataset.values\n",
    "  # split into input (X) and output (y) variables\n",
    "  X = data[:, :-1]\n",
    "  y = data[:,-1]\n",
    "  # format all fields as string\n",
    "  X = X.astype(str)\n",
    "  return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "  oe = OrdinalEncoder()\n",
    "  oe.fit(X_train)\n",
    "  X_train_enc = oe.transform(X_train)\n",
    "  X_test_enc = oe.transform(X_test)\n",
    "  return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "  le = LabelEncoder()\n",
    "  le.fit(y_train)\n",
    "  y_train_enc = le.transform(y_train)\n",
    "  y_test_enc = le.transform(y_test)\n",
    "  return y_train_enc, y_test_enc\n",
    "  \n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "  # Select features according to the k highest scores.\n",
    "  # k : int or \"all\", default=10, Number of top features to select.\n",
    "  # The \"all\" option bypasses selection, for use in a parameter search.\n",
    "  fs = SelectKBest(score_func=chi2, k='all')\n",
    "  # Run score function on (X, y) and get the appropriate features.\n",
    "  fs.fit(X_train, y_train)\n",
    "  # Reduce X to the selected features.\n",
    "  X_train_fs = fs.transform(X_train)\n",
    "  X_test_fs = fs.transform(X_test)\n",
    "  return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "  print('Feature %d: %f' % (i, fs.scores_[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see the scores are small and it is hard to get an idea from the number\n",
    "alone as to which features are more relevant. Perhaps features 3, 4, 5, and 8 are most relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bar chart of the feature importance scores for each input feature is created. This clearly\n",
    "shows that feature 3 might be the most relevant (according to chi-squared) and that perhaps\n",
    "four of the nine input features are the most relevant. We could set k = 4 when configuring the\n",
    "SelectKBest to select these top four features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Mutual Information Feature Selection**\n",
    "Mutual information from the field of information theory is the application of information gain\n",
    "(typically used in the construction of decision trees) to feature selection. Mutual information is\n",
    "calculated between two variables and measures the reduction in uncertainty for one variable\n",
    "given a known value of the other variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of mutual information feature selection for categorical data\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from matplotlib import pyplot\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "  # load the dataset as a pandas DataFrame\n",
    "  dataset = read_csv(filename, header=None)\n",
    "  # retrieve numpy array\n",
    "  data = dataset.values\n",
    "  # split into input (X) and output (y) variables\n",
    "  X = data[:, :-1]\n",
    "  y = data[:,-1]\n",
    "  # format all fields as string\n",
    "  X = X.astype(str)\n",
    "  return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "  oe = OrdinalEncoder()\n",
    "  oe.fit(X_train)\n",
    "  X_train_enc = oe.transform(X_train)\n",
    "  X_test_enc = oe.transform(X_test)\n",
    "  return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "  le = LabelEncoder()\n",
    "  le.fit(y_train)\n",
    "  y_train_enc = le.transform(y_train)\n",
    "  y_test_enc = le.transform(y_test)\n",
    "  return y_train_enc, y_test_enc\n",
    "  \n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "  fs = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "  fs.fit(X_train, y_train)\n",
    "  X_train_fs = fs.transform(X_train)\n",
    "  X_test_fs = fs.transform(X_test)\n",
    "  return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "  print('Feature %d: %f' % (i, fs.scores_[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that some of the features have a very low score, suggesting that\n",
    "perhaps they can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bar chart of the feature importance scores for each input feature is created. Importantly,\n",
    "a different mixture of features is promoted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Modeling With Selected Features\n",
    "A robust approach is to evaluate models using different\n",
    "feature selection methods (and numbers of features) and select the method that results in a\n",
    "model with the best performance. We will evaluate a Logistic Regression model\n",
    "with all features compared to a model built from features selected by chi-squared and those\n",
    "features selected via mutual information. Logistic regression is a good model for testing feature\n",
    "selection methods as it can perform better if irrelevant features are removed from the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Model Built Using All Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model using all input features\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "  # load the dataset as a pandas DataFrame\n",
    "  dataset = read_csv(filename, header=None)\n",
    "  # retrieve numpy array\n",
    "  data = dataset.values\n",
    "  # split into input (X) and output (y) variables\n",
    "  X = data[:, :-1]\n",
    "  y = data[:,-1]\n",
    "  # format all fields as string\n",
    "  X = X.astype(str)\n",
    "  return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "  oe = OrdinalEncoder()\n",
    "  oe.fit(X_train)\n",
    "  X_train_enc = oe.transform(X_train)\n",
    "  X_test_enc = oe.transform(X_test)\n",
    "  return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "  le = LabelEncoder()\n",
    "  le.fit(y_train)\n",
    "  y_train_enc = le.transform(y_train)\n",
    "  y_test_enc = le.transform(y_test)\n",
    "  return y_train_enc, y_test_enc\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train_enc, y_train_enc)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_enc)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the model achieves a classification accuracy of about 75 percent.\n",
    "We would prefer to use a subset of features that achieves a classification accuracy that is as\n",
    "good or better than this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Model Built Using Chi-Squared Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model fit using chi squared input features\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "  # load the dataset as a pandas DataFrame\n",
    "  dataset = read_csv(filename, header=None)\n",
    "  # retrieve numpy array\n",
    "  data = dataset.values\n",
    "  # split into input (X) and output (y) variables\n",
    "  X = data[:, :-1]\n",
    "  y = data[:,-1]\n",
    "  # format all fields as string\n",
    "  X = X.astype(str)\n",
    "  return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "  oe = OrdinalEncoder()\n",
    "  oe.fit(X_train)\n",
    "  X_train_enc = oe.transform(X_train)\n",
    "  X_test_enc = oe.transform(X_test)\n",
    "  return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "  le = LabelEncoder()\n",
    "  le.fit(y_train)\n",
    "  y_train_enc = le.transform(y_train)\n",
    "  y_test_enc = le.transform(y_test)\n",
    "  return y_train_enc, y_test_enc\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "  fs = SelectKBest(score_func=chi2, k=4)\n",
    "  fs.fit(X_train, y_train)\n",
    "  X_train_fs = fs.transform(X_train)\n",
    "  X_test_fs = fs.transform(X_test)\n",
    "  return X_train_fs, X_test_fs\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train_fs, y_train_enc)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that the model achieved an accuracy of about 74 percent, a slight drop in\n",
    "performance. It is possible that some of the features removed are, in fact, adding value directly\n",
    "or in concert with the selected features. At this stage, we would probably prefer to use all of\n",
    "the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Model Built Using Mutual Information Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of a model fit using mutual information input features\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "  # load the dataset as a pandas DataFrame\n",
    "  dataset = read_csv(filename, header=None)\n",
    "  # retrieve numpy array\n",
    "  data = dataset.values\n",
    "  # split into input (X) and output (y) variables\n",
    "  X = data[:, :-1]\n",
    "  y = data[:,-1]\n",
    "  # format all fields as string\n",
    "  X = X.astype(str)\n",
    "  return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "  oe = OrdinalEncoder()\n",
    "  oe.fit(X_train)\n",
    "  X_train_enc = oe.transform(X_train)\n",
    "  X_test_enc = oe.transform(X_test)\n",
    "  return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "  le = LabelEncoder()\n",
    "  le.fit(y_train)\n",
    "  y_train_enc = le.transform(y_train)\n",
    "  y_test_enc = le.transform(y_test)\n",
    "  return y_train_enc, y_test_enc\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "  fs = SelectKBest(score_func=mutual_info_classif, k=4)\n",
    "  fs.fit(X_train, y_train)\n",
    "  X_train_fs = fs.transform(X_train)\n",
    "  X_test_fs = fs.transform(X_test)\n",
    "  return X_train_fs, X_test_fs\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train_fs, y_train_enc)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test_enc, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see drop in classification accuracy. To be sure that\n",
    "the effect is real, it would be a good idea to repeat each experiment multiple times and compare\n",
    "the mean performance. It may also be a good idea to explore using k-fold cross-validation\n",
    "instead of a simple train/test split."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
