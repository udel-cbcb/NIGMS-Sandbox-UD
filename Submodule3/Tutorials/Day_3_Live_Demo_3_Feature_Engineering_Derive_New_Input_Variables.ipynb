{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Derive New Input Variables**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, the input features for a predictive modeling task interact in unexpected and often\n",
    "non-linear ways. These interactions can be identified and modeled by a learning algorithm.\n",
    "\n",
    "Another approach is to engineer new features that expose these interactions and see if they\n",
    "improve model performance. Additionally, transforms like raising input variables to a power\n",
    "can help to better expose the important relationships between input variables and the target\n",
    "variable.\n",
    "\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "\n",
    "* Some machine learning algorithms prefer or perform better with polynomial input features.\n",
    "* How to use the polynomial features transform to create new versions of input variables for\n",
    "predictive modeling.\n",
    "* How the degree of the polynomial impacts the number of input features created by the\n",
    "transform.\n",
    "\n",
    "Apdated from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Polynomial Features\n",
    "\n",
    "Polynomial features are those features created by raising existing features to an exponent. For\n",
    "example, if a dataset had one input feature X, then a polynomial feature would be the addition\n",
    "of a new feature (column) where values were calculated by squaring the values in X, e.g. $X^2$.\n",
    "This process can be repeated for each input variable in the dataset, creating a transformed\n",
    "version of each. As such, polynomial features are a type of feature engineering, e.g. the creation\n",
    "of new input features based on the existing features. The degree of the polynomial is used to\n",
    "control the number of features added, e.g. a degree of 3 will add two new variables for each\n",
    "input variable. Typically a small degree is used such as 2 or 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also common to add new variables that represent the interaction between features, e.g. \n",
    "a new column that represents one variable multiplied by another. This too can be repeated\n",
    "for each input variable creating a new interaction variable for each pair of input variables. A\n",
    "squared or cubed version of an input variable will change the probability distribution, separating\n",
    "the small and large values, a separation that is increased with the size of the exponent.\n",
    "\n",
    "This separation can help some machine learning algorithms make better predictions and is\n",
    "common for regression predictive modeling tasks and generally tasks that have numerical input\n",
    "variables. Typically linear algorithms, such as linear regression and logistic regression, respond\n",
    "well to the use of polynomial input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Polynomial Feature Transform\n",
    "The polynomial features transform is available in the scikit-learn Python machine learning\n",
    "library via the **PolynomialFeatures** class.\n",
    "\n",
    "It generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate the types of polynomial features created\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# define the dataset\n",
    "data = asarray([[2,3],[2,3],[2,3]])\n",
    "print(data)\n",
    "# perform a polynomial features transform of the dataset\n",
    "# generate polynomial and interaction features.\n",
    "# 'degree' specifies the maximal degree of the polynomial features\n",
    "trans = PolynomialFeatures(degree=2)\n",
    "data = trans.fit_transform(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the raw data with two features (columns) and each feature\n",
    "has the same value, either 2 or 3. Then the polynomial features are created, resulting in six\n",
    "features, matching what was described above.\n",
    "\n",
    "The *degree* argument controls the number of features created and defaults to 2. The\n",
    "*interaction_only* argument means that only the raw values (degree 1) and the interaction\n",
    "(pairs of values multiplied with each other) are included, defaulting to False. The *include_bias*\n",
    "argument defaults to True to include the bias feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sonar Dataset\n",
    "The sonar dataset is a standard machine learning dataset for binary classification. It involves\n",
    "60 real-valued inputs and a two-class target variable. The data set contains\n",
    "bouncing sonar\n",
    "signals off a metal cylinder or rocks obtained from a variety of different aspect angles. Each number\n",
    "represents the energy within a particular frequency band, integrated over\n",
    "a certain period of time. There are 208 examples in the dataset\n",
    "and the classes are reasonably balanced. The dataset describes sonar returns of rocks or simulated mines. You can learn more\n",
    "about the dataset from here:\n",
    "* Sonar Dataset ([sonar.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv))\n",
    "* Sonar Dataset Description ([sonar.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Download Sonar data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!python -m wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\" -o sonar.csv\n",
    "!python -m wget \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.names\" -o sonar.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Summarizing the variables from the sonar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the sonar dataset\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "dataset = read_csv('sonar.csv', header=None)\n",
    "print(dataset.head())\n",
    "# summarize the shape of the dataset\n",
    "print(dataset.shape)\n",
    "# summarize each variable\n",
    "print(dataset.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the 60\n",
    "input variables, one output variable, and 208 rows of data. A statistical summary of the input\n",
    "variables is provided showing that values are numeric and range approximately from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the variables\n",
    "fig = dataset.hist(xlabelsize=4, ylabelsize=4)\n",
    "[x.title.set_size(4) for x in fig.ravel()]\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a histogram is created for each input variable. If we ignore the clutter of the plots and\n",
    "focus on the histograms themselves, we can see that many variables have a skewed distribution.\n",
    "The dataset provides a good candidate for using a quantile transform to make the variables\n",
    "more-Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit and evaluate a machine learning model on the raw dataset. We will use\n",
    "a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated\n",
    "stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the raw sonar dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# KFold \n",
    "#   is a cross-validator that divides the dataset into k folds.\n",
    "# Stratified\n",
    "#   is to ensure that each fold of dataset has the same proportion of observations with a given label.\n",
    "# Repeated \n",
    "#   provides a way to improve the estimated performance of a machine learning model. \n",
    "# This involves simply repeating the cross-validation procedure multiple times and reporting the mean \n",
    "# result across all folds from all runs. This mean result is expected to be a more accurate estimate \n",
    "# of the true unknown underlying mean performance of the model on the dataset, as calculated using the standard error.\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load dataset\n",
    "dataset = read_csv('sonar.csv', header=None)\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "# define and configure the model\n",
    "# n_neighbors : int, default=5\n",
    "model = KNeighborsClassifier()\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report model performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the model achieved a mean classification accuracy of about 79.7\n",
    "percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Polynomial Feature Transform\n",
    "We can apply the polynomial features transform to the Sonar dataset directly. In this case, we\n",
    "will use a degree of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a polynomial features transform of the sonar dataset\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# load dataset\n",
    "dataset = read_csv('sonar.csv', header=None)\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "# perform a polynomial features transform of the dataset\n",
    "trans = PolynomialFeatures(degree=3)\n",
    "data = trans.fit_transform(data)\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "# summarize\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We\n",
    "can see that our features increased from 61 (60 input features) for the raw dataset to 39,711\n",
    "features (39,710 input features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the same KNN model as the previous section, but in this case on a\n",
    "polynomial features transform of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the sonar dataset with polynomial features transform\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataset = read_csv('sonar.csv', header=None)\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "# define the pipeline\n",
    "trans = PolynomialFeatures(degree=3)\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)])\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the polynomial features transform results in a lift in\n",
    "performance from 79.7 percent accuracy without the transform to about 80.0 percent with the\n",
    "transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Effect of Polynomial Degree\n",
    "The degree of the polynomial dramatically increases the number of input features. To get an\n",
    "idea of how much this impacts the number of features, we can perform the transform with a\n",
    "range of different degrees and compare the number of features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the effect of the degree on the number of created features\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset(filename):\n",
    "  # load dataset\n",
    "  dataset = read_csv(filename, header=None)\n",
    "  data = dataset.values\n",
    "  # separate into input and output columns\n",
    "  X, y = data[:, :-1], data[:, -1]\n",
    "  # ensure inputs are floats and output is an integer label\n",
    "  X = X.astype('float32')\n",
    "  y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "  return X, y\n",
    "  \n",
    "# define dataset\n",
    "X, y = get_dataset('sonar.csv')\n",
    "# calculate change in number of features\n",
    "num_features = list()\n",
    "degress = [i for i in range(1, 5)]\n",
    "for d in degress:\n",
    "  # create transform\n",
    "  trans = PolynomialFeatures(degree=d)\n",
    "  # fit and transform\n",
    "  data = trans.fit_transform(X)\n",
    "  # record number of features\n",
    "  num_features.append(data.shape[1])\n",
    "  # summarize\n",
    "  print('Degree: %d, Features: %d' % (d, data.shape[1]))\n",
    "# plot degree vs number of features\n",
    "pyplot.plot(degress, num_features)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a degree of 1 has no effect and that the number of features dramatically\n",
    "increases from 2 through to 4. This highlights that for anything other than very small datasets,\n",
    "a degree of 2 or 3 should be used to avoid a dramatic increase in input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More features may result in more overfitting, and in turn, worse results. It may be a good\n",
    "idea to treat the degree for the polynomial features transform as a hyperparameter and test\n",
    "different values for your dataset. The example below explores degree values from 1 to 4 and\n",
    "evaluates their effect on classification accuracy with the chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the effect of degree on accuracy for the polynomial features transform\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset(filename):\n",
    "  # load dataset\n",
    "  dataset = read_csv(filename, header=None)\n",
    "  data = dataset.values\n",
    "  # separate into input and output columns\n",
    "  X, y = data[:, :-1], data[:, -1]\n",
    "  # ensure inputs are floats and output is an integer label\n",
    "  X = X.astype('float32')\n",
    "  y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "  return X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "  models = dict()\n",
    "  for d in range(1,5):\n",
    "  # define the pipeline\n",
    "    trans = PolynomialFeatures(degree=d)\n",
    "    model = KNeighborsClassifier()\n",
    "    models[str(d)] = Pipeline(steps=[('t', trans), ('m', model)])\n",
    "  return models\n",
    "  \n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "  scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "  return scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset('sonar.csv')\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "  scores = evaluate_model(model, X, y)\n",
    "  results.append(scores)\n",
    "  names.append(name)\n",
    "  print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that performance is generally worse than no transform (degree\n",
    "1) except for a degree 3. It might be interesting to explore scaling the data before or after\n",
    "performing the transform to see how it impacts model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box and whisker plots can be created to summarize the classification accuracy scores for each\n",
    "polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that performance remains \n",
    "flat, perhaps with the first signs of\n",
    "overfitting with a degree of 4."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
