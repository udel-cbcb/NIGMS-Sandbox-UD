{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Types Analysis Exercise Solution\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module explores predicting wine types (red vs white) using machine learning techniques. We'll use a dataset containing various chemical properties of wines to build and evaluate a classification model.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn to prepare and merge wine datasets for classification\n",
    "- Build a logistic regression model to predict wine types\n",
    "- Evaluate model performance using various metrics\n",
    "- Interpret model results through feature importance and visualizations\n",
    "- Understand how to handle binary classification problems with imbalanced classes\n",
    "\n",
    "### Tasks to complete:\n",
    "\n",
    "- Train logistic regression model\n",
    "- Evaluate model performance metrics\n",
    "- Generate and analyze feature importance\n",
    "- Create ROC curve visualization\n",
    "- Plot model decision surface\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python programming environment\n",
    "- Basic understanding of statistical and machine learning concepts\n",
    "- Familiarity with common ML libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "# We wil use matplotlib and seaborn for exploratory data analysis and visualizations\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interp\n",
    "from skater.core.explanations import Interpretation\n",
    "from skater.model import InMemoryModel\n",
    "from sklearn import metrics\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and merge datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winequality_white_data = \"../../Data/winequality-white.csv\"\n",
    "winequality_red_data = \"../../Data/winequality-red.csv\"\n",
    "white_wine = pd.read_csv(winequality_white_data, sep=\";\")\n",
    "red_wine = pd.read_csv(winequality_red_data, sep=\";\")\n",
    "\n",
    "# store wine type as an attribute\n",
    "red_wine[\"wine_type\"] = \"red\"\n",
    "white_wine[\"wine_type\"] = \"white\"\n",
    "# bucket wine quality scores into qualitative quality labels\n",
    "# Wine quality scores of 3, 4, and 5 are mapped to low quality,\n",
    "# 6 and 7 are mapped to medium quality, 8 and 9 are mapped to high quality\n",
    "# wines under the quality_label attribute.\n",
    "\n",
    "def categorize_quality(quality):\n",
    "    \"\"\"Categorize wine quality into low, medium, or high.\"\"\"\n",
    "    if quality <= 5:\n",
    "        return \"low\"\n",
    "    elif quality <= 7:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "red_wine[\"quality_label\"] = red_wine[\"quality\"].apply(categorize_quality)\n",
    "white_wine[\"quality_label\"] = white_wine[\"quality\"].apply(categorize_quality)\n",
    "\n",
    "# Merge datasets and shuffle\n",
    "wines = pd.concat([red_wine, white_wine]).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand dataset features and values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(white_wine.shape, red_wine.shape)\n",
    "print(wines.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4898 white wine data points and 1599 red wine data points. The\n",
    "merged dataset contains a total of 6497 data points and we also get an idea of numeric and categorical\n",
    "attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letâ€™s take a peek at our dataset to see some sample data points.\n",
    "wines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilty functions for model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for model evaluation\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculate and print accuracy, precision, recall, and F1 score.\"\"\"\n",
    "    print(\"Accuracy:\", np.round(metrics.accuracy_score(true_labels, predicted_labels), 4))\n",
    "    print(\"Precision:\", np.round(metrics.precision_score(true_labels, predicted_labels, average=\"weighted\"), 4))\n",
    "    print(\"Recall:\", np.round(metrics.recall_score(true_labels, predicted_labels, average=\"weighted\"), 4))\n",
    "    print(\"F1 Score:\", np.round(metrics.f1_score(true_labels, predicted_labels, average=\"weighted\"), 4))\n",
    "\n",
    "\n",
    "def display_classification_report(true_labels, predicted_labels, classes):\n",
    "    \"\"\"Display a detailed classification report.\"\"\"\n",
    "    report = metrics.classification_report(true_labels, predicted_labels, labels=classes)\n",
    "    print(report)\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes):\n",
    "    \"\"\"Display a confusion matrix.\"\"\"\n",
    "    cm = metrics.confusion_matrix(true_labels, predicted_labels, labels=classes)\n",
    "    cm_frame = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_frame)\n",
    "\n",
    "\n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes):\n",
    "    \"\"\"Display model performance metrics.\"\"\"\n",
    "    print(\"Model Performance Metrics:\")\n",
    "    print(\"-\" * 30)\n",
    "    get_metrics(true_labels, predicted_labels)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"-\" * 30)\n",
    "    display_classification_report(true_labels, predicted_labels, classes)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"-\" * 30)\n",
    "    display_confusion_matrix(true_labels, predicted_labels, classes)\n",
    "\n",
    "\n",
    "def plot_model_roc_curve(clf, features, true_labels, class_names):\n",
    "    \"\"\"Plot ROC curve for the model.\"\"\"\n",
    "    y_test = label_binarize(true_labels, classes=class_names)\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        y_score = clf.predict_proba(features)\n",
    "    else:\n",
    "        y_score = clf.decision_function(features)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.2f})\", linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", linewidth=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_decision_surface(\n",
    "    clf,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    plot_step=0.02,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    markers=None,\n",
    "    alphas=None,\n",
    "    colors=None,\n",
    "):\n",
    "    if train_features.shape[1] != 2:\n",
    "        raise ValueError(\"X_train should have exactly 2 columnns!\")\n",
    "\n",
    "    x_min, x_max = (\n",
    "        train_features[:, 0].min() - plot_step,\n",
    "        train_features[:, 0].max() + plot_step,\n",
    "    )\n",
    "    y_min, y_max = (\n",
    "        train_features[:, 1].min() - plot_step,\n",
    "        train_features[:, 1].max() + plot_step,\n",
    "    )\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\n",
    "    )\n",
    "\n",
    "    clf_est = clone(clf)\n",
    "    clf_est.fit(train_features, train_labels)\n",
    "    if hasattr(clf_est, \"predict_proba\"):\n",
    "        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    else:\n",
    "        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(train_labels)\n",
    "    n_classes = len(le.classes_)\n",
    "    plot_colors = \"\".join(colors) if colors else [None] * n_classes\n",
    "    label_names = le.classes_\n",
    "    markers = markers if markers else [None] * n_classes\n",
    "    alphas = alphas if alphas else [None] * n_classes\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_enc == i)\n",
    "        plt.scatter(\n",
    "            train_features[idx, 0],\n",
    "            train_features[idx, 1],\n",
    "            c=color,\n",
    "            label=label_names[i],\n",
    "            cmap=cmap,\n",
    "            edgecolors=\"black\",\n",
    "            marker=markers[i],\n",
    "            alpha=alphas[i],\n",
    "        )\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Wine Types\n",
    "\n",
    "We will predict the wine type based on other features. To start with, we\n",
    "will first select our necessary features and separate out the prediction class labels and prepare train and test\n",
    "datasets. We use the prefix **wtp\\_** in our variables to easily identify them as needed, where **wtp** depicts wine\n",
    "type prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "wtp_features = wines.iloc[:, :-3]\n",
    "wtp_feature_names = wtp_features.columns\n",
    "wtp_class_labels = wines[\"wine_type\"]\n",
    "\n",
    "# Split data into train and test sets\n",
    "wtp_train_X, wtp_test_X, wtp_train_y, wtp_test_y = train_test_split(\n",
    "    wtp_features, wtp_class_labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train Class Distribution:\", Counter(wtp_train_y))\n",
    "print(\"Test Class Distribution:\", Counter(wtp_test_y))\n",
    "print(\"Features:\", list(wtp_feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers show us the wine samples for each class and we can also see the feature names which will\n",
    "be used in our feature set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "We will be using a standard scaler in this\n",
    "scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "wtp_ss = StandardScaler().fit(wtp_train_X)\n",
    "\n",
    "# Scale the train set\n",
    "wtp_train_SX = wtp_ss.transform(wtp_train_X)\n",
    "\n",
    "# Scale the test set\n",
    "wtp_test_SX = wtp_ss.transform(wtp_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model using Logistic Regression\n",
    "\n",
    "Since we are dealing with a binary classification problem, one of the traditional Machine Learning\n",
    "algorithms we can use is the logistic regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp_lr = LogisticRegression()\n",
    "wtp_lr.fit(wtp_train_SX, wtp_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letâ€™s predict the wine types for our test data samples and evaluate the performance.\n",
    "wtp_lr_predictions = wtp_lr.predict(wtp_test_SX)\n",
    "display_model_performance_metrics(\n",
    "    true_labels=wtp_test_y,\n",
    "    predicted_labels=wtp_lr_predictions,\n",
    "    classes=[\"red\", \"white\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an overall F1 Score and model accuracy of 99.2%! In spite of low samples of red wine, we seem to do pretty well. In case your models do not perform\n",
    "well on other datasets due to a class imbalance problem, you can consider over-sampling or under-sampling\n",
    "techniques including sample selection as well as SMOTE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp_interpreter = Interpretation(wtp_test_SX, feature_names=wtp_features.columns)\n",
    "wtp_im_model = InMemoryModel(\n",
    "    wtp_lr.predict_proba, examples=wtp_train_SX, target_names=wtp_lr.classes_\n",
    ")\n",
    "plots = wtp_interpreter.feature_importance.plot_feature_importance(\n",
    "    wtp_im_model, ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that density, total sulfur dioxide, and residual sugar are the top three\n",
    "features that contributed toward classifying wine samples as red or white\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize model ROC Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_roc_curve(wtp_lr, wtp_test_SX, wtp_test_y, class_names=wtp_lr.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved almost 100% accuracy if you remember for this model and hence the ROC curve is almost\n",
    "perfect where we also see that the area under curve (AUC) is 1 which is perfect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model Decision Surface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = [\n",
    "    i\n",
    "    for i, feature in enumerate(wtp_feature_names)\n",
    "    if feature in [\"density\", \"total sulfur dioxide\"]\n",
    "]\n",
    "plot_model_decision_surface(\n",
    "    clf=wtp_lr,\n",
    "    train_features=wtp_train_SX[:, feature_indices],\n",
    "    train_labels=wtp_train_y,\n",
    "    plot_step=0.02,\n",
    "    cmap=plt.cm.Wistia_r,\n",
    "    markers=[\",\", \"o\"],\n",
    "    alphas=[0.9, 0.6],\n",
    "    colors=[\"r\", \"y\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that our model has learned the underlying patterns\n",
    "quite well based on just the two most important features, which it has used to separate out majority\n",
    "of the red wine samples from the white wine samples depicted by the scatter dots. There are very few\n",
    "misclassifications here and there, which are evident based on the statistics we obtained earlier in the\n",
    "confusion matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this module, we learned how to:\n",
    "\n",
    "- Successfully predict wine types using logistic regression\n",
    "- Identify key features like density, total sulfur dioxide, and residual sugar that best distinguish wine types\n",
    "- Evaluate model performance using metrics, ROC curves, and decision surfaces\n",
    "- Handle binary classification with imbalanced classes effectively\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to:\n",
    "\n",
    "- Save any generated plots and analysis results\n",
    "- Clear notebook output if sharing\n",
    "- Close dataset files\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
