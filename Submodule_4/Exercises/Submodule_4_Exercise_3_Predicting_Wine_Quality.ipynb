{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Analysis Exercise Solution\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module focuses on building predictive models to predict wine quality (low, medium and high) based on other features, following the standard classification Machine Learning pipeline.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Build and evaluate predictive models for wine quality classification\n",
    "- Apply and compare different machine learning algorithms:\n",
    "  - Decision Trees\n",
    "  - Random Forests\n",
    "  - Extreme Gradient Boosting\n",
    "- Interpret model results using:\n",
    "  - Feature importance analysis\n",
    "  - ROC curves\n",
    "  - Decision surfaces\n",
    "  - Partial dependence plots\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Train and evaluate models using:\n",
    "  - Decision Trees\n",
    "  - Random Forests\n",
    "  - XGBoost\n",
    "- Generate model interpretations and visualizations\n",
    "- Compare model performances\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python programming environment\n",
    "- Basic understanding of statistical and machine learning concepts\n",
    "- Familiarity with common ML libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "# We wil use matplotlib and seaborn for exploratory data analysis and visualizations\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from graphviz import Source\n",
    "from IPython.display import Image\n",
    "from numpy import interp  # Use numpy.interp instead of scipy.interp\n",
    "from skater.core.explanations import Interpretation\n",
    "from skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\n",
    "from skater.model import InMemoryModel\n",
    "from sklearn import metrics, tree\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Set matplotlib inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and merge datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths (consider making this configurable)\n",
    "white_wine_path = \"../../Data/winequality-white.csv\"\n",
    "red_wine_path = \"../../Data/winequality-red.csv\"\n",
    "\n",
    "# Load data, handling potential exceptions\n",
    "try:\n",
    "    white_wine = pd.read_csv(white_wine_path, sep=\";\")\n",
    "    red_wine = pd.read_csv(red_wine_path, sep=\";\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Wine data files not found. Check paths: {white_wine_path}, {red_wine_path}\")\n",
    "    exit()  # Or handle the error differently\n",
    "\n",
    "# Add wine type and quality label (combined for efficiency)\n",
    "for wine, color in [(red_wine, \"red\"), (white_wine, \"white\")]:\n",
    "    wine[\"wine_type\"] = color\n",
    "\n",
    "# bucket wine quality scores into qualitative quality labels\n",
    "# Wine quality scores of 3, 4, and 5 are mapped to low quality,\n",
    "# 6 and 7 are mapped to medium quality, 8 and 9 are mapped to high quality\n",
    "# wines under the quality_label attribute.\n",
    "red_wine[\"quality_label\"] = red_wine[\"quality\"].apply(\n",
    "    lambda value: \"low\" if value <= 5 else \"medium\" if value <= 7 else \"high\"\n",
    ")\n",
    "red_wine[\"quality_label\"] = pd.Categorical(\n",
    "    red_wine[\"quality_label\"], categories=[\"low\", \"medium\", \"high\"]\n",
    ")\n",
    "\n",
    "white_wine[\"quality_label\"] = white_wine[\"quality\"].apply(\n",
    "    lambda value: \"low\" if value <= 5 else \"medium\" if value <= 7 else \"high\"\n",
    ")\n",
    "white_wine[\"quality_label\"] = pd.Categorical(\n",
    "    white_wine[\"quality_label\"], categories=[\"low\", \"medium\", \"high\"]\n",
    ")\n",
    "\n",
    "# merge red and white wine datasets\n",
    "wines = pd.concat([red_wine, white_wine])\n",
    "# re-shuffle records just to randomize data points\n",
    "wines = wines.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# merge red and white wine datasets\n",
    "wines = pd.concat([red_wine, white_wine])\n",
    "# re-shuffle records just to randomize data points\n",
    "wines = wines.sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand dataset features and values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(white_wine.shape, red_wine.shape)\n",
    "print(wines.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4898 white wine data points and 1599 red wine data points. The\n",
    "merged dataset contains a total of 6497 data points and we also get an idea of numeric and categorical\n",
    "attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letâ€™s take a peek at our dataset to see some sample data points.\n",
    "wines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilty functions for model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print(\n",
    "        \"Accuracy:\", np.round(metrics.accuracy_score(true_labels, predicted_labels), 4)\n",
    "    )\n",
    "    print(\n",
    "        \"Precision:\",\n",
    "        np.round(\n",
    "            metrics.precision_score(true_labels, predicted_labels, average=\"weighted\"),\n",
    "            4,\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        \"Recall:\",\n",
    "        np.round(\n",
    "            metrics.recall_score(true_labels, predicted_labels, average=\"weighted\"), 4\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        \"F1 Score:\",\n",
    "        np.round(\n",
    "            metrics.f1_score(true_labels, predicted_labels, average=\"weighted\"), 4\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def display_classification_report(true_labels, predicted_labels, classes=None): # classes can be None\n",
    "    report = metrics.classification_report(y_true=true_labels, y_pred=predicted_labels, labels=classes)\n",
    "    print(report)\n",
    "\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    total_classes = len(classes)\n",
    "    level_labels = [total_classes * [0], list(range(total_classes))]\n",
    "    cm = metrics.confusion_matrix(\n",
    "        y_true=true_labels, y_pred=predicted_labels, labels=classes\n",
    "    )\n",
    "    cm_frame = pd.DataFrame(\n",
    "        data=cm,\n",
    "        columns=pd.MultiIndex(levels=[[\"Predicted:\"], classes], codes=level_labels),\n",
    "        index=pd.MultiIndex(levels=[[\"Actual:\"], classes], codes=level_labels),\n",
    "    )\n",
    "    print(cm_frame)\n",
    "\n",
    "\n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    print(\"Model Performance metrics:\")\n",
    "    print(\"-\" * 30)\n",
    "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n",
    "    print(\"\\nModel Classification report:\")\n",
    "    print(\"-\" * 30)\n",
    "    display_classification_report(\n",
    "        true_labels=true_labels, predicted_labels=predicted_labels, classes=classes\n",
    "    )\n",
    "    print(\"\\nPrediction Confusion Matrix:\")\n",
    "    print(\"-\" * 30)\n",
    "    display_confusion_matrix(\n",
    "        true_labels=true_labels, predicted_labels=predicted_labels, classes=classes\n",
    "    )\n",
    "\n",
    "def plot_model_roc_curve(\n",
    "    clf, features, true_labels, label_encoder=None, class_names=None\n",
    "):\n",
    "    ## Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    if hasattr(clf, \"classes_\"):\n",
    "        class_labels = clf.classes_\n",
    "    elif label_encoder:\n",
    "        class_labels = label_encoder.classes_\n",
    "    elif class_names:\n",
    "        class_labels = class_names\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unable to derive prediction classes, please specify class_names!\"\n",
    "        )\n",
    "    n_classes = len(np.unique(true_labels)) # directly get the number of classes\n",
    "    y_test = label_binarize(true_labels, classes=np.unique(true_labels)) # binarize based on actual classes\n",
    "    if n_classes == 2:\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            prob = clf.predict_proba(features)\n",
    "            y_score = prob[:, prob.shape[1] - 1]\n",
    "        elif hasattr(clf, \"decision_function\"):\n",
    "            prob = clf.decision_function(features)\n",
    "            y_score = prob[:, prob.shape[1] - 1]\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"Estimator doesn't have a probability or confidence scoring system!\"\n",
    "            )\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(\n",
    "            fpr,\n",
    "            tpr,\n",
    "            label=\"ROC curve (area = {0:0.2f})\" \"\".format(roc_auc),\n",
    "            linewidth=2.5,\n",
    "        )\n",
    "\n",
    "    elif n_classes > 2:\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            y_score = clf.predict_proba(features)\n",
    "        elif hasattr(clf, \"decision_function\"):\n",
    "            y_score = clf.decision_function(features)\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"Estimator doesn't have a probability or confidence scoring system!\"\n",
    "            )\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        ## Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        ## Compute macro-average ROC curve and ROC area\n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])  # Use numpy.interp instead of scipy.interp\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        ## Plot ROC curves\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(\n",
    "            fpr[\"micro\"],\n",
    "            tpr[\"micro\"],\n",
    "            label=\"micro-average ROC curve (area = {0:0.2f})\" \"\".format(\n",
    "                roc_auc[\"micro\"]\n",
    "            ),\n",
    "            linewidth=3,\n",
    "        )\n",
    "\n",
    "        plt.plot(\n",
    "            fpr[\"macro\"],\n",
    "            tpr[\"macro\"],\n",
    "            label=\"macro-average ROC curve (area = {0:0.2f})\" \"\".format(\n",
    "                roc_auc[\"macro\"]\n",
    "            ),\n",
    "            linewidth=3,\n",
    "        )\n",
    "\n",
    "        for i, label in enumerate(class_labels):\n",
    "            plt.plot(\n",
    "                fpr[i],\n",
    "                tpr[i],\n",
    "                label=\"ROC curve of class {0} (area = {1:0.2f})\" \"\".format(\n",
    "                    label, roc_auc[i]\n",
    "                ),\n",
    "                linewidth=2,\n",
    "                linestyle=\":\",\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\"Number of classes should be atleast 2 or more\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_decision_surface(\n",
    "    clf,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    plot_step=0.02,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    markers=None,\n",
    "    alphas=None,\n",
    "    colors=None,\n",
    "):\n",
    "    if train_features.shape[1] != 2:\n",
    "        raise ValueError(\"X_train should have exactly 2 columnns!\")\n",
    "\n",
    "    x_min, x_max = (\n",
    "        train_features[:, 0].min() - plot_step,\n",
    "        train_features[:, 0].max() + plot_step,\n",
    "    )\n",
    "    y_min, y_max = (\n",
    "        train_features[:, 1].min() - plot_step,\n",
    "        train_features[:, 1].max() + plot_step,\n",
    "    )\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\n",
    "    )\n",
    "\n",
    "    clf_est = clone(clf)\n",
    "    clf_est.fit(train_features, train_labels)\n",
    "    if hasattr(clf_est, \"predict_proba\"):\n",
    "        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    else:\n",
    "        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(train_labels)\n",
    "    n_classes = len(le.classes_)\n",
    "    plot_colors = \"\".join(colors) if colors else [None] * n_classes\n",
    "    label_names = le.classes_\n",
    "    markers = markers if markers else [None] * n_classes\n",
    "    alphas = alphas if alphas else [None] * n_classes\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_enc == i)\n",
    "        plt.scatter(\n",
    "            train_features[idx, 0],\n",
    "            train_features[idx, 1],\n",
    "            c=color,\n",
    "            label=label_names[i],\n",
    "            cmap=cmap,\n",
    "            edgecolors=\"black\",\n",
    "            marker=markers[i],\n",
    "            alpha=alphas[i],\n",
    "        )\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Wine Quality\n",
    "\n",
    "We will predict the wine quality ratings based on other features.\n",
    "\n",
    "### Prepare features\n",
    "\n",
    "#### Feature selection\n",
    "\n",
    "To start with, we\n",
    "will first select our necessary features and separate out the prediction class labels and prepare train and test\n",
    "datasets. We use the prefix **wqp\\_** in our variables to easily identify them as needed, where **wqp** depicts wine\n",
    "quality prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wqp_features = wines.iloc[:, :-3]\n",
    "wqp_class_labels = wines[\"quality_label\"] # Directly use the series\n",
    "wqp_feature_names = list(wqp_features.columns)\n",
    "\n",
    "wqp_train_X, wqp_test_X, wqp_train_y, wqp_test_y = train_test_split(wqp_features, wqp_class_labels, test_size=0.3, random_state=42, stratify=wqp_class_labels) # Add stratification\n",
    "\n",
    "print(Counter(wqp_train_y), Counter(wqp_test_y))\n",
    "print(\"Features:\", wqp_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers show us the wine samples for each class and we can also see the feature names which will\n",
    "be used in our feature set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling\n",
    "\n",
    "We will be using a standard scaler in this\n",
    "scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "wqp_ss = StandardScaler()\n",
    "\n",
    "# Scale the train set\n",
    "wqp_train_SX = wqp_ss.fit_transform(wqp_train_X)\n",
    "\n",
    "# Scale the test set\n",
    "wqp_test_SX = wqp_ss.transform(wqp_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Predict & Evaluate Model using Decision Tree\n",
    "\n",
    "The main advantage of decision tree based models is model\n",
    "interpretability, since it is quite easy to understand and interpret the decision rules which led to a specific\n",
    "model prediction. Besides this, other advantages include the modelâ€™s ability to handle both categorical\n",
    "and numeric data with ease as well as multi-class classification problems. Trees can be even visualized to\n",
    "understand and interpret decision rules better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model using DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wqp_dt = DecisionTreeClassifier()\n",
    "wqp_dt.fit(wqp_train_SX, wqp_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letâ€™s predict the wine types for our test data samples and evaluate the performance.\n",
    "wqp_dt_predictions = wqp_dt.predict(wqp_test_SX)\n",
    "\n",
    "display_model_performance_metrics(\n",
    "    true_labels=wqp_test_y, predicted_labels=wqp_dt_predictions, classes=wqp_label_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an overall F1 Score and model accuracy of approximately 72%.\n",
    "\n",
    "Looking at the class based statistics; we can see the recall for the high quality\n",
    "wine samples is pretty bad since a lot of them have been misclassified into medium and low quality ratings.\n",
    "This is kind of expected since we do not have a lot of training samples for high quality wine if you remember\n",
    "our training sample sizes from earlier. Considering low and high quality rated wine samples, we should at\n",
    "least try to see if we can prevent our model from predicting a low quality wine as high and similarly prevent\n",
    "predicting a high quality wine as low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize Feature Importances from Decision Tree Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wqp_dt_feature_importances = wqp_dt.feature_importances_\n",
    "wqp_dt_feature_names, wqp_dt_feature_scores = zip(\n",
    "    *sorted(zip(wqp_feature_names, wqp_dt_feature_importances), key=lambda x: x[1])\n",
    ")\n",
    "y_position = list(range(len(wqp_dt_feature_names)))\n",
    "plt.barh(y_position, wqp_dt_feature_scores, height=0.6, align=\"center\")\n",
    "plt.yticks(y_position, wqp_dt_feature_names)\n",
    "plt.xlabel(\"Relative Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "t = plt.title(\"Feature Importances for Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly observe that the most important features have changed as compared to\n",
    "our previous model. _Alcohol_ and _volatile acidity_ occupy the top two ranks and _total sulfur dioxide_\n",
    "seems to be one of the most important features for classifying both wine type and quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize the Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Source(\n",
    "    tree.export_graphviz(\n",
    "        wqp_dt,\n",
    "        out_file=None,\n",
    "        class_names=wqp_label_names,\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        special_characters=False,\n",
    "        feature_names=wqp_feature_names,\n",
    "        max_depth=3,\n",
    "    )\n",
    ")\n",
    "png_data = graph.pipe(format=\"png\")\n",
    "with open(\"dtree_structure.png\", \"wb\") as f:\n",
    "    f.write(png_data)\n",
    "\n",
    "Image(png_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our decision tree model has a huge number of nodes and branches hence we visualized our tree for a\n",
    "max depth of three.\n",
    "\n",
    "You can start observing the decision rules from the tree\n",
    "in the figure where the starting split is determined by the rule of alcohol <= -0.128 and with each\n",
    "yes\\no decision branch split, we have further decision nodes as we descend into the tree at each depth level.\n",
    "The class variable is what we are trying to predict, i.e. wine quality being low, medium, or high and value\n",
    "determines the total number of samples at each class present in the current decision node at each instance.\n",
    "\n",
    "The gini parameter is basically the criterion which is used to determine and measure the quality of the split\n",
    "at each decision node. Best splits can be determined by metrics like gini impurity\\gini index or information\n",
    "gain, a metric that helps in minimizing the probability of\n",
    "misclassification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Predict & Evaluate Model using Random Forests\n",
    "\n",
    "In the random\n",
    "forest model, each base learner is a decision tree model trained on a bootstrap sample of the training data.\n",
    "Besides this, when we want to split a decision node in the tree, the split is chosen from a random subset of all\n",
    "the features instead of taking the best split from all the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model using RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "wqp_rf = # Your code goes here\n",
    "wqp_rf.fit(# Your code goes here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and evaluate performance\n",
    "wqp_rf_predictions = # Your code goes here\n",
    "display_model_performance_metrics(\n",
    "    true_labels=wqp_test_y, predicted_labels=wqp_rf_predictions, classes=wqp_label_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model prediction results on the test dataset depict an overall F1 Score and model accuracy of\n",
    "approximately 80%. This is definitely an improvement of 7% from what we obtained\n",
    "with just decision trees proving that ensemble learning is working better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning with Grid Search & Cross Validation\n",
    "\n",
    "Another way to further improve on this result is model tuning. To be more specific, models have\n",
    "hyperparameters that can be tuned.\n",
    "\n",
    "Hyperparameters are also known as meta-parameters\n",
    "and are usually set before we start the model training process. These hyperparameters do not have any\n",
    "dependency on being derived from the underlying data on which the model is trained. Usually these\n",
    "hyperparameters represent some high level concepts or knobs, which can be used to tweak and tune the\n",
    "model during training to improve its performance. Our random forest model has several hyperparameters as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wqp_rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the best hyperparameter values using Grid Search\n",
    "\n",
    "TODO: this fit gives quite a few warnings/errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Grid Search for Random Forest\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 500], # Removed 300\n",
    "    \"max_features\": [\"sqrt\", \"log2\"], # Removed None, often not beneficial\n",
    "    \"min_samples_split\": [2, 5, 10],   # Added regularization parameters\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "wqp_clf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1) # n_jobs for parallel processing\n",
    "wqp_clf.fit(wqp_train_SX, wqp_train_y)\n",
    "print(wqp_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 500 estimators and auto maximum features which represents the square root of the total\n",
    "number of features to be considered during the best split operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View grid search results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = wqp_clf.cv_results_\n",
    "for param, score_mean, score_sd in zip(\n",
    "    results[\"params\"], results[\"mean_test_score\"], results[\"std_test_score\"]\n",
    "):\n",
    "    print(param, round(score_mean, 4), round(score_sd, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the selected hyperparameter combinations and its corresponding mean\n",
    "accuracy and standard deviation values across the grid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Predict & Evaluate Random Forest Model with tuned hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wqp_rf = RandomForestClassifier(# Your code goes here)\n",
    "wqp_rf.fit(wqp_train_SX, wqp_train_y)\n",
    "\n",
    "wqp_rf_predictions = wqp_rf.predict(wqp_test_SX)\n",
    "display_model_performance_metrics(\n",
    "    true_labels=wqp_test_y, predicted_labels=wqp_rf_predictions, classes=wqp_label_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model prediction results on the test dataset improved the overall F1 Score and model accuracy a little bit from the initial random forest model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Predict & Evaluate Model using Extreme Gradient Boosting\n",
    "\n",
    "Another way of modeling ensemble based methods is boosting. A very popular method is XGBoost\n",
    "which stands for Extreme Gradient Boosting. It is a variant of the Gradient Boosting Machines (GBM)\n",
    "model. This model is extremely popular in the Data Science community owing to its superior performance\n",
    "in several Data Science challenges and competitions especially on Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and set dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "wqp_train_y_encoded = # Your code goes here\n",
    "wqp_test_y_encoded = label_encoder.transform(wqp_test_y) # Encode test labels as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wqp_xgb_model = xgb.XGBClassifier(seed=42, eval_metric='mlogloss') # use_label_encoder=False and eval_metric for multiclass\n",
    "wqp_xgb_model.fit(wqp_train_SX, wqp_train_y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and Evaluate Model\n",
    "wqp_xgb_predictions = wqp_xgb_model.predict(wqp_test_SX)\n",
    "\n",
    "# Decode the predictions back to string labels\n",
    "wqp_xgb_predictions_decoded = label_encoder.inverse_transform(wqp_xgb_predictions)\n",
    "\n",
    "# Display model performance metrics\n",
    "display_model_performance_metrics(\n",
    "    true_labels=wqp_test_y,\n",
    "    predicted_labels=wqp_xgb_predictions_decoded,  # Use decoded predictions\n",
    "    classes=wqp_label_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters\n",
    "\n",
    "#### Get the best hyperparameter values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Grid Search for XGBoost\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [5, 7, 10], # Removed 15\n",
    "    \"learning_rate\": [0.1, 0.3], # Added 0.1\n",
    "    \"gamma\": [0, 0.1, 0.2],       # Added regularization\n",
    "    \"subsample\": [0.8, 1.0],     # Added subsampling\n",
    "    \"colsample_bytree\": [0.8, 1.0] # Added column sampling\n",
    "}\n",
    "\n",
    "wqp_clf = GridSearchCV(xgb.XGBClassifier(seed=42, eval_metric='mlogloss'), param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n",
    "wqp_clf.fit(wqp_train_SX, wqp_train_y_encoded)\n",
    "print(wqp_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View grid search results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = wqp_clf.cv_results_\n",
    "for param, score_mean, score_sd in zip(\n",
    "    results[\"params\"], results[\"mean_test_score\"], results[\"std_test_score\"]\n",
    "):\n",
    "    print(param, round(score_mean, 4), round(score_sd, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Predict & Evaluate Extreme Gradient Boosted Model with tuned hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost model with the best hyperparameters\n",
    "wqp_xgb_model = xgb.XGBClassifier(\n",
    "    seed=42, # Your code goes here\n",
    ")\n",
    "wqp_xgb_model.fit(wqp_train_SX, wqp_train_y_encoded)  # Use encoded labels here\n",
    "\n",
    "# Predict on the test set\n",
    "wqp_xgb_predictions = wqp_xgb_model.predict(wqp_test_SX)\n",
    "\n",
    "# Decode the predictions back to string labels\n",
    "wqp_xgb_predictions_decoded = label_encoder.inverse_transform(wqp_xgb_predictions)\n",
    "\n",
    "# Display model performance metrics\n",
    "display_model_performance_metrics(\n",
    "    true_labels=wqp_test_y,\n",
    "    predicted_labels=wqp_xgb_predictions_decoded,  # Use decoded predictions\n",
    "    classes=wqp_label_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model prediction results on the test dataset depict an overall F1 Score and model accuracy of\n",
    "approximately 78%. Though random forests perform slightly better, it definitely\n",
    "performs better than a basic model like a decision tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative analysis of Model Feature importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leveraging skater for feature importances\n",
    "interpreter = Interpretation(wqp_test_SX, feature_names=wqp_feature_names)\n",
    "wqp_im_model = InMemoryModel(\n",
    "    wqp_rf.predict_proba, examples=wqp_train_SX, target_names=wqp_rf.classes_\n",
    ")\n",
    "# retrieving feature importances from the scikit-learn estimator\n",
    "wqp_rf_feature_importances = wqp_rf.feature_importances_\n",
    "wqp_rf_feature_names, wqp_rf_feature_scores = zip(\n",
    "    *sorted(zip(wqp_feature_names, wqp_rf_feature_importances), key=lambda x: x[1])\n",
    ")\n",
    "# plot the feature importance plots\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "t = f.suptitle(\"Feature Importances for Random Forest\", fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.6)\n",
    "y_position = list(range(len(wqp_rf_feature_names)))\n",
    "ax1.barh(\n",
    "    y_position,\n",
    "    wqp_rf_feature_scores,\n",
    "    height=0.6,\n",
    "    align=\"center\",\n",
    "    tick_label=wqp_rf_feature_names,\n",
    ")\n",
    "ax1.set_title(\"Scikit-Learn\")\n",
    "ax1.set_xlabel(\"Relative Importance Score\")\n",
    "ax1.set_ylabel(\"Feature\")\n",
    "plots = interpreter.feature_importance.plot_feature_importance(\n",
    "    wqp_im_model, ascending=True, ax=ax2\n",
    ")\n",
    "ax2.set_title(\"Skater\")\n",
    "ax2.set_xlabel(\"Relative Importance Score\")\n",
    "ax2.set_ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly observe from Figure that the most important features are consistent across the\n",
    "two plots, which is expected considering we are just using different interfaces on the same model. The\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model ROC Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_roc_curve(wqp_rf, wqp_test_SX, wqp_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC is pretty good based on what we see. The dotted lines indicate the per-class ROC curves and\n",
    "the lines in bold are the macro and micro-average ROC curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model Decision Surface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = [\n",
    "    i\n",
    "    for i, feature in enumerate(wqp_feature_names)\n",
    "    if feature in [\"alcohol\", \"volatile acidity\"]\n",
    "]\n",
    "plot_model_decision_surface(\n",
    "    clf=wqp_rf,\n",
    "    train_features=wqp_train_SX[:, feature_indices],\n",
    "    train_labels=wqp_train_y,\n",
    "    plot_step=0.02,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    markers=[\",\", \"d\", \"+\"],\n",
    "    alphas=[1.0, 0.8, 0.5],\n",
    "    colors=[\"r\", \"b\", \"y\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Model Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = LimeTabularExplainer(\n",
    "    wqp_train_SX,\n",
    "    feature_names=wqp_feature_names,\n",
    "    discretize_continuous=True,\n",
    "    class_names=wqp_rf.classes_,\n",
    ")\n",
    "# Model interpretation for our wine quality model's prediction for a low quality wine\n",
    "exp.explain_instance(\n",
    "    wqp_test_SX[10], wqp_rf.predict_proba, top_labels=1\n",
    ").show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows us the features that were primarily responsible for the model to\n",
    "predict the wine quality as low. We can see that the most important feature was alcohol, which makes sense\n",
    "considering what we obtained in our analyses so far from feature importances and model decision surface\n",
    "interpretations. The values for each corresponding feature depicted here are the scaled values obtained after\n",
    "feature scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretation for our wine quality model's prediction for a high quality wine\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the features responsible for the model correctly\n",
    "predicting the wine quality as high and the primary feature was again alcohol by volume (besides other\n",
    "features like density, volatile acidity, and so on). Also you can notice a stark difference in the scaled\n",
    "values of alcohol for the two instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Partial Dependencies\n",
    "\n",
    "In general, partial dependence helps describe the marginal impact\n",
    "or influence of a feature on the model prediction decision by holding the other features constant. Because it\n",
    "is very difficult to visualize high dimensional feature spaces, typically one or two influential and important\n",
    "features are used to visualize partial dependence plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-way partial dependence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one-way partial dependence plots for our model prediction function\n",
    "# based on the most important feature, alcohol.\n",
    "axes_list = interpreter.partial_dependence.plot_partial_dependence(\n",
    "    [\"alcohol\"], wqp_im_model, grid_resolution=100, with_variance=True, figsize=(6, 4)\n",
    ")\n",
    "axs = axes_list[0][3:]\n",
    "[ax.set_ylim(0, 1) for ax in axs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with an increase in the quantity of alcohol content, the\n",
    "confidence\\probability of the model predictor increases in predicting the wine to be either medium or high\n",
    "and similarly it decreases for the probability of wine to be of low quality. This shows there is definitely some\n",
    "relationship between the class predictions with the alcohol content and again the influence of alcohol for\n",
    "predictions of class high is pretty low, which is expected considering training samples for high quality wine\n",
    "are less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two-way partial dependence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numpy import VisibleDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=VisibleDeprecationWarning)\n",
    "\n",
    "# plot wwo-way partial dependence plots for our random forest model predictor\n",
    "# based on alcohol and volatile acidity\n",
    "plots_list = # Your code goes here\n",
    "axs = plots_list[0][3:]\n",
    "[ax.set_zlim(0, 1) for ax in axs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see predicting high\n",
    "quality wine, due to the lack of training data, while some dependency is there for high wine quality class\n",
    "prediction with the increase in alcohol and corresponding decrease in volatile acidity is it quite weak, as\n",
    "we can see in the left most plot. There also seems to be a strong dependency on low wine quality class\n",
    "prediction with the corresponding decrease in alcohol and the increase in volatile acidity levels. This\n",
    "is clearly visible in the rightmost plot. The plot in the middle talks about medium wine quality class\n",
    "predictions. We can observe predictions having a strong dependency with corresponding increase in alcohol\n",
    "and with decrease in volatile acidity levels. This should give you a good foundation on leveraging partial\n",
    "dependence plots to dive deeper into model interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this module, we covered:\n",
    "\n",
    "- Building and evaluating multiple classification models\n",
    "- Applying different machine learning algorithms and comparing their performance\n",
    "- Using model interpretation techniques including:\n",
    "  - Feature importance analysis\n",
    "  - ROC curves and performance metrics\n",
    "  - Decision surface visualization\n",
    "  - Partial dependence plots\n",
    "- Understanding how different features influence model predictions\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
