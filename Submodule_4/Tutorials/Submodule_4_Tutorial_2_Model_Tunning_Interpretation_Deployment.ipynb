{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning, Interpretation and Deployment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers essential aspects of the machine learning workflow after initial model building, focusing on model interpretation techniques, tuning approaches, and deployment considerations. We'll explore how to make machine learning models more interpretable, optimize their performance through tuning, and prepare them for real-world deployment.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the importance of model interpretation in machine learning\n",
    "    - Learn how interpretability benefits analytics teams and stakeholders\n",
    "    - Recognize how interpretability bridges technical and business understanding\n",
    "- Master techniques for model tuning and optimization\n",
    "- Learn best practices for model deployment\n",
    "- Develop skills to explain model decisions to non-technical stakeholders\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Implement model interpretation techniques\n",
    "- Perform model tuning exercises\n",
    "- Practice model deployment steps\n",
    "- Create interpretability visualizations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "To start, we install required packages and import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages\n",
    "\n",
    "If you don't already have skater installed, this step can take a few minutes (five minutes on my machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from skater.core.explanations import Interpretation\n",
    "from skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\n",
    "from skater.model import InMemoryModel\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tunning, Interpretation and Deployment\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn:\n",
    "* How to tune the hyperparameters of Machine Learning algorithms\n",
    "* How to interpret models using open source frameworks\n",
    "* How to persist and deploy the developed models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "\n",
    "Model tuning is one of the\n",
    "most important concepts of Machine Learning and it does require some knowledge of the underlying math\n",
    "and logic of the algorithm in focus. In this tutorial, we will delve deeper into the models that we are targeting, look at the knobs\n",
    "that can be tuned and set to extract the best performance out of any given models. This process of iterative\n",
    "experimentation with dataset, model parameters, and features is the very core of the model tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Evaluate Default Model\n",
    "\n",
    "We will use Wisconsin Breast Cancer Dataset as an example. We first split the breast cancer datast variables X and y into train and test datasets and build an SVM model with default parameters. Then we will evaluate its performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wisconsin Breast Cancer Dataset\n",
    "\n",
    "# load data\n",
    "bc = load_breast_cancer()\n",
    "\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "print(X.shape, bc.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for model evaluation\n",
    "\n",
    "# Get model performance evaluation matrics\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print(\n",
    "        \"Accuracy:\", np.round(metrics.accuracy_score(true_labels, predicted_labels), 4)\n",
    "    )\n",
    "    print(\n",
    "        \"Precision:\",\n",
    "        np.round(\n",
    "            metrics.precision_score(true_labels, predicted_labels, average=\"weighted\"),\n",
    "            4,\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        \"Recall:\",\n",
    "        np.round(\n",
    "            metrics.recall_score(true_labels, predicted_labels, average=\"weighted\"), 4\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        \"F1 Score:\",\n",
    "        np.round(\n",
    "            metrics.f1_score(true_labels, predicted_labels, average=\"weighted\"), 4\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# Show the classification report\n",
    "def display_classification_report(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    # Build a text report showing the main classification metrics\n",
    "    # The reported averages include macro average (averaging the unweighted\n",
    "    # mean per label), weighted average (averaging the support-weighted mean\n",
    "    # per label), and sample average (only for multilabel classification).\n",
    "    # Micro average (averaging the total true positives, false negatives and\n",
    "    # false positives) is only shown for multi-label or multi-class\n",
    "    # with a subset of classes, because it corresponds to accuracy\n",
    "    # otherwise and would be the same for all metrics.\n",
    "    report = metrics.classification_report(\n",
    "        y_true=true_labels, y_pred=predicted_labels, labels=classes\n",
    "    )\n",
    "    print(report)\n",
    "\n",
    "\n",
    "# Show the confusion matrix\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    total_classes = len(classes)\n",
    "    level_labels = [total_classes * [0], list(range(total_classes))]\n",
    "    # Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "    cm = metrics.confusion_matrix(\n",
    "        y_true=true_labels, y_pred=predicted_labels, labels=classes\n",
    "    )\n",
    "    cm_frame = pd.DataFrame(\n",
    "        data=cm,\n",
    "        columns=pd.MultiIndex(levels=[[\"Predicted:\"], classes], codes=level_labels),\n",
    "        index=pd.MultiIndex(levels=[[\"Actual:\"], classes], codes=level_labels),\n",
    "    )\n",
    "    print(cm_frame)\n",
    "\n",
    "\n",
    "# Show the model performace matrics\n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    print(\"Model Performance metrics:\")\n",
    "    print(\"-\" * 30)\n",
    "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n",
    "    print(\"\\nModel Classification report:\")\n",
    "    print(\"-\" * 30)\n",
    "    display_classification_report(\n",
    "        true_labels=true_labels, predicted_labels=predicted_labels, classes=classes\n",
    "    )\n",
    "    print(\"\\nPrediction Confusion Matrix:\")\n",
    "    print(\"-\" * 30)\n",
    "    display_confusion_matrix(\n",
    "        true_labels=true_labels, predicted_labels=predicted_labels, classes=classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# build default SVM model\n",
    "# C-Support Vector Classification\n",
    "def_svc = SVC(random_state=42)\n",
    "def_svc.fit(X_train, y_train)\n",
    "\n",
    "# predict and evaluate performance\n",
    "def_y_pred = def_svc.predict(X_test)\n",
    "print(\"Default Model Stats:\")\n",
    "display_model_performance_metrics(\n",
    "    true_labels=y_test, predicted_labels=def_y_pred, classes=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model with Grid Search\n",
    "\n",
    "Since we have chosen a SVM model, we specify some hyperparameters specific\n",
    "to it, which includes the Regularization parameter C (deals with the margin parameter in SVM), the kernel function (used\n",
    "for transforming data into a higher dimensional feature space) and gamma (determines the influence a\n",
    "single training data point has). There are a lot of other hyperparameters to tune, which you can check out [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for further details.\n",
    "\n",
    "\n",
    "We will build a grid by supplying some pre-set values. The next choice is selecting the score or metric we want\n",
    "to maximize here we have chosen to maximize accuracy of the model. Once that is done, we will be using\n",
    "five-fold cross-validation to build multiple models over this grid and evaluate them to get the best model.\n",
    "\n",
    "(This step may take a few minutes to complete.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameter grid\n",
    "grid_parameters = {\n",
    "    \"kernel\": [\"linear\", \"rbf\"],\n",
    "    \"gamma\": [1e-3, 1e-4],\n",
    "    \"C\": [1, 10, 50, 100],\n",
    "}\n",
    "\n",
    "# perform hyperparameter tuning\n",
    "print(\"# Tuning hyper-parameters for accuracy\\n\")\n",
    "\n",
    "# Exhaustive search over specified parameter values for an estimator.\n",
    "clf = GridSearchCV(SVC(random_state=42), grid_parameters, cv=5, scoring=\"accuracy\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# view accuracy scores for all the models\n",
    "print(\"Grid scores for all the models based on CV:\\n\")\n",
    "means = clf.cv_results_[\"mean_test_score\"]\n",
    "stds = clf.cv_results_[\"std_test_score\"]\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
    "    print(\"%0.5f (+/-%0.05f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "# check out best model performance\n",
    "print(\"\\nBest parameters set found on development set:\", clf.best_params_)\n",
    "print(\"Best model validation accuracy:\", clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the best model parameters were obtained\n",
    "based on cross-validation accuracy and we get a pretty awesome validation accuracy of 96%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Grid Search Tuned Model\n",
    "\n",
    "Let’s take this\n",
    "optimized and tuned model and put it to the test on our test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_best = clf.best_estimator_\n",
    "tuned_y_pred = gs_best.predict(X_test)\n",
    "\n",
    "print(\"\\n\\nTuned Model Stats:\")\n",
    "display_model_performance_metrics(\n",
    "    true_labels=y_test, predicted_labels=tuned_y_pred, classes=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model gives an overall F1 Score and model\n",
    "accuracy of 97% on the test dataset too. This should give you a clear indication of\n",
    "the power of hyperparameter tuning! This scheme of things can be extended for different models and their\n",
    "respective hyperparameters. We can also play around with the evaluation measure we want to optimize.\n",
    "The scikit-learn framework provides us with different values that we can optimize. Some of them are\n",
    "adjusted_rand_score, average_precision, f1, average_recall, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model with Randomized Search\n",
    "\n",
    "Grid search suffers from some major shortcomings, the most important one being\n",
    "the limitation of manually specifying the grid. This brings a human element into a process that could benefit\n",
    "from a purely automatic mechanism.\n",
    "\n",
    "Randomized parameter search is a modification to the traditional grid search. It takes input for\n",
    "grid elements as in normal grid search but it can also take distributions as input. For example consider\n",
    "the parameter gamma whose values we supplied explicitly in the last section instead we can supply a\n",
    "distribution from which to sample gamma. \n",
    "\n",
    "The efficacy of randomized parameter search is based on the\n",
    "proven (empirically and mathematically) result that the hyperparameter optimization functions normally\n",
    "have low dimensionality and the effect of certain parameters are more than others. \n",
    "\n",
    "We control the number\n",
    "of times we want to do the random parameter sampling by specifying the number of iterations we want to\n",
    "run (n_iter). Normally a higher number of iterations mean a more granular parameter search but higher\n",
    "computation time.\n",
    "\n",
    "(This step may take a few minutes to complete.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the gamma and C values with a distribution (exponential distribution)\n",
    "param_grid = {\n",
    "    \"C\": scipy.stats.expon(scale=10),\n",
    "    \"gamma\": scipy.stats.expon(scale=0.1),\n",
    "    \"kernel\": [\"rbf\", \"linear\"],\n",
    "}\n",
    "\n",
    "# Randomized search on hyperparameters.\n",
    "random_search = RandomizedSearchCV(\n",
    "    SVC(random_state=42), param_distributions=param_grid, n_iter=50, cv=5\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Grid scores for all the models based on CV:\\n\")\n",
    "means = random_search.cv_results_[\"mean_test_score\"]\n",
    "stds = random_search.cv_results_[\"std_test_score\"]\n",
    "for mean, std, params in zip(means, stds, random_search.cv_results_[\"params\"]):\n",
    "    print(\"%0.5f (+/-%0.05f) for %r\" % (mean, std * 2, params))\n",
    "print(\"\\nBest parameters set found on development set:\", random_search.best_params_)\n",
    "print(\"Best model validation accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Randomized Search Tuned Model\n",
    "\n",
    "Get the best model, predict and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_best = random_search.best_estimator_\n",
    "rs_y_pred = rs_best.predict(X_test)\n",
    "get_metrics(true_labels=y_test, predicted_labels=rs_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting the values of parameter C and gamma from an exponential distribution\n",
    "and we are controlling the number of iterations of model search by the parameter n_iter. While the overall\n",
    "model performance is similar to grid search, the intent is to be aware of the different strategies in model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "The ability to interpret Machine Learning models in an easy to understand way will benefit not only analytics teams but also key stakeholders in trying to explain how models really work.\n",
    "\n",
    "Some Machine Learning models use interpretable algorithms, for example a decision tree will give you\n",
    "the importance of all the variables as an output. Unfortunately, this\n",
    "can’t be said for a lot of models, especially for the ones who have no notion of variable importance.\n",
    "\n",
    "The lack of understanding of the complex nature\n",
    "of Machine Learned decision policies makes predictive models to be still viewed as black boxes. Model\n",
    "interpretations can help a data scientist and an end user in a variety of ways. \n",
    "* It will help bridge the gap that\n",
    "often exists between the technology teams and the business. For example, it can help identify the reason\n",
    "why a particular prediction is being made and it can be verified using the domain knowledge of the end\n",
    "user by leveraging that easy to understand interpretation. \n",
    "* It can also help the data scientists understand the\n",
    "interactions among features that can lead to better feature engineering and enhanced performance. \n",
    "* It can\n",
    "also help in model comparisons and explaining the results better to the business stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Skater\n",
    "\n",
    "Skater is an open sourced Python library designed to\n",
    "demystify the inner workings of of predictive models. Skater defines the scope of interpretating models\n",
    "1.Globally (on the basis of a complete dataset) and 2. Locally (on the basis of an individual prediction). For\n",
    "global explanations, Skater makes use of model-agnostic variable importance and partial dependence plots\n",
    "to judge the bias of a model and understand its general behavior. To validate a model’s decision policies\n",
    "for a single prediction, on the other hand, the library currently embraces a novel technique called local\n",
    "interpretable model agnostic explanation (LIME, Ribeiro et al., 2016), which uses local surrogate models to\n",
    "assess performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use a logistic regression model to do classification.\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a skater interpretation and in memory model object.\n",
    "\n",
    "interpreter = Interpretation(X_test, feature_names=bc.feature_names)\n",
    "# Probability estimates.\n",
    "model = InMemoryModel(\n",
    "    logistic.predict_proba, examples=X_train, target_names=logistic.classes_\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Importance\n",
    "\n",
    "The skater framework’s feature importance implementation is based on an information\n",
    "theory criterion, where it measures the entropy in the change of predictions, given a perturbation of a\n",
    "specific feature. The idea is that the more a model’s decision making criteria depends on a feature, the more\n",
    "the predictions will change as a function of perturbing the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = interpreter.feature_importance.plot_feature_importance(\n",
    "    model, ascending=True, progressbar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature in our model is worst perimeter, followed by mean area and worst area. Let’s now consider the most important feature,\n",
    "worst perimeter, and think about ways it might influence the model decision making process during\n",
    "predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-way partial dependence plot\n",
    "\n",
    "Partial dependence plots are an excellent tool to leverage to visualize this. In general,\n",
    "partial dependence plots help describe the marginal impact of a specific feature on model prediction\n",
    "by holding the other features in the model constant. The derivative of partial dependence, describes\n",
    "the impact of a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.axes\n",
    "\n",
    "# Computes partial_dependence of a set of variables. Essentially approximates\n",
    "# the partial partial_dependence of the predict_fn with respect to the variables\n",
    "# passed.\n",
    "p = interpreter.partial_dependence.plot_partial_dependence(\n",
    "    [\"worst perimeter\"],\n",
    "    model,\n",
    "    grid_resolution=50,\n",
    "    with_variance=True,\n",
    "    figsize=(6, 4),\n",
    "    progressbar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the worst perimeter feature has a strong influence on the\n",
    "model decision making process. Based on the plot, if the worst perimeter value decreases from 110, the model\n",
    "is more prone to classify the data point as benign (label 1) which indicates no cancer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Predictions\n",
    "\n",
    "Let’s try to interpret some actual predictions now. We will predict two data points, one not\n",
    "having cancer (label 1) and one having cancer (label 0), and try to interpret the prediction making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explains predictions on tabular (i.e. matrix) data.\n",
    "# For numerical features, perturb them by sampling from a Normal(0,1) and\n",
    "# doing the inverse operation of mean-centering and scaling, according to the\n",
    "# means and stds in the training data. For categorical features, perturb by\n",
    "# sampling according to the training distribution, and making a binary\n",
    "# feature that is 1 when the value is the same as the instance being\n",
    "# explained.\n",
    "exp = LimeTabularExplainer(\n",
    "    X_train,\n",
    "    feature_names=bc.feature_names,\n",
    "    discretize_continuous=True,\n",
    "    class_names=[\"0\", \"1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates explanations for a prediction.\n",
    "# First, we generate neighborhood data by randomly perturbing features\n",
    "# from the instance (see __data_inverse). We then learn locally weighted\n",
    "# linear models on this neighborhood data to explain each of the classes\n",
    "# in an interpretable way (see lime_base.py).\n",
    "exp.explain_instance(X_test[0], logistic.predict_proba).show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the features that were primarily responsible for the model\n",
    "to predict the data point as label 1, i.e. having no cancer. We can also see the feature that was the most\n",
    "influential in this decision was worst perimeter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run a similar interpretation on a data point with malignant\n",
    "cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.explain_instance(X_test[1], logistic.predict_proba).show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the features that were primarily responsible\n",
    "for the model to predict the data point as label 0, i.e. having malignant cancer. The feature worst perimeter\n",
    "was again the most influential one and you can notice the stark difference in its value as compared to the\n",
    "previous data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "The final piece of the Machine Learning modeling puzzle is that of\n",
    "deploying the model in production so that we actually start using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist model to disk\n",
    "\n",
    "For persisting our model to disk, we can leverage libraries like pickle or joblib, which is also available\n",
    "with scikit-learn. This allows us to deploy and use the model in the future, without having to retrain it\n",
    "each time we want to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(logistic, \"lr_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from disk\n",
    "\n",
    "So whenever we will load\n",
    "this object in memory again we will get the logistic regression model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = joblib.load(\"lr_model.pkl\")\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with loaded model\n",
    "\n",
    "We can now use this lr object, which is our model loaded from the disk, and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True value: \", y_test[10:11])\n",
    "print(\"Predicted value: \", lr.predict(X_test[10:11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this tutorial, you have gained practical experience in making machine learning models more transparent and interpretable, optimizing their performance through proper tuning, and preparing them for real-world deployment. These skills are essential for bridging the gap between technical implementation and business understanding in machine learning projects.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
