{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning, Interpretation and Deployment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers essential aspects of the machine learning workflow after initial model building, focusing on model interpretation techniques, tuning approaches, and deployment considerations. We'll explore how to make machine learning models more interpretable, optimize their performance through tuning, and prepare them for real-world deployment.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the importance of model interpretation in machine learning\n",
    "  - Learn how interpretability benefits analytics teams and stakeholders\n",
    "  - Recognize how interpretability bridges technical and business understanding\n",
    "- Master techniques for model tuning and optimization\n",
    "- Learn best practices for model deployment\n",
    "- Develop skills to explain model decisions to non-technical stakeholders\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Implement model interpretation techniques\n",
    "- Perform model tuning exercises\n",
    "- Practice model deployment steps\n",
    "- Create interpretability visualizations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "- Please select kernel \"conda_tensorflow2_p310\" from SageMaker notebook instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the 'lime' and 'shap' Python packages using pip.\n",
    "# These packages are commonly used for model interpretability and explainability in machine learning.\n",
    "%pip install lime shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the warnings module to handle warnings\n",
    "import warnings\n",
    "\n",
    "# Import joblib for efficient saving and loading of Python objects\n",
    "import joblib\n",
    "\n",
    "# Import NumPy for numerical operations, especially for handling arrays\n",
    "import numpy as np\n",
    "\n",
    "# Import pandas for data manipulation and analysis, particularly for working with DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Import SciPy for scientific and technical computing, including statistical functions\n",
    "import scipy\n",
    "\n",
    "# Import SHAP library for explaining the output of machine learning models\n",
    "import shap  # Import SHAP for explanation\n",
    "\n",
    "# Import LimeTabularExplainer from the lime library for explaining tabular data predictions\n",
    "from lime.lime_tabular import LimeTabularExplainer  # Import LIME for explanation\n",
    "\n",
    "# Import linear_model module from scikit-learn for linear models like Logistic Regression\n",
    "from sklearn import linear_model, metrics\n",
    "\n",
    "# Import the load_breast_cancer dataset from scikit-learn for demonstration purposes\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Import GridSearchCV and RandomizedSearchCV for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "\n",
    "# Import SVC (Support Vector Classifier) from scikit-learn for classification tasks\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tunning, Interpretation and Deployment\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn:\n",
    "\n",
    "- How to tune the hyperparameters of Machine Learning algorithms\n",
    "- How to interpret models using open source frameworks\n",
    "- How to persist and deploy the developed models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "\n",
    "Model tuning is one of the\n",
    "most important concepts of Machine Learning and it does require some knowledge of the underlying math\n",
    "and logic of the algorithm in focus. In this tutorial, we will delve deeper into the models that we are targeting, look at the knobs\n",
    "that can be tuned and set to extract the best performance out of any given models. This process of iterative\n",
    "experimentation with dataset, model parameters, and features is the very core of the model tuning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Evaluate Default Model\n",
    "\n",
    "We will use Wisconsin Breast Cancer Dataset as an example. We first split the breast cancer datast variables X and y into train and test datasets and build an SVM model with default parameters. Then we will evaluate its performance on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wisconsin Breast Cancer Dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# load data\n",
    "bc = load_breast_cancer()\n",
    "\n",
    "# Extract the feature data (input features) from the dataset\n",
    "X = bc.data\n",
    "# Extract the target data (labels or output) from the dataset\n",
    "y = bc.target\n",
    "\n",
    "# Print the shape of the feature data (number of samples and features) and the names of the features\n",
    "print(X.shape, bc.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for model evaluation\n",
    "\n",
    "# Get model performance evaluation matrics\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    # Print the accuracy score, rounded to 4 decimal places, by comparing true labels to predicted labels.\n",
    "    print(\n",
    "        \"Accuracy:\", np.round(metrics.accuracy_score(true_labels, predicted_labels), 4)\n",
    "    )\n",
    "    # Print the precision score, rounded to 4 decimal places, calculated with weighted averaging for multi-class, by comparing true labels to predicted labels.\n",
    "    print(\n",
    "        \"Precision:\",\n",
    "        np.round(\n",
    "            metrics.precision_score(true_labels, predicted_labels, average=\"weighted\"),\n",
    "            4,\n",
    "        ),\n",
    "    )\n",
    "    # Print the recall score, rounded to 4 decimal places, calculated with weighted averaging for multi-class, by comparing true labels to predicted labels.\n",
    "    print(\n",
    "        \"Recall:\",\n",
    "        np.round(\n",
    "            metrics.recall_score(true_labels, predicted_labels, average=\"weighted\"), 4\n",
    "        ),\n",
    "    )\n",
    "    # Print the F1 score, rounded to 4 decimal places, calculated with weighted averaging for multi-class, by comparing true labels to predicted labels.\n",
    "    print(\n",
    "        \"F1 Score:\",\n",
    "        np.round(\n",
    "            metrics.f1_score(true_labels, predicted_labels, average=\"weighted\"), 4\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# Show the classification report\n",
    "def display_classification_report(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    # Build a text report showing the main classification metrics\n",
    "    # This line calculates and stores the classification report as a string.\n",
    "    # It uses the `classification_report` function from the `metrics` module (likely scikit-learn).\n",
    "    # `y_true=true_labels`:  Specifies the true class labels.\n",
    "    # `y_pred=predicted_labels`: Specifies the predicted class labels from the model.\n",
    "    # `labels=classes`:  Specifies the classes to be included in the report, here defaulting to [1, 0].\n",
    "    report = metrics.classification_report(\n",
    "        y_true=true_labels, y_pred=predicted_labels, labels=classes\n",
    "    )\n",
    "    # Print the classification report to the console.\n",
    "    # This will display the precision, recall, f1-score, and support for each class,\n",
    "    # as well as overall accuracy and macro/weighted averages.\n",
    "    print(report)\n",
    "\n",
    "\n",
    "# Show the confusion matrix\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    # Determine the total number of classes from the classes list.\n",
    "    total_classes = len(classes)\n",
    "    # Define levels for MultiIndex labels in the DataFrame, used for formatting the confusion matrix.\n",
    "    level_labels = [total_classes * [0], list(range(total_classes))]\n",
    "    # Compute the confusion matrix using scikit-learn's metrics.confusion_matrix function.\n",
    "    cm = metrics.confusion_matrix(\n",
    "        y_true=true_labels, y_pred=predicted_labels, labels=classes\n",
    "    )\n",
    "    # Create a Pandas DataFrame to display the confusion matrix in a structured format.\n",
    "    cm_frame = pd.DataFrame(\n",
    "        data=cm,\n",
    "        # Set column names for the DataFrame using MultiIndex to represent 'Predicted' and class labels.\n",
    "        columns=pd.MultiIndex(levels=[[\"Predicted:\"], classes], codes=level_labels),\n",
    "        # Set index names for the DataFrame using MultiIndex to represent 'Actual' and class labels.\n",
    "        index=pd.MultiIndex(levels=[[\"Actual:\"], classes], codes=level_labels),\n",
    "    )\n",
    "    # Print the confusion matrix DataFrame to the console.\n",
    "    print(cm_frame)\n",
    "\n",
    "\n",
    "# Show the model performace matrics\n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    # Prints a header for model performance metrics\n",
    "    print(\"Model Performance metrics:\")\n",
    "    # Prints a separator line for visual clarity\n",
    "    print(\"-\" * 30)\n",
    "    # Calls the function to calculate and print performance metrics\n",
    "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n",
    "    # Prints a newline and header for the classification report\n",
    "    print(\"\\nModel Classification report:\")\n",
    "    # Prints a separator line for visual clarity\n",
    "    print(\"-\" * 30)\n",
    "    # Calls the function to display the classification report\n",
    "    display_classification_report(\n",
    "        true_labels=true_labels, predicted_labels=predicted_labels, classes=classes\n",
    "    )\n",
    "    # Prints a newline and header for the confusion matrix\n",
    "    print(\"\\nPrediction Confusion Matrix:\")\n",
    "    # Prints a separator line for visual clarity\n",
    "    print(\"-\" * 30)\n",
    "    # Calls the function to display the confusion matrix\n",
    "    display_confusion_matrix(\n",
    "        true_labels=true_labels, predicted_labels=predicted_labels, classes=classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for training and testing, splitting the data into training and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Build a default Support Vector Machine (SVM) model.\n",
    "# Initialize a C-Support Vector Classification model with a fixed random state for reproducibility.\n",
    "def_svc = SVC(random_state=42)\n",
    "# Train the default SVM model using the training data (features X_train and labels y_train).\n",
    "def_svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test dataset using the trained default SVM model.\n",
    "def_y_pred = def_svc.predict(X_test)\n",
    "# Print a header to indicate the performance metrics for the default model.\n",
    "print(\"Default Model Stats:\")\n",
    "# Display and print the performance metrics of the default model using the test labels (y_test) and the predicted labels (def_y_pred).\n",
    "# The metrics will be displayed for classes 0 and 1.\n",
    "display_model_performance_metrics(\n",
    "    true_labels=y_test, predicted_labels=def_y_pred, classes=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model with Grid Search\n",
    "\n",
    "Since we have chosen a SVM model, we specify some hyperparameters specific\n",
    "to it, which includes the Regularization parameter C (deals with the margin parameter in SVM), the kernel function (used\n",
    "for transforming data into a higher dimensional feature space) and gamma (determines the influence a\n",
    "single training data point has). There are a lot of other hyperparameters to tune, which you can check out [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for further details.\n",
    "\n",
    "We will build a grid by supplying some pre-set values. The next choice is selecting the score or metric we want\n",
    "to maximize here we have chosen to maximize accuracy of the model. Once that is done, we will be using\n",
    "five-fold cross-validation to build multiple models over this grid and evaluate them to get the best model.\n",
    "\n",
    "(This step should take about three minutes to complete.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid of hyperparameters to search over for the SVM model.\n",
    "grid_parameters = {\n",
    "    \"kernel\": [\n",
    "        \"linear\",\n",
    "        \"rbf\",\n",
    "    ],  # Kernels to try: linear and radial basis function (rbf).\n",
    "    \"gamma\": [1e-3, 1e-4],  # Gamma values to try for rbf kernel (kernel coefficient).\n",
    "    \"C\": [1, 10, 50, 100],  # C values to try (regularization parameter).\n",
    "}\n",
    "\n",
    "# Indicate that hyperparameter tuning is starting, specifically for optimizing accuracy.\n",
    "print(\"# Tuning hyper-parameters for accuracy\\n\")\n",
    "\n",
    "# Initialize GridSearchCV for hyperparameter tuning of an SVC classifier.\n",
    "# SVC(random_state=42): Creates an SVC classifier with a fixed random state for reproducibility.\n",
    "# grid_parameters: The parameter grid defined above to search through.\n",
    "# cv=5: Perform 5-fold cross-validation.\n",
    "# scoring=\"accuracy\": Evaluate models based on accuracy.\n",
    "clf = GridSearchCV(SVC(random_state=42), grid_parameters, cv=5, scoring=\"accuracy\")\n",
    "# Fit the GridSearchCV object to the training data (X_train, y_train) to find the best hyperparameter combination.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Display the accuracy scores obtained for each hyperparameter combination during cross-validation.\n",
    "print(\"Grid scores for all the models based on CV:\\n\")\n",
    "# Extract the mean test scores from the GridSearchCV results. These are the average accuracy scores across the cross-validation folds for each parameter combination.\n",
    "means = clf.cv_results_[\"mean_test_score\"]\n",
    "# Extract the standard deviation of the test scores from the GridSearchCV results. This indicates the variability of the accuracy scores across the cross-validation folds for each parameter combination.\n",
    "stds = clf.cv_results_[\"std_test_score\"]\n",
    "# Iterate through the mean scores, standard deviations, and parameter combinations to print the results for each model.\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
    "    # Print the mean accuracy score and its standard deviation (multiplied by 2 to represent approximately 95% confidence interval) for each parameter setting.\n",
    "    print(\"%0.5f (+/-%0.05f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "# Output the best hyperparameter combination found by GridSearchCV on the development set (which is the training set in this case, due to cross-validation).\n",
    "print(\"\\nBest parameters set found on development set:\", clf.best_params_)\n",
    "# Output the best mean cross-validation score (accuracy) achieved with the best hyperparameter combination. This is an estimate of the model's performance on unseen data.\n",
    "print(\"Best model validation accuracy:\", clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the best model parameters were obtained\n",
    "based on cross-validation accuracy and we get a pretty awesome validation accuracy of 96%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Grid Search Tuned Model\n",
    "\n",
    "Let’s take this\n",
    "optimized and tuned model and put it to the test on our test data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the best estimator found by GridSearchCV (or similar hyperparameter tuning process).\n",
    "gs_best = clf.best_estimator_\n",
    "# Uses the best estimator to make predictions on the test set (X_test).\n",
    "tuned_y_pred = gs_best.predict(X_test)\n",
    "\n",
    "# Prints a header to indicate the performance of the tuned model.\n",
    "print(\"\\n\\nTuned Model Stats:\")\n",
    "# Calls a function to display performance metrics for the tuned model.\n",
    "display_model_performance_metrics(\n",
    "    true_labels=y_test, predicted_labels=tuned_y_pred, classes=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model gives an overall F1 Score and model\n",
    "accuracy of 97% on the test dataset too. This should give you a clear indication of\n",
    "the power of hyperparameter tuning! This scheme of things can be extended for different models and their\n",
    "respective hyperparameters. We can also play around with the evaluation measure we want to optimize.\n",
    "The scikit-learn framework provides us with different values that we can optimize. Some of them are\n",
    "adjusted_rand_score, average_precision, f1, average_recall, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model with Randomized Search\n",
    "\n",
    "Grid search suffers from some major shortcomings, the most important one being\n",
    "the limitation of manually specifying the grid. This brings a human element into a process that could benefit\n",
    "from a purely automatic mechanism.\n",
    "\n",
    "Randomized parameter search is a modification to the traditional grid search. It takes input for\n",
    "grid elements as in normal grid search but it can also take distributions as input. For example consider\n",
    "the parameter gamma whose values we supplied explicitly in the last section instead we can supply a\n",
    "distribution from which to sample gamma.\n",
    "\n",
    "The efficacy of randomized parameter search is based on the\n",
    "proven (empirically and mathematically) result that the hyperparameter optimization functions normally\n",
    "have low dimensionality and the effect of certain parameters are more than others.\n",
    "\n",
    "We control the number\n",
    "of times we want to do the random parameter sampling by specifying the number of iterations we want to\n",
    "run (n_iter). Normally a higher number of iterations mean a more granular parameter search but higher\n",
    "computation time.\n",
    "\n",
    "(This step should take about five minutes to complete.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for RandomizedSearchCV.\n",
    "param_grid = {\n",
    "    # Define 'C' hyperparameter to be sampled from an exponential distribution with scale=10.\n",
    "    \"C\": scipy.stats.expon(scale=10),\n",
    "    # Define 'gamma' hyperparameter to be sampled from an exponential distribution with scale=0.1.\n",
    "    \"gamma\": scipy.stats.expon(scale=0.1),\n",
    "    # Define 'kernel' hyperparameter to choose from 'rbf' or 'linear'.\n",
    "    \"kernel\": [\"rbf\", \"linear\"],\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV for hyperparameter tuning of SVC.\n",
    "random_search = RandomizedSearchCV(\n",
    "    # Use SVC classifier with a fixed random state for reproducibility.\n",
    "    SVC(random_state=42),\n",
    "    # Specify the parameter distributions to sample from.\n",
    "    param_distributions=param_grid,\n",
    "    # Set the number of iterations for random parameter combinations to 50.\n",
    "    n_iter=50,\n",
    "    # Use 5-fold cross-validation.\n",
    "    cv=5,\n",
    ")\n",
    "# Fit the RandomizedSearchCV model to the training data (X_train, y_train).\n",
    "random_search.fit(X_train, y_train)\n",
    "# Print a header for the grid scores from cross-validation.\n",
    "print(\"Grid scores for all the models based on CV:\\n\")\n",
    "# Extract the mean test scores from the RandomizedSearchCV results.\n",
    "means = random_search.cv_results_[\"mean_test_score\"]\n",
    "# Extract the standard deviation of the test scores from the RandomizedSearchCV results.\n",
    "stds = random_search.cv_results_[\"std_test_score\"]\n",
    "# Iterate through the mean scores, standard deviations, and parameter sets from the cross-validation.\n",
    "for mean, std, params in zip(means, stds, random_search.cv_results_[\"params\"]):\n",
    "    # Print the mean score, 95% confidence interval (std * 2), and the corresponding parameter set for each model.\n",
    "    print(\"%0.5f (+/-%0.05f) for %r\" % (mean, std * 2, params))\n",
    "# Print a header for the best parameter set found by RandomizedSearchCV.\n",
    "print(\"\\nBest parameters set found on development set:\", random_search.best_params_)\n",
    "# Print the best model's validation accuracy (mean cross-validation score for the best parameter set).\n",
    "print(\"Best model validation accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Randomized Search Tuned Model\n",
    "\n",
    "Get the best model, predict and evaluate performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best estimator found by RandomizedSearchCV.\n",
    "rs_best = random_search.best_estimator_\n",
    "# Use the best estimator to predict labels for the test dataset (X_test).\n",
    "rs_y_pred = rs_best.predict(X_test)\n",
    "# Evaluate the performance of the best estimator using a function called get_metrics,\n",
    "# comparing the true labels (y_test) with the predicted labels (rs_y_pred).\n",
    "get_metrics(true_labels=y_test, predicted_labels=rs_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting the values of parameter C and gamma from an exponential distribution\n",
    "and we are controlling the number of iterations of model search by the parameter n_iter. While the overall\n",
    "model performance is similar to grid search, the intent is to be aware of the different strategies in model tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "The ability to interpret Machine Learning models in an easy to understand way will benefit not only analytics teams but also key stakeholders in trying to explain how models really work.\n",
    "\n",
    "Some Machine Learning models use interpretable algorithms, for example a decision tree will give you\n",
    "the importance of all the variables as an output. Unfortunately, this\n",
    "can’t be said for a lot of models, especially for the ones who have no notion of variable importance.\n",
    "\n",
    "The lack of understanding of the complex nature\n",
    "of Machine Learned decision policies makes predictive models to be still viewed as black boxes. Model\n",
    "interpretations can help a data scientist and an end user in a variety of ways.\n",
    "\n",
    "- It will help bridge the gap that\n",
    "  often exists between the technology teams and the business. For example, it can help identify the reason\n",
    "  why a particular prediction is being made and it can be verified using the domain knowledge of the end\n",
    "  user by leveraging that easy to understand interpretation.\n",
    "- It can also help the data scientists understand the\n",
    "  interactions among features that can lead to better feature engineering and enhanced performance.\n",
    "- It can\n",
    "  also help in model comparisons and explaining the results better to the business stakeholders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding LIME and SHAP\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) are two powerful tools designed to explain machine learning model predictions in an interpretable manner, making complex models more transparent. LIME works by approximating a machine learning model with a simple, interpretable model (such as a linear regression) around the instance being predicted. It perturbs the input data, generates predictions, and learns a locally faithful model that explains the specific prediction for that instance. This method provides insights into how a model makes decisions for individual instances. SHAP, on the other hand, is grounded in game theory and computes the contribution of each feature to the model’s prediction by distributing the prediction's total impact fairly across all features. SHAP values are additive, and the method provides both local and global interpretability, making it highly effective for understanding the overall impact of features on model predictions. While LIME focuses on local explanations, SHAP excels in providing consistent global feature importance, making both techniques complementary for enhancing model transparency and trustworthiness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the warnings module to handle and filter warnings.\n",
    "import warnings\n",
    "\n",
    "# Filter all warnings to be ignored. This is often used to suppress less important warning messages for cleaner output.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import the LogisticRegression class from the linear_model module of the sklearn library.\n",
    "# This class will be used to create a logistic regression model for classification.\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Initialize a Logistic Regression model object.\n",
    "# This creates an instance of the LogisticRegression classifier with default parameters.\n",
    "logistic = linear_model.LogisticRegression()\n",
    "# Train the Logistic Regression model using the training data.\n",
    "# X_train is the feature matrix of the training data, and y_train is the target variable (labels) for the training data.\n",
    "# The fit() method learns the relationship between features and target variable from the training data.\n",
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explanation (instead of Skater's Interpretation)\n",
    "# Uses kmeans clustering to create a background dataset from the training data (X_train) for KernelExplainer.\n",
    "background = shap.kmeans(X_train, 50)  # Summarize background using 50 clusters\n",
    "# Initializes a KernelExplainer object. KernelExplainer is model-agnostic and approximates SHAP values for any prediction function.\n",
    "# It uses the logistic.predict_proba function (likely from a trained logistic regression model) to explain predictions.\n",
    "# The 'background' dataset is used to estimate expected values in the SHAP calculation, improving efficiency.\n",
    "explainer = shap.KernelExplainer(\n",
    "    logistic.predict_proba, background\n",
    ")  # Use KernelExplainer\n",
    "# Calculates SHAP values for the test dataset (X_test).\n",
    "# SHAP values quantify the contribution of each feature to the prediction for each instance in X_test, based on the KernelExplainer and the model's predict_proba function.\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary plot\n",
    "# This line generates a SHAP summary plot, which is a visualization to understand feature importance and their impact on the model output.\n",
    "# shap_values: The SHAP values calculated for the test dataset (X_test). These values represent the contribution of each feature to each individual prediction.\n",
    "# X_test: The test dataset used for prediction. This is needed to show the actual feature values in the summary plot.\n",
    "# feature_names=bc.feature_names:  Specifies the names of the features. It's assumed 'bc.feature_names' contains a list of feature names corresponding to the columns in X_test, likely from a dataset object 'bc'.\n",
    "shap.summary_plot(shap_values, X_test, feature_names=bc.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways from the Plot\n",
    "\n",
    "- The SHAP interaction values are centered around 0, meaning no strong interaction effects dominate.\n",
    "- Mean radius and mean texture seem to have moderate interactions, with red and blue points somewhat spread out.\n",
    "- If you see larger deviations from 0 in any interaction, it indicates that the combination of those features significantly impacts predictions (either increasing or decreasing the model output more than their individual contributions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Predictions\n",
    "\n",
    "Let’s try to interpret some actual predictions now. We will predict two data points, one not\n",
    "having cancer (label 1) and one having cancer (label 0), and try to interpret the prediction making process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LimeTabularExplainer for explaining tabular data predictions.\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    X_train,  # Training data (numpy array or pandas DataFrame) used to understand the feature ranges and distributions.\n",
    "    feature_names=bc.feature_names,  # List of feature names corresponding to the columns in X_train.\n",
    "    discretize_continuous=True,  # Whether to discretize continuous features. Set to True for tabular data.\n",
    "    class_names=[\"0\", \"1\"],  # List of class names or labels for the target variable.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate explanation for an individual prediction from the test set using LIME.\n",
    "lime_exp = lime_explainer.explain_instance(X_test[0], logistic.predict_proba)\n",
    "# Display the LIME explanation in the notebook for visual interpretation.\n",
    "lime_exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways from the LIME Explanation above\n",
    "\n",
    "1. Prediction Probabilities\n",
    "\n",
    "   - The model predicts 87% (0.87) probability for Malignant (1).\n",
    "   - The probability for Benign (0) is only 13% (0.13).\n",
    "   - This means the model strongly believes the tumor is malignant.\n",
    "\n",
    "2. Feature Contributions\n",
    "\n",
    "   - Features supporting Malignant (1) are in orange (positive contribution to malignancy).\n",
    "   - Features supporting Benign (0) are in blue (negative contribution to malignancy, pushing toward benign).\n",
    "   - The most important malignant indicators are:\n",
    "     - Worst perimeter (96.05)\n",
    "     - Worst area (677.90)\n",
    "     - Mean area (481.90)\n",
    "     - Area error (30.29)\n",
    "   - The most important benign indicators (blue) are:\n",
    "     - Worst radius (14.97)\n",
    "     - Mean perimeter (81.09)\n",
    "     - Mean radius (12.47)\n",
    "\n",
    "3. Feature Value Ranges\n",
    "   - The middle section lists decision splits from the model (e.g., “84.54 < worst perimeter <= 97.75” means a higher perimeter increases malignancy probability).\n",
    "   - Larger values for worst perimeter, worst area, and mean area strongly push the prediction toward malignancy.\n",
    "\n",
    "#### Final Interpretation\n",
    "\n",
    "Even though some features (blue) support the benign classification, the dominant malignant-supporting features outweigh them. Therefore, the model predicts Malignant (1) with high confidence (87%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run a similar interpretation on a data point with malignant\n",
    "cancer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the prediction for a single instance (the second instance) from the test set (X_test[1])\n",
    "lime_exp = lime_explainer.explain_instance(X_test[1], logistic.predict_proba)\n",
    "# Display the Lime explanation in the notebook for visual interpretation.\n",
    "lime_exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways from the LIME Explanation above\n",
    "\n",
    "1. Prediction Probabilities\n",
    "   - The model is 100% certain this is a benign tumor (0.00 probability for malignant (1)).\n",
    "   - The blue bar (0) is fully filled, meaning all supporting features push toward benign.\n",
    "2. Feature Contributions\n",
    "   - Blue bars (supporting benign classification):\n",
    "     - Worst perimeter (165.90)\n",
    "     - Mean area (1130.00)\n",
    "     - Worst area (1866.00)\n",
    "     - Worst texture (26.58)\n",
    "   - Orange bars (supporting malignant classification):\n",
    "     - Mean perimeter (123.60)\n",
    "     - Worst radius (24.86)\n",
    "     - Mean radius (18.94)\n",
    "     - Area error (96.05)\n",
    "   - Since the blue features dominate, the model predicts benign (0) with full confidence.\n",
    "3. Decision Splits & Feature Importance\n",
    "   - The middle section shows decision splits used by the model.\n",
    "     - For example, “mean perimeter > 105.62” slightly supports malignancy (orange).\n",
    "     - However, features like “worst perimeter > 125.30” strongly push toward benign.\n",
    "\n",
    "#### Final Interpretation\n",
    "\n",
    "Even though a few features (like mean perimeter and worst radius) slightly support malignancy, the overall strong benign indicators (worst perimeter, mean area, worst area) outweigh them completely.\n",
    "The model is highly confident this tumor is benign (100% probability).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "The final piece of the Machine Learning modeling puzzle is that of\n",
    "deploying the model in production so that we actually start using it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist model to disk\n",
    "\n",
    "For persisting our model to disk, we can leverage libraries like pickle or joblib, which is also available\n",
    "with scikit-learn. This allows us to deploy and use the model in the future, without having to retrain it\n",
    "each time we want to use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the trained logistic regression model to a file named 'lr_model.pkl' using joblib for later use.\n",
    "joblib.dump(logistic, \"lr_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from disk\n",
    "\n",
    "So whenever we will load\n",
    "this object in memory again we will get the logistic regression model object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Logistic Regression model from a pickle file named \"lr_model.pkl\".\n",
    "lr = joblib.load(\"lr_model.pkl\")\n",
    "# Display the loaded Logistic Regression model object. This will show the model's parameters and structure.\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with loaded model\n",
    "\n",
    "We can now use this lr object, which is our model loaded from the disk, and make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the true value from the y_test dataset for the index range 10 to 11 (exclusive of 11, so effectively index 10).\n",
    "print(\"True value: \", y_test[10:11])\n",
    "# Print the predicted value for the corresponding input data from X_test dataset at index range 10 to 11, using the trained linear regression model 'lr'.\n",
    "print(\"Predicted value: \", lr.predict(X_test[10:11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this tutorial, you have gained practical experience in making machine learning models more transparent and interpretable, optimizing their performance through proper tuning, and preparing them for real-world deployment. These skills are essential for bridging the gap between technical implementation and business understanding in machine learning projects.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
