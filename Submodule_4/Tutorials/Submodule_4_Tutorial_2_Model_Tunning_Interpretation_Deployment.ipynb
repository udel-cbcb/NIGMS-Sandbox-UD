{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning, Interpretation and Deployment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial delves into the critical stages of the machine learning workflow that follow the initial model development. It focuses on three key areas: **model interpretation**, **performance tuning**, and **deployment strategies**. These steps are essential for transforming a raw machine learning model into a robust, interpretable, and production-ready solution.\n",
    "\n",
    "We will explore:\n",
    "\n",
    "- **Model Interpretation**: Techniques to make machine learning models more transparent and understandable, enabling stakeholders to trust and effectively utilize their predictions.\n",
    "- **Model Tuning**: Methods to optimize model performance by fine-tuning hyperparameters, addressing overfitting or underfitting, and improving generalization to unseen data.\n",
    "\n",
    "- **Deployment Considerations**: Best practices for preparing and deploying machine learning models into real-world environments, ensuring scalability, reliability, and maintainability.\n",
    "\n",
    "By the end of this tutorial, you will gain practical insights into how to refine your models, interpret their decisions, and successfully deploy them to solve real-world problems. Whether you're working on a small-scale project or a large-scale enterprise application, these skills are crucial for delivering impactful machine learning solutions.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the importance of model interpretation in machine learning\n",
    "  - Learn how interpretability benefits analytics teams and stakeholders\n",
    "  - Recognize how interpretability bridges technical and business understanding\n",
    "- Master techniques for model tuning and optimization\n",
    "- Learn best practices for model deployment\n",
    "- Develop skills to explain model decisions to non-technical stakeholders\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Implement model interpretation techniques\n",
    "- Perform model tuning exercises\n",
    "- Practice model deployment steps\n",
    "- Create interpretability visualizations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "- Please select kernel \"conda_tensorflow2_p310\" from SageMaker notebook instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the 'lime' and 'shap' Python packages using pip.\n",
    "# These packages are commonly used for model interpretability and explainability in machine learning.\n",
    "#%pip install setuptools==57.5.0  # Downgrade setuptools\n",
    "%pip install git+https://github.com/marcotcr/lime.git\n",
    "%pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the warnings module to handle warnings\n",
    "import warnings\n",
    "\n",
    "# Import joblib for efficient saving and loading of Python objects\n",
    "import joblib\n",
    "\n",
    "# Import NumPy for numerical operations, especially for handling arrays\n",
    "import numpy as np\n",
    "\n",
    "# Import pandas for data manipulation and analysis, particularly for working with DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Import SciPy for scientific and technical computing, including statistical functions\n",
    "import scipy\n",
    "\n",
    "# Import SHAP library for explaining the output of machine learning models\n",
    "import shap  # Import SHAP for explanation\n",
    "\n",
    "# Import LimeTabularExplainer from the lime library for explaining tabular data predictions\n",
    "from lime.lime_tabular import LimeTabularExplainer  # Import LIME for explanation\n",
    "\n",
    "# Import linear_model module from scikit-learn for linear models like Logistic Regression\n",
    "from sklearn import linear_model, metrics\n",
    "\n",
    "# Import the load_breast_cancer dataset from scikit-learn for demonstration purposes\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Import GridSearchCV and RandomizedSearchCV for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "\n",
    "# Import SVC (Support Vector Classifier) from scikit-learn for classification tasks\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tunning, Interpretation and Deployment\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn:\n",
    "- **Hyperparameter Tuning**\n",
    "  - Techniques for optimizing ML algorithm parameters\n",
    "  - Best practices for efficient parameter search\n",
    "\n",
    "- **Model Interpretation**\n",
    "  - Using open-source frameworks (e.g., SHAP, LIME)\n",
    "  - Understanding model decisions and feature importance\n",
    "\n",
    "- **Model Operations**\n",
    "  - Persistence strategies for trained models\n",
    "  - Deployment approaches for production environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "\n",
    "Model tuning is a cornerstone of machine learning, requiring both a strong grasp of mathematical foundations and an intuitive understanding of algorithmic behavior. In this tutorial, we’ll dissect target models to pinpoint tunable parameters (the \"knobs\" of the system), then methodically adjust them to maximize performance. This iterative process—spanning dataset experimentation, hyperparameter optimization, and feature engineering—forms the essence of model refinement. By rigorously testing and calibrating these components, we unlock a model’s full potential, ensuring robust results tailored to specific use cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Evaluate Default Model\n",
    "\n",
    "In this section, we will use the Wisconsin Breast Cancer Dataset as an example to demonstrate the process of building and evaluating a machine learning model. Here's how we'll proceed:\n",
    "\n",
    "- **Dataset Preparation**:\n",
    "    - We will start by splitting the dataset into features (X) and target labels (y).\n",
    "    - The data will then be divided into training and testing sets to ensure proper evaluation.\n",
    "- **Model Building**:\n",
    "    - Using the training data, we will construct a Support Vector Machine (SVM) model with its default parameters.\n",
    "\n",
    "- **Model Evaluation**:\n",
    "    - The trained model will be applied to the test dataset to assess its performance.\n",
    "    - We will analyze key metrics to understand how well the default model performs on unseen data.\n",
    "\n",
    "This step-by-step approach will provide a baseline understanding of the model's capabilities before we move on to tuning and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wisconsin Breast Cancer Dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# load data\n",
    "bc = load_breast_cancer()\n",
    "\n",
    "# Extract the feature data (input features) from the dataset\n",
    "X = bc.data\n",
    "\n",
    "# Extract the target data (labels or output) from the dataset\n",
    "y = bc.target\n",
    "\n",
    "# Print the shape of the feature data (number of samples and features) and the names of the features\n",
    "print(X.shape, bc.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for model evaluation\n",
    "\n",
    "# Get model performance evaluation matrics\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    # Print the accuracy score, rounded to 4 decimal places, by comparing true labels to predicted labels.\n",
    "    print(\n",
    "        \"Accuracy:\", np.round(metrics.accuracy_score(true_labels, predicted_labels), 4)\n",
    "    )\n",
    "    # Print the precision score, rounded to 4 decimal places, calculated with weighted averaging for multi-class, by comparing true labels to predicted labels.\n",
    "    print(\n",
    "        \"Precision:\",\n",
    "        np.round(\n",
    "            metrics.precision_score(true_labels, predicted_labels, average=\"weighted\"),\n",
    "            4,\n",
    "        ),\n",
    "    )\n",
    "    # Print the recall score, rounded to 4 decimal places, calculated with weighted averaging for multi-class, by comparing true labels to predicted labels.\n",
    "    print(\n",
    "        \"Recall:\",\n",
    "        np.round(\n",
    "            metrics.recall_score(true_labels, predicted_labels, average=\"weighted\"), 4\n",
    "        ),\n",
    "    )\n",
    "    # Print the F1 score, rounded to 4 decimal places, calculated with weighted averaging for multi-class, by comparing true labels to predicted labels.\n",
    "    print(\n",
    "        \"F1 Score:\",\n",
    "        np.round(\n",
    "            metrics.f1_score(true_labels, predicted_labels, average=\"weighted\"), 4\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# Show the classification report\n",
    "def display_classification_report(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    # Build a text report showing the main classification metrics\n",
    "    # This line calculates and stores the classification report as a string.\n",
    "    # It uses the `classification_report` function from the `metrics` module (likely scikit-learn).\n",
    "    # `y_true=true_labels`:  Specifies the true class labels.\n",
    "    # `y_pred=predicted_labels`: Specifies the predicted class labels from the model.\n",
    "    # `labels=classes`:  Specifies the classes to be included in the report, here defaulting to [1, 0].\n",
    "    report = metrics.classification_report(\n",
    "        y_true=true_labels, y_pred=predicted_labels, labels=classes\n",
    "    )\n",
    "    \n",
    "    # Print the classification report to the console.\n",
    "    # This will display the precision, recall, f1-score, and support for each class,\n",
    "    # as well as overall accuracy and macro/weighted averages.\n",
    "    print(report)\n",
    "\n",
    "\n",
    "# Show the confusion matrix\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    # Determine the total number of classes from the classes list.\n",
    "    total_classes = len(classes)\n",
    "    \n",
    "    # Define levels for MultiIndex labels in the DataFrame, used for formatting the confusion matrix.\n",
    "    level_labels = [total_classes * [0], list(range(total_classes))]\n",
    "    \n",
    "    # Compute the confusion matrix using scikit-learn's metrics.confusion_matrix function.\n",
    "    cm = metrics.confusion_matrix(\n",
    "        y_true=true_labels, y_pred=predicted_labels, labels=classes\n",
    "    )\n",
    "    # Create a Pandas DataFrame to display the confusion matrix in a structured format.\n",
    "    cm_frame = pd.DataFrame(\n",
    "        data=cm,\n",
    "        # Set column names for the DataFrame using MultiIndex to represent 'Predicted' and class labels.\n",
    "        columns=pd.MultiIndex(levels=[[\"Predicted:\"], classes], codes=level_labels),\n",
    "        \n",
    "        # Set index names for the DataFrame using MultiIndex to represent 'Actual' and class labels.\n",
    "        index=pd.MultiIndex(levels=[[\"Actual:\"], classes], codes=level_labels),\n",
    "    )\n",
    "    # Print the confusion matrix DataFrame to the console.\n",
    "    print(cm_frame)\n",
    "\n",
    "\n",
    "# Show the model performace matrics\n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    # Prints a header for model performance metrics\n",
    "    print(\"Model Performance metrics:\")\n",
    "    \n",
    "    # Prints a separator line for visual clarity\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Calls the function to calculate and print performance metrics\n",
    "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n",
    "    \n",
    "    # Prints a newline and header for the classification report\n",
    "    print(\"\\nModel Classification report:\")\n",
    "    \n",
    "    # Prints a separator line for visual clarity\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Calls the function to display the classification report\n",
    "    display_classification_report(\n",
    "        true_labels=true_labels, predicted_labels=predicted_labels, classes=classes\n",
    "    )\n",
    "    # Prints a newline and header for the confusion matrix\n",
    "    print(\"\\nPrediction Confusion Matrix:\")\n",
    "    \n",
    "    # Prints a separator line for visual clarity\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Calls the function to display the confusion matrix\n",
    "    display_confusion_matrix(\n",
    "        true_labels=true_labels, predicted_labels=predicted_labels, classes=classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for training and testing, splitting the data into training (70%) and test (30%) sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Build a default Support Vector Machine (SVM) model.\n",
    "# Initialize a C-Support Vector Classification model with a fixed random state for reproducibility.\n",
    "def_svc = SVC(random_state=42)\n",
    "\n",
    "# Train the default SVM model using the training data (features X_train and labels y_train).\n",
    "def_svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test dataset using the trained default SVM model.\n",
    "def_y_pred = def_svc.predict(X_test)\n",
    "\n",
    "# Print a header to indicate the performance metrics for the default model.\n",
    "print(\"Default Model Stats:\")\n",
    "\n",
    "# Display and print the performance metrics of the default model using the test labels (y_test) and the predicted labels (def_y_pred).\n",
    "# The metrics will be displayed for classes 0 and 1.\n",
    "display_model_performance_metrics(\n",
    "    true_labels=y_test, predicted_labels=def_y_pred, classes=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model with Grid Search\n",
    "\n",
    "Since we are working with an SVM (Support Vector Machine) model, we will focus on tuning several key hyperparameters specific to this algorithm. These include:\n",
    "\n",
    "- **Regularization Parameter (C)**: Controls the trade-off between achieving a wide margin and correctly classifying training points. A smaller C creates a larger margin but may allow some misclassifications, while a larger C aims for fewer misclassifications at the cost of a narrower margin.\n",
    "- **Kernel Function**: Determines how data is transformed into a higher-dimensional feature space. Common choices include linear, polynomial, and radial basis function (RBF) kernels.\n",
    "- **Gamma**: Defines the influence of a single training example. A low gamma means a training point has influence over a larger area, while a high gamma restricts its influence to a smaller region.\n",
    "\n",
    "There are additional hyperparameters that can be tuned, and you can explore them in detail [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "To systematically search for the best combination of hyperparameters, we will create a grid of predefined values for C, kernel, and gamma. Next, we need to select a scoring metric to evaluate model performance—in this case, we aim to maximize accuracy.\n",
    "\n",
    "Once the grid and scoring metric are defined, we will use five-fold cross-validation to train and evaluate multiple models across the grid. This process involves splitting the training data into five subsets, training the model on four subsets, and validating it on the fifth. This is repeated five times, ensuring each subset is used for validation once. The result is a robust evaluation of each hyperparameter combination, allowing us to identify the best-performing model.\n",
    "\n",
    "(This step should take about three minutes to complete.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid of hyperparameters to search over for the SVM model.\n",
    "grid_parameters = {\n",
    "    \"kernel\": [\n",
    "        \"linear\",\n",
    "        \"rbf\",\n",
    "    ],  # Kernels to try: linear and radial basis function (rbf).\n",
    "    \"gamma\": [1e-3, 1e-4],  # Gamma values to try for rbf kernel (kernel coefficient).\n",
    "    \"C\": [1, 10, 50, 100],  # C values to try (regularization parameter).\n",
    "}\n",
    "\n",
    "# Indicate that hyperparameter tuning is starting, specifically for optimizing accuracy.\n",
    "print(\"# Tuning hyper-parameters for accuracy\\n\")\n",
    "\n",
    "# Initialize GridSearchCV for hyperparameter tuning of an SVC classifier.\n",
    "# SVC(random_state=42): Creates an SVC classifier with a fixed random state for reproducibility.\n",
    "# grid_parameters: The parameter grid defined above to search through.\n",
    "# cv=5: Perform 5-fold cross-validation.\n",
    "# scoring=\"accuracy\": Evaluate models based on accuracy.\n",
    "clf = GridSearchCV(SVC(random_state=42), grid_parameters, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "# Fit the GridSearchCV object to the training data (X_train, y_train) to find the best hyperparameter combination.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Display the accuracy scores obtained for each hyperparameter combination during cross-validation.\n",
    "print(\"Grid scores for all the models based on CV:\\n\")\n",
    "\n",
    "# Extract the mean test scores from the GridSearchCV results. These are the average accuracy scores across the cross-validation folds for each parameter combination.\n",
    "means = clf.cv_results_[\"mean_test_score\"]\n",
    "\n",
    "# Extract the standard deviation of the test scores from the GridSearchCV results. This indicates the variability of the accuracy scores across the cross-validation folds for each parameter combination.\n",
    "stds = clf.cv_results_[\"std_test_score\"]\n",
    "\n",
    "# Iterate through the mean scores, standard deviations, and parameter combinations to print the results for each model.\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
    "    # Print the mean accuracy score and its standard deviation (multiplied by 2 to represent approximately 95% confidence interval) for each parameter setting.\n",
    "    print(\"%0.5f (+/-%0.05f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "# Output the best hyperparameter combination found by GridSearchCV on the development set (which is the training set in this case, due to cross-validation).\n",
    "print(\"\\nBest parameters set found on development set:\", clf.best_params_)\n",
    "\n",
    "# Output the best mean cross-validation score (accuracy) achieved with the best hyperparameter combination. This is an estimate of the model's performance on unseen data.\n",
    "print(\"Best model validation accuracy:\", clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the best model parameters were obtained\n",
    "based on cross-validation accuracy and we get a pretty awesome validation accuracy of 96%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Grid Search Tuned Model\n",
    "\n",
    "Now that we’ve optimized and fine-tuned our model, it’s time to put it to the ultimate test—evaluating its performance on the test dataset! This step will help us understand how well our model generalizes to unseen data and whether the tuning process has truly enhanced its predictive power. Let’s dive in and see how our refined model performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the best estimator found by GridSearchCV (or similar hyperparameter tuning process).\n",
    "gs_best = clf.best_estimator_\n",
    "\n",
    "# Uses the best estimator to make predictions on the test set (X_test).\n",
    "tuned_y_pred = gs_best.predict(X_test)\n",
    "\n",
    "# Prints a header to indicate the performance of the tuned model.\n",
    "print(\"\\n\\nTuned Model Stats:\")\n",
    "\n",
    "# Calls a function to display performance metrics for the tuned model.\n",
    "display_model_performance_metrics(\n",
    "    true_labels=y_test, predicted_labels=tuned_y_pred, classes=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our optimized model achieves an impressive **F1 Score** and **model accuracy** of 97% on the test dataset, demonstrating the significant impact of hyperparameter tuning. This result highlights how fine-tuning can dramatically enhance a model's performance and its ability to generalize to new data.\n",
    "\n",
    "This approach isn’t limited to just one type of model—it can be extended to various machine learning algorithms and their respective hyperparameters. Additionally, the evaluation metric you choose to optimize can be tailored to your specific use case. The scikit-learn framework offers a wide range of scoring metrics to suit different needs, such as:\n",
    "\n",
    "- **adjusted_rand_score**: Useful for clustering tasks.\n",
    "- **average_precision**: Ideal for imbalanced classification problems.\n",
    "- **average_recall**: Focuses on the model’s ability to capture positive instances.\n",
    "\n",
    "By experimenting with different models, hyperparameters, and evaluation metrics, you can unlock the full potential of your machine learning workflows. The flexibility provided by scikit-learn makes it easier to adapt these techniques to diverse problems and datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model with Randomized Search\n",
    "\n",
    "While **grid search** is a common approach for hyperparameter tuning, it comes with significant limitations. One of the most notable drawbacks is the need to manually specify the grid of hyperparameter values, which introduces a human element into what could otherwise be a fully automated process.\n",
    "\n",
    "**Randomized Search** addresses this limitation by offering a more flexible and efficient alternative. Unlike grid search, which requires predefined values for each hyperparameter, randomized search can accept distributions as input. For example, instead of explicitly listing values for the gamma parameter (as we did in the previous section), we can provide a statistical distribution from which values are randomly sampled.\n",
    "\n",
    "The effectiveness of randomized search is supported by both empirical evidence and mathematical theory. It leverages the fact that hyperparameter optimization problems often have low effective dimensionality, meaning that only a few hyperparameters significantly impact model performance. By focusing on these influential parameters, randomized search can achieve comparable or even better results than grid search, often with far fewer iterations.\n",
    "\n",
    "To control the extent of the search, we specify the number of iterations (n_iter). A higher number of iterations allows for a more thorough exploration of the hyperparameter space, potentially leading to better results. However, this comes at the cost of increased computational time. Balancing the number of iterations is key to achieving an efficient and effective tuning process.\n",
    "\n",
    "(This step should take about five minutes to complete.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for RandomizedSearchCV.\n",
    "param_grid = {\n",
    "    # Define 'C' hyperparameter to be sampled from an exponential distribution with scale=10.\n",
    "    \"C\": scipy.stats.expon(scale=10),\n",
    "    \n",
    "    # Define 'gamma' hyperparameter to be sampled from an exponential distribution with scale=0.1.\n",
    "    \"gamma\": scipy.stats.expon(scale=0.1),\n",
    "    \n",
    "    # Define 'kernel' hyperparameter to choose from 'rbf' or 'linear'.\n",
    "    \"kernel\": [\"rbf\", \"linear\"],\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV for hyperparameter tuning of SVC.\n",
    "random_search = RandomizedSearchCV(\n",
    "    # Use SVC classifier with a fixed random state for reproducibility.\n",
    "    SVC(random_state=42),\n",
    "    \n",
    "    # Specify the parameter distributions to sample from.\n",
    "    param_distributions=param_grid,\n",
    "    \n",
    "    # Set the number of iterations for random parameter combinations to 50.\n",
    "    n_iter=50,\n",
    "    \n",
    "    # Use 5-fold cross-validation.\n",
    "    cv=5,\n",
    ")\n",
    "# Fit the RandomizedSearchCV model to the training data (X_train, y_train).\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print a header for the grid scores from cross-validation.\n",
    "print(\"Grid scores for all the models based on CV:\\n\")\n",
    "\n",
    "# Extract the mean test scores from the RandomizedSearchCV results.\n",
    "means = random_search.cv_results_[\"mean_test_score\"]\n",
    "\n",
    "# Extract the standard deviation of the test scores from the RandomizedSearchCV results.\n",
    "stds = random_search.cv_results_[\"std_test_score\"]\n",
    "\n",
    "# Iterate through the mean scores, standard deviations, and parameter sets from the cross-validation.\n",
    "for mean, std, params in zip(means, stds, random_search.cv_results_[\"params\"]):\n",
    "    # Print the mean score, 95% confidence interval (std * 2), and the corresponding parameter set for each model.\n",
    "    print(\"%0.5f (+/-%0.05f) for %r\" % (mean, std * 2, params))\n",
    "    \n",
    "# Print a header for the best parameter set found by RandomizedSearchCV.\n",
    "print(\"\\nBest parameters set found on development set:\", random_search.best_params_)\n",
    "\n",
    "# Print the best model's validation accuracy (mean cross-validation score for the best parameter set).\n",
    "print(\"Best model validation accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Randomized Search Tuned Model\n",
    "\n",
    "After completing the randomized search process, the next step is to evaluate the performance of the best model identified during the tuning phase. Here's how we’ll proceed:\n",
    "\n",
    "- **Retrieve the Best Model**:\n",
    "    - Randomized search returns the best combination of hyperparameters based on the specified scoring metric. We will extract this optimal model for further evaluation.\n",
    "\n",
    "- **Make Predictions**:\n",
    "    - Using the best model, we will generate predictions on the test dataset. This allows us to assess how well the model generalizes to unseen data.\n",
    "\n",
    "- **Evaluate Performance**:\n",
    "    - We will evaluate the model's performance using relevant metrics, such as accuracy, precision, recall, F1 score, or any other metric appropriate for the task. These metrics provide a comprehensive understanding of the model's effectiveness.\n",
    "\n",
    "- **Compare Results**:\n",
    "    - To gauge the impact of randomized search, we will compare the performance of the tuned model with the baseline model (e.g., the model trained with default hyperparameters). This comparison highlights the improvements achieved through hyperparameter optimization.\n",
    "\n",
    "By systematically evaluating the tuned model, we can ensure that the randomized search process has successfully enhanced the model's performance and that it is ready for deployment or further refinement. This step is crucial for validating the effectiveness of the tuning process and building confidence in the model's predictive capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best estimator found by RandomizedSearchCV.\n",
    "rs_best = random_search.best_estimator_\n",
    "\n",
    "# Use the best estimator to predict labels for the test dataset (X_test).\n",
    "rs_y_pred = rs_best.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the best estimator using a function called get_metrics,\n",
    "# comparing the true labels (y_test) with the predicted labels (rs_y_pred).\n",
    "get_metrics(true_labels=y_test, predicted_labels=rs_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, we are sampling the values for the hyperparameters C (regularization parameter) and gamma from an exponential distribution. This allows for a more dynamic and flexible exploration of the hyperparameter space compared to manually specifying fixed values. Additionally, we control the extent of the search by setting the n_iter parameter, which determines the number of iterations or random combinations of hyperparameters to evaluate.\n",
    "\n",
    "While the overall performance of the model tuned using randomized search is often comparable to that achieved with grid search, the primary goal here is to highlight the different strategies available for model tuning. Randomized search offers a more efficient and scalable alternative, especially when dealing with a large hyperparameter space or limited computational resources. By understanding and leveraging these diverse tuning strategies, you can choose the most suitable approach for your specific use case and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "The ability to interpret machine learning models in a clear and understandable way is invaluable—not only for analytics teams but also for key stakeholders who need to understand how these models make decisions. While some machine learning algorithms, like decision trees, inherently provide interpretable outputs (such as variable importance), many others, especially complex models, lack this transparency. As a result, predictive models are often perceived as black boxes, making it challenging to trust or explain their decision-making processes.\n",
    "\n",
    "Model interpretation addresses this challenge by shedding light on how models operate, offering several key benefits:\n",
    "\n",
    "- **Bridging the Gap Between Technology and Business**:\n",
    "    - Interpretable models help align technical teams with business stakeholders by providing clear explanations for predictions. For instance, if a model predicts a specific outcome, interpretation techniques can reveal the underlying reasons, which can then be validated using domain expertise. This fosters trust and collaboration between teams.\n",
    "\n",
    "- **Enhancing Feature Engineering**:\n",
    "    - By understanding how features interact and contribute to predictions, data scientists can refine their feature engineering processes. This deeper insight often leads to improved model performance and more robust solutions.\n",
    "\n",
    "- **Facilitating Model Comparisons**:\n",
    "    - Interpretation tools enable data scientists to compare different models effectively, identifying which one aligns best with business objectives. This makes it easier to justify model choices to stakeholders.\n",
    "\n",
    "- **Improving Stakeholder Communication**:\n",
    "    - Clear interpretations make it easier to explain model results to non-technical audiences, ensuring that business decisions are informed and actionable.\n",
    "\n",
    "Model interpretation is a critical step in the machine learning workflow. It not only demystifies complex models but also empowers both data scientists and business stakeholders to make better, more informed decisions. By leveraging interpretation techniques, we can transform black-box models into transparent, trustworthy tools that drive meaningful outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding LIME and SHAP\n",
    "\n",
    "**LIME (Local Interpretable Model-agnostic Explanations)** and **SHAP (SHapley Additive exPlanations)** are two advanced techniques designed to make machine learning models more interpretable and transparent. Both tools aim to explain how models arrive at their predictions, but they do so in different ways, each with its own strengths.\n",
    "\n",
    "**LIME** focuses on providing local explanations for individual predictions. It works by approximating the behavior of a complex machine learning model using a simpler, interpretable model (such as linear regression) in the vicinity of a specific data point. Here’s how it works:\n",
    "- **Perturbation**: LIME generates slight variations of the input data by perturbing the features.\n",
    "- **Prediction**: It uses the original model to make predictions for these perturbed instances.\n",
    "- **Interpretable Model**: A simple model is then trained on the perturbed data and predictions, focusing on the local region around the instance of interest.\n",
    "- **Explanation**: The simple model provides insights into how the original model made its prediction for that specific instance.\n",
    "\n",
    "LIME is particularly useful for understanding individual predictions and is model-agnostic, meaning it can be applied to any machine learning model. However, its explanations are limited to local regions and may not capture the global behavior of the model.\n",
    "\n",
    "\n",
    "**SHAP**, on the other hand, is rooted in game theory and provides a more comprehensive framework for model interpretation. It calculates the contribution of each feature to the model’s prediction by fairly distributing the prediction’s impact across all features. Key aspects of SHAP include:\n",
    "\n",
    "- **Shapley Values**: SHAP uses Shapley values, a concept from cooperative game theory, to quantify the contribution of each feature.\n",
    "- **Additivity**: SHAP values are additive, meaning the sum of all feature contributions equals the difference between the model’s prediction and the baseline (average) prediction.\n",
    "- **Global and Local Interpretability**: SHAP provides both local explanations (for individual predictions) and global explanations (for overall feature importance), making it a versatile tool.\n",
    "\n",
    "SHAP is particularly powerful because it ensures consistency and fairness in feature attribution, making it highly effective for understanding the overall impact of features on model predictions.\n",
    "\n",
    "**Complementary Strengths**:\n",
    "- LIME is ideal for explaining individual predictions in a simple, intuitive way.\n",
    "- SHAP excels in providing consistent and fair feature importance, both locally and globally.\n",
    "\n",
    "Together, LIME and SHAP offer a robust toolkit for enhancing the transparency and trustworthiness of machine learning models, enabling data scientists and stakeholders to better understand and trust complex decision-making processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the warnings module to handle and filter warnings.\n",
    "import warnings\n",
    "\n",
    "# Filter all warnings to be ignored. This is often used to suppress less important warning messages for cleaner output.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import the LogisticRegression class from the linear_model module of the sklearn library.\n",
    "# This class will be used to create a logistic regression model for classification.\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Initialize a Logistic Regression model object.\n",
    "# This creates an instance of the LogisticRegression classifier with default parameters.\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Train the Logistic Regression model using the training data.\n",
    "# X_train is the feature matrix of the training data, and y_train is the target variable (labels) for the training data.\n",
    "# The fit() method learns the relationship between features and target variable from the training data.\n",
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explanation\n",
    "# Uses kmeans clustering to create a background dataset from the training data (X_train) for KernelExplainer.\n",
    "background = shap.kmeans(X_train, 50)  # Summarize background using 50 clusters\n",
    "\n",
    "# Initializes a KernelExplainer object. KernelExplainer is model-agnostic and approximates SHAP values for any prediction function.\n",
    "# It uses the logistic.predict_proba function (likely from a trained logistic regression model) to explain predictions.\n",
    "# The 'background' dataset is used to estimate expected values in the SHAP calculation, improving efficiency.\n",
    "explainer = shap.KernelExplainer(\n",
    "    logistic.predict_proba, background\n",
    ")  # Use KernelExplainer\n",
    "\n",
    "# Calculates SHAP values for the test dataset (X_test).\n",
    "# SHAP values quantify the contribution of each feature to the prediction for each instance in X_test, based on the KernelExplainer and the model's predict_proba function.\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary plot\n",
    "# This line generates a SHAP summary plot, which is a visualization to understand feature importance and their impact on the model output.\n",
    "# shap_values: The SHAP values calculated for the test dataset (X_test). These values represent the contribution of each feature to each individual prediction.\n",
    "# X_test: The test dataset used for prediction. This is needed to show the actual feature values in the summary plot.\n",
    "# feature_names=bc.feature_names:  Specifies the names of the features. It's assumed 'bc.feature_names' contains a list of feature names corresponding to the columns in X_test, likely from a dataset object 'bc'.\n",
    "shap.summary_plot(shap_values, X_test, feature_names=bc.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways from the Plot\n",
    "\n",
    "- The SHAP interaction values are centered around 0, meaning no strong interaction effects dominate.\n",
    "- Mean radius and mean texture seem to have moderate interactions, with red and blue points somewhat spread out.\n",
    "- If you see larger deviations from 0 in any interaction, it indicates that the combination of those features significantly impacts predictions (either increasing or decreasing the model output more than their individual contributions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Predictions\n",
    "\n",
    "Let’s dive into interpreting actual predictions from our model. We’ll select two data points from the dataset:\n",
    "- A data point classified as benigh or not having cancer (label 1).\n",
    "- A data point classified as malignant or having cancer (label 0).\n",
    "\n",
    "For each of these instances, we’ll use interpretation techniques (such as LIME or SHAP) to understand how the model arrived at its predictions. This process will help us uncover the key features influencing the model’s decisions and provide insights into the prediction-making process. By doing so, we can better explain the model’s behavior to stakeholders and ensure its predictions are both accurate and interpretable. Let’s get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LimeTabularExplainer for explaining tabular data predictions.\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    X_train,  # Training data (numpy array or pandas DataFrame) used to understand the feature ranges and distributions.\n",
    "    feature_names=bc.feature_names,  # List of feature names corresponding to the columns in X_train.\n",
    "    discretize_continuous=True,  # Whether to discretize continuous features. Set to True for tabular data.\n",
    "    class_names=[\"0\", \"1\"],  # List of class names or labels for the target variable.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate explanation for an individual prediction from the test set using LIME.\n",
    "lime_exp = lime_explainer.explain_instance(X_test[0], logistic.predict_proba)\n",
    "\n",
    "# Display the LIME explanation in the notebook for visual interpretation.\n",
    "lime_exp.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways from the LIME Explanation above\n",
    "\n",
    "1. Prediction Probabilities\n",
    "\n",
    "   - The model predicts 87% (0.87) probability for Benign (1).\n",
    "   - The probability for Malignant (0) is only 13% (0.13).\n",
    "   - This means the model strongly believes the tumor is benigh.\n",
    "\n",
    "2. Feature Contributions\n",
    "\n",
    "   - Features supporting Benign (1) are in orange (positive contribution to benign).\n",
    "   - Features supporting Malignant (0) are in blue (negative contribution to benign, pushing toward malignancy).\n",
    "   - The most important benign indicators are:\n",
    "     - Worst perimeter (96.05)\n",
    "     - Worst area (677.90)\n",
    "     - Mean area (481.90)\n",
    "     - Area error (30.29)\n",
    "   - The most important malignancy indicators (blue) are:\n",
    "     - Mean perimeter (81.09)\n",
    "     - Worst radius (14.97)\n",
    "     - Mean radius (12.47)\n",
    "\n",
    "3. Feature Value Ranges\n",
    "   - The middle section lists decision splits from the model (e.g., “84.54 < worst perimeter <= 97.75” means a higher perimeter increases benign probability).\n",
    "   - Larger values for worst perimeter, worst area, and mean area strongly push the prediction toward benign.\n",
    "\n",
    "#### Final Interpretation\n",
    "\n",
    "Even though some features (blue) support the malignant classification, the dominant benign-supporting features outweigh them. Therefore, the model predicts Benign (1) with high confidence (87%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s apply a similar analytical approach to interpret a data point associated with malignant cancer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the prediction for a single instance (the second instance) from the test set (X_test[1])\n",
    "lime_exp = lime_explainer.explain_instance(X_test[1], logistic.predict_proba)\n",
    "\n",
    "# Display the Lime explanation in the notebook for visual interpretation.\n",
    "lime_exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways from the LIME Explanation above\n",
    "\n",
    "1. Prediction Probabilities\n",
    "   - The model is 100% certain this is a malignant tumor (0.00 probability for benign (1)).\n",
    "   - The blue bar (0) is fully filled, meaning all supporting features push toward malignant.\n",
    "2. Feature Contributions\n",
    "   - Blue bars (supporting malignant classification):\n",
    "     - Worst perimeter (165.90)\n",
    "     - Worst area (1866.00)\n",
    "     - Mean area (1130.00)\n",
    "     - Worst texture (26.58)\n",
    "   - Orange bars (supporting benign classification):\n",
    "     - Mean perimeter (123.60)\n",
    "     - Worst radius (24.86)\n",
    "     - Mean radius (18.94)\n",
    "   - Since the blue features dominate, the model predicts malignant (0) with full confidence.\n",
    "3. Decision Splits & Feature Importance\n",
    "   - The middle section shows decision splits used by the model.\n",
    "     - For example, “mean perimeter > 105.62” slightly supports benign (orange).\n",
    "     - However, features like “worst perimeter > 125.30” strongly push toward malignant.\n",
    "\n",
    "#### Final Interpretation\n",
    "\n",
    "Even though a few features (like mean perimeter and worst radius) slightly support benign, the overall strong benign indicators (worst perimeter, mean area, worst area) outweigh them completely.\n",
    "The model is highly confident this tumor is malignant (100% probability).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "The final and crucial step in the machine learning workflow is model deployment, where we transition from a trained model to a fully operational system that can be used in real-world applications. Deployment is the process of integrating the model into a production environment, making it accessible to end-users or other systems for generating predictions on new data.\n",
    "\n",
    "**Why is Model Deployment Important?**\n",
    "- *Real-World Impact*: A model’s true value is realized only when it is deployed and used to solve real-world problems.\n",
    "- *Automation*: Deployment allows for automated decision-making, reducing the need for manual intervention.\n",
    "- *Scalability*: A deployed model can handle large volumes of data and serve multiple users simultaneously.\n",
    "- *Continuous Improvement*: Once deployed, the model can be monitored and updated to maintain or improve its performance over time.\n",
    "\n",
    "**Key Considerations for Model Deployment**\n",
    "- Choosing the Right Deployment Strategy:\n",
    "    - *Batch Processing*: The model processes data in batches at scheduled intervals (e.g., daily or weekly).\n",
    "    - *Real-Time Inference*: The model provides instant predictions as new data arrives (e.g., fraud detection or recommendation systems).\n",
    "- Infrastructure and Tools:\n",
    "    - *Cloud Platforms*: Services like AWS SageMaker, Google AI Platform, or Azure ML simplify deployment.\n",
    "    - *Containerization*: Tools like Docker and Kubernetes help package and manage models for scalable deployment.\n",
    "    - *APIs*: Exposing the model as a REST API or GraphQL endpoint allows easy integration with other systems.\n",
    "- Monitoring and Maintenance:\n",
    "    - *Performance Monitoring*: Track metrics like prediction accuracy, latency, and throughput to ensure the model is functioning as expected.\n",
    "    - *Data Drift Detection*: Monitor for changes in input data distribution that may degrade model performance.\n",
    "    - *Model Retraining*: Periodically retrain the model with new data to maintain its relevance and accuracy.\n",
    "- Security and Compliance:\n",
    "    - Ensure the deployed model adheres to data privacy regulations (e.g., GDPR, HIPAA).\n",
    "    - Implement security measures to protect the model and data from unauthorized access or attacks.\n",
    "\n",
    "**Steps to Deploy a Model**\n",
    "- *Export the Model*: Save the trained model in a format suitable for deployment (e.g., .pkl for scikit-learn models, .pt for PyTorch models).\n",
    "- *Set Up the Environment*: Create a production environment with the necessary dependencies and infrastructure.\n",
    "- *Build an API*: Use frameworks like Flask, FastAPI, or Django to create an API for serving predictions.\n",
    "- *Deploy to Production*: Use cloud platforms, containers, or serverless architectures to deploy the model.\n",
    "- *Test and Monitor*: Thoroughly test the deployed model and set up monitoring tools to track its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist model to disk\n",
    "\n",
    "To persist our model to disk, we can use serialization libraries such as `pickle` or `joblib` (the latter being available through scikit-learn). This approach enables model deployment and future reuse without requiring retraining every time we need to make predictions. The serialized model file maintains all learned parameters and can be reloaded when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the trained logistic regression model to a file named 'lr_model.pkl' using joblib for later use.\n",
    "joblib.dump(logistic, \"lr_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from disk\n",
    "\n",
    "Whenever we load this object into memory again, we will retrieve the logistic regression model object with its trained parameters intact. This allows us to reuse the model for predictions or further analysis without needing to retrain it each time. The serialized object preserves all the learned weights, feature coefficients, and model configuration, ensuring consistent behavior when reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Logistic Regression model from a pickle file named \"lr_model.pkl\".\n",
    "lr = joblib.load(\"lr_model.pkl\")\n",
    "\n",
    "# Display the loaded Logistic Regression model object. This will show the model's parameters and structure.\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with loaded model\n",
    "Now that the logistic regression model (`lr`) has been successfully loaded from disk, we can use this trained model object to generate predictions on new data. The `lr` object contains all the learned coefficients and model parameters from the training process, allowing us to call standard methods like `.predict()` or `.predict_proba()` on fresh input samples. This enables us to apply the previously trained model to make classifications or probability estimates without needing to retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the true value from the y_test dataset for the index range 10 to 11 (exclusive of 11, so effectively index 10).\n",
    "print(\"True value: \", y_test[10:11])\n",
    "\n",
    "# Print the predicted value for the corresponding input data from X_test dataset at index range 10 to 11, using the trained linear regression model 'lr'.\n",
    "print(\"Predicted value: \", lr.predict(X_test[10:11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this tutorial, you have gained practical experience in making machine learning models more transparent and interpretable, optimizing their performance through proper tuning, and preparing them for real-world deployment. These skills are essential for bridging the gap between technical implementation and business understanding in machine learning projects.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
