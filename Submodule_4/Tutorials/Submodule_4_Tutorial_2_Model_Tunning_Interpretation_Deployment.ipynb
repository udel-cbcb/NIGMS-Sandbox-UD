{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning, Interpretation and Deployment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers essential aspects of the machine learning workflow after initial model building, focusing on model interpretation techniques, tuning approaches, and deployment considerations. We'll explore how to make machine learning models more interpretable, optimize their performance through tuning, and prepare them for real-world deployment.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the importance of model interpretation in machine learning\n",
    "  - Learn how interpretability benefits analytics teams and stakeholders\n",
    "  - Recognize how interpretability bridges technical and business understanding\n",
    "- Master techniques for model tuning and optimization\n",
    "- Learn best practices for model deployment\n",
    "- Develop skills to explain model decisions to non-technical stakeholders\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Implement model interpretation techniques\n",
    "- Perform model tuning exercises\n",
    "- Practice model deployment steps\n",
    "- Create interpretability visualizations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with pandas and numpy libraries\n",
    "- Knowledge of basic statistical concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "- Please select kernel \"conda_tensorflow2_p310\" from SageMaker notebook instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lime shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import shap  # Import SHAP for explanation\n",
    "from lime.lime_tabular import LimeTabularExplainer  # Import LIME for explanation\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tunning, Interpretation and Deployment\n",
    "\n",
    "Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn:\n",
    "\n",
    "- How to tune the hyperparameters of Machine Learning algorithms\n",
    "- How to interpret models using open source frameworks\n",
    "- How to persist and deploy the developed models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "\n",
    "Model tuning is one of the\n",
    "most important concepts of Machine Learning and it does require some knowledge of the underlying math\n",
    "and logic of the algorithm in focus. In this tutorial, we will delve deeper into the models that we are targeting, look at the knobs\n",
    "that can be tuned and set to extract the best performance out of any given models. This process of iterative\n",
    "experimentation with dataset, model parameters, and features is the very core of the model tuning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Evaluate Default Model\n",
    "\n",
    "We will use Wisconsin Breast Cancer Dataset as an example. We first split the breast cancer datast variables X and y into train and test datasets and build an SVM model with default parameters. Then we will evaluate its performance on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wisconsin Breast Cancer Dataset\n",
    "\n",
    "# load data\n",
    "bc = load_breast_cancer()\n",
    "\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "print(X.shape, bc.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for model evaluation\n",
    "\n",
    "# Get model performance evaluation matrics\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print(\n",
    "        \"Accuracy:\", np.round(metrics.accuracy_score(true_labels, predicted_labels), 4)\n",
    "    )\n",
    "    print(\n",
    "        \"Precision:\",\n",
    "        np.round(\n",
    "            metrics.precision_score(true_labels, predicted_labels, average=\"weighted\"),\n",
    "            4,\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        \"Recall:\",\n",
    "        np.round(\n",
    "            metrics.recall_score(true_labels, predicted_labels, average=\"weighted\"), 4\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        \"F1 Score:\",\n",
    "        np.round(\n",
    "            metrics.f1_score(true_labels, predicted_labels, average=\"weighted\"), 4\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# Show the classification report\n",
    "def display_classification_report(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    # Build a text report showing the main classification metrics\n",
    "    # The reported averages include macro average (averaging the unweighted\n",
    "    # mean per label), weighted average (averaging the support-weighted mean\n",
    "    # per label), and sample average (only for multilabel classification).\n",
    "    # Micro average (averaging the total true positives, false negatives and\n",
    "    # false positives) is only shown for multi-label or multi-class\n",
    "    # with a subset of classes, because it corresponds to accuracy\n",
    "    # otherwise and would be the same for all metrics.\n",
    "    report = metrics.classification_report(\n",
    "        y_true=true_labels, y_pred=predicted_labels, labels=classes\n",
    "    )\n",
    "    print(report)\n",
    "\n",
    "\n",
    "# Show the confusion matrix\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    total_classes = len(classes)\n",
    "    level_labels = [total_classes * [0], list(range(total_classes))]\n",
    "    # Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "    cm = metrics.confusion_matrix(\n",
    "        y_true=true_labels, y_pred=predicted_labels, labels=classes\n",
    "    )\n",
    "    cm_frame = pd.DataFrame(\n",
    "        data=cm,\n",
    "        columns=pd.MultiIndex(levels=[[\"Predicted:\"], classes], codes=level_labels),\n",
    "        index=pd.MultiIndex(levels=[[\"Actual:\"], classes], codes=level_labels),\n",
    "    )\n",
    "    print(cm_frame)\n",
    "\n",
    "\n",
    "# Show the model performace matrics\n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    print(\"Model Performance metrics:\")\n",
    "    print(\"-\" * 30)\n",
    "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n",
    "    print(\"\\nModel Classification report:\")\n",
    "    print(\"-\" * 30)\n",
    "    display_classification_report(\n",
    "        true_labels=true_labels, predicted_labels=predicted_labels, classes=classes\n",
    "    )\n",
    "    print(\"\\nPrediction Confusion Matrix:\")\n",
    "    print(\"-\" * 30)\n",
    "    display_confusion_matrix(\n",
    "        true_labels=true_labels, predicted_labels=predicted_labels, classes=classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# build default SVM model\n",
    "# C-Support Vector Classification\n",
    "def_svc = SVC(random_state=42)\n",
    "def_svc.fit(X_train, y_train)\n",
    "\n",
    "# predict and evaluate performance\n",
    "def_y_pred = def_svc.predict(X_test)\n",
    "print(\"Default Model Stats:\")\n",
    "display_model_performance_metrics(\n",
    "    true_labels=y_test, predicted_labels=def_y_pred, classes=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model with Grid Search\n",
    "\n",
    "Since we have chosen a SVM model, we specify some hyperparameters specific\n",
    "to it, which includes the Regularization parameter C (deals with the margin parameter in SVM), the kernel function (used\n",
    "for transforming data into a higher dimensional feature space) and gamma (determines the influence a\n",
    "single training data point has). There are a lot of other hyperparameters to tune, which you can check out [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for further details.\n",
    "\n",
    "We will build a grid by supplying some pre-set values. The next choice is selecting the score or metric we want\n",
    "to maximize here we have chosen to maximize accuracy of the model. Once that is done, we will be using\n",
    "five-fold cross-validation to build multiple models over this grid and evaluate them to get the best model.\n",
    "\n",
    "(This step may take a few minutes to complete.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameter grid\n",
    "grid_parameters = {\n",
    "    \"kernel\": [\"linear\", \"rbf\"],\n",
    "    \"gamma\": [1e-3, 1e-4],\n",
    "    \"C\": [1, 10, 50, 100],\n",
    "}\n",
    "\n",
    "# perform hyperparameter tuning\n",
    "print(\"# Tuning hyper-parameters for accuracy\\n\")\n",
    "\n",
    "# Exhaustive search over specified parameter values for an estimator.\n",
    "clf = GridSearchCV(SVC(random_state=42), grid_parameters, cv=5, scoring=\"accuracy\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# view accuracy scores for all the models\n",
    "print(\"Grid scores for all the models based on CV:\\n\")\n",
    "means = clf.cv_results_[\"mean_test_score\"]\n",
    "stds = clf.cv_results_[\"std_test_score\"]\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
    "    print(\"%0.5f (+/-%0.05f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "# check out best model performance\n",
    "print(\"\\nBest parameters set found on development set:\", clf.best_params_)\n",
    "print(\"Best model validation accuracy:\", clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the best model parameters were obtained\n",
    "based on cross-validation accuracy and we get a pretty awesome validation accuracy of 96%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Grid Search Tuned Model\n",
    "\n",
    "Let’s take this\n",
    "optimized and tuned model and put it to the test on our test data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_best = clf.best_estimator_\n",
    "tuned_y_pred = gs_best.predict(X_test)\n",
    "\n",
    "print(\"\\n\\nTuned Model Stats:\")\n",
    "display_model_performance_metrics(\n",
    "    true_labels=y_test, predicted_labels=tuned_y_pred, classes=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model gives an overall F1 Score and model\n",
    "accuracy of 97% on the test dataset too. This should give you a clear indication of\n",
    "the power of hyperparameter tuning! This scheme of things can be extended for different models and their\n",
    "respective hyperparameters. We can also play around with the evaluation measure we want to optimize.\n",
    "The scikit-learn framework provides us with different values that we can optimize. Some of them are\n",
    "adjusted_rand_score, average_precision, f1, average_recall, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model with Randomized Search\n",
    "\n",
    "Grid search suffers from some major shortcomings, the most important one being\n",
    "the limitation of manually specifying the grid. This brings a human element into a process that could benefit\n",
    "from a purely automatic mechanism.\n",
    "\n",
    "Randomized parameter search is a modification to the traditional grid search. It takes input for\n",
    "grid elements as in normal grid search but it can also take distributions as input. For example consider\n",
    "the parameter gamma whose values we supplied explicitly in the last section instead we can supply a\n",
    "distribution from which to sample gamma.\n",
    "\n",
    "The efficacy of randomized parameter search is based on the\n",
    "proven (empirically and mathematically) result that the hyperparameter optimization functions normally\n",
    "have low dimensionality and the effect of certain parameters are more than others.\n",
    "\n",
    "We control the number\n",
    "of times we want to do the random parameter sampling by specifying the number of iterations we want to\n",
    "run (n_iter). Normally a higher number of iterations mean a more granular parameter search but higher\n",
    "computation time.\n",
    "\n",
    "(This step may take a few minutes to complete.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the gamma and C values with a distribution (exponential distribution)\n",
    "param_grid = {\n",
    "    \"C\": scipy.stats.expon(scale=10),\n",
    "    \"gamma\": scipy.stats.expon(scale=0.1),\n",
    "    \"kernel\": [\"rbf\", \"linear\"],\n",
    "}\n",
    "\n",
    "# Randomized search on hyperparameters.\n",
    "random_search = RandomizedSearchCV(\n",
    "    SVC(random_state=42), param_distributions=param_grid, n_iter=50, cv=5\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Grid scores for all the models based on CV:\\n\")\n",
    "means = random_search.cv_results_[\"mean_test_score\"]\n",
    "stds = random_search.cv_results_[\"std_test_score\"]\n",
    "for mean, std, params in zip(means, stds, random_search.cv_results_[\"params\"]):\n",
    "    print(\"%0.5f (+/-%0.05f) for %r\" % (mean, std * 2, params))\n",
    "print(\"\\nBest parameters set found on development set:\", random_search.best_params_)\n",
    "print(\"Best model validation accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Randomized Search Tuned Model\n",
    "\n",
    "Get the best model, predict and evaluate performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_best = random_search.best_estimator_\n",
    "rs_y_pred = rs_best.predict(X_test)\n",
    "get_metrics(true_labels=y_test, predicted_labels=rs_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting the values of parameter C and gamma from an exponential distribution\n",
    "and we are controlling the number of iterations of model search by the parameter n_iter. While the overall\n",
    "model performance is similar to grid search, the intent is to be aware of the different strategies in model tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "The ability to interpret Machine Learning models in an easy to understand way will benefit not only analytics teams but also key stakeholders in trying to explain how models really work.\n",
    "\n",
    "Some Machine Learning models use interpretable algorithms, for example a decision tree will give you\n",
    "the importance of all the variables as an output. Unfortunately, this\n",
    "can’t be said for a lot of models, especially for the ones who have no notion of variable importance.\n",
    "\n",
    "The lack of understanding of the complex nature\n",
    "of Machine Learned decision policies makes predictive models to be still viewed as black boxes. Model\n",
    "interpretations can help a data scientist and an end user in a variety of ways.\n",
    "\n",
    "- It will help bridge the gap that\n",
    "  often exists between the technology teams and the business. For example, it can help identify the reason\n",
    "  why a particular prediction is being made and it can be verified using the domain knowledge of the end\n",
    "  user by leveraging that easy to understand interpretation.\n",
    "- It can also help the data scientists understand the\n",
    "  interactions among features that can lead to better feature engineering and enhanced performance.\n",
    "- It can\n",
    "  also help in model comparisons and explaining the results better to the business stakeholders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding LIME and SHAP\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) are two powerful tools designed to explain machine learning model predictions in an interpretable manner, making complex models more transparent. LIME works by approximating a machine learning model with a simple, interpretable model (such as a linear regression) around the instance being predicted. It perturbs the input data, generates predictions, and learns a locally faithful model that explains the specific prediction for that instance. This method provides insights into how a model makes decisions for individual instances. SHAP, on the other hand, is grounded in game theory and computes the contribution of each feature to the model’s prediction by distributing the prediction's total impact fairly across all features. SHAP values are additive, and the method provides both local and global interpretability, making it highly effective for understanding the overall impact of features on model predictions. While LIME focuses on local explanations, SHAP excels in providing consistent global feature importance, making both techniques complementary for enhancing model transparency and trustworthiness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use a logistic regression model to do classification.\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explanation (instead of Skater's Interpretation)\n",
    "background = shap.kmeans(X_train, 50)  # Summarize background using 50 clusters\n",
    "explainer = shap.KernelExplainer(logistic.predict_proba, background)  # Use KernelExplainer\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_test, feature_names=bc.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways from the Plot\n",
    "- The SHAP interaction values are centered around 0, meaning no strong interaction effects dominate.\n",
    "- Mean radius and mean texture seem to have moderate interactions, with red and blue points somewhat spread out.\n",
    "- If you see larger deviations from 0 in any interaction, it indicates that the combination of those features significantly impacts predictions (either increasing or decreasing the model output more than their individual contributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Predictions\n",
    "\n",
    "Let’s try to interpret some actual predictions now. We will predict two data points, one not\n",
    "having cancer (label 1) and one having cancer (label 0), and try to interpret the prediction making process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME Explanation (instead of Skater's LimeTabularExplainer)\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    X_train,\n",
    "    feature_names=bc.feature_names,\n",
    "    discretize_continuous=True,\n",
    "    class_names=[\"0\", \"1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate explanation for an individual prediction (LIME)\n",
    "lime_exp = lime_explainer.explain_instance(X_test[0], logistic.predict_proba)\n",
    "lime_exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways from the LIME Explanation above\n",
    "1. Prediction Probabilities\n",
    "    - The model predicts 87% (0.87) probability for Malignant (1).\n",
    "    - The probability for Benign (0) is only 13% (0.13).\n",
    "    - This means the model strongly believes the tumor is malignant.\n",
    "\n",
    "2. Feature Contributions\n",
    "    - Features supporting Malignant (1) are in orange (positive contribution to malignancy).\n",
    "    - Features supporting Benign (0) are in blue (negative contribution to malignancy, pushing toward benign).\n",
    "    - The most important malignant indicators are:\n",
    "        - Worst perimeter (96.05)\n",
    "        - Worst area (677.90)\n",
    "        - Mean area (481.90)\n",
    "        - Area error (30.29)\n",
    "    - The most important benign indicators (blue) are:\n",
    "        - Worst radius (14.97)\n",
    "        - Mean perimeter (81.09)\n",
    "        - Mean radius (12.47)\n",
    "\n",
    "3. Feature Value Ranges\n",
    "    - The middle section lists decision splits from the model (e.g., “84.54 < worst perimeter <= 97.75” means a higher perimeter increases malignancy probability).\n",
    "    - Larger values for worst perimeter, worst area, and mean area strongly push the prediction toward malignancy.\n",
    "\n",
    "#### Final Interpretation\n",
    "\n",
    "Even though some features (blue) support the benign classification, the dominant malignant-supporting features outweigh them. Therefore, the model predicts Malignant (1) with high confidence (87%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run a similar interpretation on a data point with malignant\n",
    "cancer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_exp = lime_explainer.explain_instance(X_test[1], logistic.predict_proba)\n",
    "lime_exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways from the LIME Explanation above\n",
    "1. Prediction Probabilities\n",
    "    - The model is 100% certain this is a benign tumor (0.00 probability for malignant (1)).\n",
    "    - The blue bar (0) is fully filled, meaning all supporting features push toward benign.\n",
    "2. Feature Contributions\n",
    "    - Blue bars (supporting benign classification):\n",
    "        - Worst perimeter (165.90)\n",
    "        - Mean area (1130.00)\n",
    "        - Worst area (1866.00)\n",
    "        - Worst texture (26.58)\n",
    "    - Orange bars (supporting malignant classification):\n",
    "        - Mean perimeter (123.60)\n",
    "        - Worst radius (24.86)\n",
    "        - Mean radius (18.94)\n",
    "        - Area error (96.05)\n",
    "    - Since the blue features dominate, the model predicts benign (0) with full confidence.\n",
    "3. Decision Splits & Feature Importance\n",
    "    - The middle section shows decision splits used by the model.\n",
    "        - For example, “mean perimeter > 105.62” slightly supports malignancy (orange).\n",
    "        - However, features like “worst perimeter > 125.30” strongly push toward benign.\n",
    "#### Final Interpretation\n",
    "\n",
    "Even though a few features (like mean perimeter and worst radius) slightly support malignancy, the overall strong benign indicators (worst perimeter, mean area, worst area) outweigh them completely.\n",
    "The model is highly confident this tumor is benign (100% probability).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "The final piece of the Machine Learning modeling puzzle is that of\n",
    "deploying the model in production so that we actually start using it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist model to disk\n",
    "\n",
    "For persisting our model to disk, we can leverage libraries like pickle or joblib, which is also available\n",
    "with scikit-learn. This allows us to deploy and use the model in the future, without having to retrain it\n",
    "each time we want to use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(logistic, \"lr_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from disk\n",
    "\n",
    "So whenever we will load\n",
    "this object in memory again we will get the logistic regression model object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = joblib.load(\"lr_model.pkl\")\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with loaded model\n",
    "\n",
    "We can now use this lr object, which is our model loaded from the disk, and make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True value: \", y_test[10:11])\n",
    "print(\"Predicted value: \", lr.predict(X_test[10:11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this tutorial, you have gained practical experience in making machine learning models more transparent and interpretable, optimizing their performance through proper tuning, and preparing them for real-world deployment. These skills are essential for bridging the gap between technical implementation and business understanding in machine learning projects.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
