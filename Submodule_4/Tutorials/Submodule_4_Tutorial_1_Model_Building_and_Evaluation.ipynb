{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation\n",
    "\n",
    "Adapted from Wafiq Syed 2020 [How to use Scikit-Learn Datasets for Machine Learning](https://towardsdatascience.com/how-to-use-scikit-learn-datasets-for-machine-learning-d6493b38eca3) and Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers core concepts in model building and evaluation, focusing on classification and clustering approaches using the Breast Cancer Wisconsin dataset. We'll explore various algorithms and metrics for evaluating their performance.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn to build and evaluate classification models using metrics like accuracy, precision, recall, and F1-score\n",
    "- Understand how to interpret confusion matrices and ROC curves\n",
    "- Apply clustering model evaluation using both external and internal validation methods\n",
    "- Apply practical model evaluation techniques using scikit-learn's metrics module\n",
    "- Gain hands-on experience with real biomedical data analysis\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Build classification models\n",
    "- Evaluate model performance using various metrics\n",
    "- Create and analyze clustering models\n",
    "- Generate performance visualizations\n",
    "- Compare different evaluation approaches\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with numpy and sklearn libraries\n",
    "- Knowledge of basic statistical concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "### Set up conda environment\n",
    "\n",
    "Ensure that you have created then conda environment using the `environment.yml` file included in this repository.  E.g.,\n",
    "\n",
    "```\n",
    "# Create conda environment\n",
    "conda env create -f conda_env_submodule_4.yml\n",
    "\n",
    "# Register the kernel\n",
    "python -m ipykernel install --user \\\n",
    "    --name=nigms_sandbox_ud \\\n",
    "    --display-name \"Python (NIGMS Sandbox UD, Submodule 4)\"\n",
    "```\n",
    "\n",
    "Then, when starting the notebook, select the  `\"Python (nigms_sandbox_ud)\"` kernel from the list.\n",
    "\n",
    "Note that you may need to restart Jupyter Lab for these changes to take effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics  # TODO this should probably be removed\n",
    "from scipy import interp  # TODO interp is gone\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster, linkage\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.base import clone\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "\n",
    "# sets the backend of matplotlib to the 'inline' backend: With this backend,\n",
    "# the output of plotting commands is displayed inline within frontends like\n",
    "# the Jupyter notebook, directly below the code cell that produced it.\n",
    "# The resulting plots will then also be stored in the notebook document.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Example\n",
    "\n",
    "In this example, we’ll be working with the “Breast Cancer Wisconsin” dataset. We will import the data and understand how to read it. We will also build a simple ML model that is able to classify cancer scans either as malignant or benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import “Breast Cancer Wisconsin” dataaset\n",
    "\n",
    "The dataset can be found in *sklearn.datasets*. Each dataset has a corresponding function used to load the dataset. These functions follow the same format: \"load_DATASET()\", where DATASET refers to the name of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These load functions (such as *load_breast_cancer()*) don't return data in the tabular format, they return a **Bunch** object, a Scikit-Learn's fancy name for a Dictionary.\n",
    "\n",
    "Let's looking into its keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bc.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the following keys:\n",
    "\n",
    "* **data** is all the feature data (the attributes of the scan that help us identify if the tumor is malignant or benign, such as radius, area, etc.) in a NumPy array\n",
    "* **target** is the target data (the variable you want to predict, in this case whether the tumor is malignant or benign) in a NumPy array,\n",
    "* **feature_names** are the names of the feature variables, in other words names of the columns in data\n",
    "* **target_names** is the name(s) of the target variable(s), in other words name(s) of the target column(s)\n",
    "* **DESCR**, short for DESCRIPTION, is a description of the dataset\n",
    "* **filename** is the path to the actual file of the data in CSV format.\n",
    "* **data_module** is the name of the data module from where the data is being loaded.\n",
    "\n",
    "It’s important to note that all of Scikit-Learn datasets are divided into data and target. data represents the features, which are the variables that help the model learn how to predict. target includes the actual labels. In our case, the target data is one column classifies the tumor as either 0 indicating malignant or 1 for benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look the description of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bc.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Dataset\n",
    "\n",
    "We can use *pandas* to explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the DataFrame, first using the feature data\n",
    "df = pd.DataFrame(bc.data, columns=bc.feature_names)\n",
    "\n",
    "# Add a target column, and fill it with the target data\n",
    "df[\"target\"] = bc.target\n",
    "\n",
    "# Show the first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the value of this dataset, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to observe:\n",
    "\n",
    "* There aren’t any missing values, all the columns have 569 values. This saves us time from having to account for missing values.\n",
    "* All the data types are numerical. This is important because Scikit-Learn models do not accept categorical variables. In the real world, when we get categorical variables, we transform them into numerical variables. Scikit-Learn’s datasets are free of categorical variables.\n",
    "\n",
    "Hence, Scikit-Learn takes care of the data cleansing work. Their datasets are extremely valuable. You will benefit from learning ML by using them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do some AI\n",
    "\n",
    "Let’s build a model that classifies cancer tumors as malignant (spreading) or benign (non-spreading). This will show you how to use the data for your own models. We’ll build a simple K-Nearest Neighbors model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s split the dataset into two, one for training the model — giving it data to learn from, and the second for testing the model — seeing how well the model performs on data (scans) it hasn’t seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the feature data\n",
    "X = bc.data\n",
    "\n",
    "# store the target data\n",
    "y = bc.target\n",
    "\n",
    "# split the data using Scikit-Learn's train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us two datasets —one for training and one for testing. Let’s get onto training the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier implementing the k-nearest neighbors vote.\n",
    "logreg = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the k-nearest neighbors classifier from the training dataset.\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Return the mean accuracy on the given test data and labels.\n",
    "print(\"Model accurcy: \", logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Example\n",
    "\n",
    "In this example, we will learn how we can fit a clustering model on “Breast Cancer Wisconsin” dataset. We will use a labeled dataset to help us see the results of the clustering model and compare it with actual labels. A point to remember here is that, usually labeled data is not available in the real world,\n",
    "which is why we choose to go for unsupervised methods like clustering. We will try to cover two different\n",
    "algorithms, one each from partitioning based clustering and hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wisconsin Breast Cancer Dataset\n",
    "\n",
    "# load data\n",
    "bc = load_breast_cancer()\n",
    "\n",
    "# Store the feature data\n",
    "X = bc.data\n",
    "\n",
    "# store the target data\n",
    "y = bc.target\n",
    "print(X.shape, bc.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that we have a total of 569 observations and 30 attributes or features for each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition based Clustering Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose the simplest yet most popular partition based clustering model for our example, which\n",
    "is **K-means** algorithm. This algorithm is a centroid based clustering algorithm, which starts with some\n",
    "assumption about the total clusters in the data and with random centers assigned to each of the clusters.\n",
    "It then reassigns each data point to the center closest to it, using Euclidean distance as the distance metric.\n",
    "After each reassignment, it recalculates the center of that cluster. The whole process is repeated iteratively\n",
    "and stopped when reassignment of data points doesn’t change the cluster centers. Variants include\n",
    "algorithms like **K-medoids**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine these two clusters (either 0 or 1) from the data by K-means clustering\n",
    "\n",
    "km = KMeans(n_clusters=2, random_state=2)\n",
    "km.fit(X)\n",
    "\n",
    "labels = km.labels_\n",
    "centers = km.cluster_centers_\n",
    "\n",
    "# labels of the first 10 data points\n",
    "print(labels[:10])\n",
    "\n",
    "# numerical value of the dimensions of the data\n",
    "# (the 30 attributes in the dataset) around which data is clustered.\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will leverage PCA to reduce the input dimensions (30) to two principal components\n",
    "# and visualize the clusters on top of the same.\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "bc_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the clusters on the reduced 2D feature space for the actual labels as\n",
    "# well as the clustered output labels.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "fig.suptitle(\"Visualizing breast cancer clusters\")\n",
    "fig.subplots_adjust(top=0.85, wspace=0.5)\n",
    "ax1.set_title(\"Actual Labels\")\n",
    "ax2.set_title(\"Clustered Labels\")\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 0:\n",
    "        c1 = ax1.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"g\", marker=\".\")\n",
    "    if y[i] == 1:\n",
    "        c2 = ax1.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"r\", marker=\".\")\n",
    "\n",
    "    if labels[i] == 0:\n",
    "        c3 = ax2.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"g\", marker=\".\")\n",
    "    if labels[i] == 1:\n",
    "        c4 = ax2.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"r\", marker=\".\")\n",
    "\n",
    "l1 = ax1.legend([c1, c2], [\"0\", \"1\"])\n",
    "l2 = ax2.legend([c3, c4], [\"0\", \"1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the clustering has worked quite well and it shows distinct\n",
    "separation between clusters with labels 0 and 1 and is quite similar to the actual labels. However we do\n",
    "have some overlap where we have mislabeled some instances.\n",
    "\n",
    "Remember in an actual real-world scenario, you will not have the actual labels to compare with and the\n",
    "main idea is to find structures or patterns in your data in the form of these clusters. \n",
    "Hence even when dealing with labeled data and running clustering do not\n",
    "compare clustered label values with actual labels and try to measure accuracy.\n",
    "\n",
    "Another very important\n",
    "point to remember is that cluster label values have no significance. The labels 0 and 1 are just values to\n",
    "distinguish cluster data points from each other. \n",
    "\n",
    "Also another important note\n",
    "is that if we had asked for more than two clusters, the algorithm would have readily supplied more clusters\n",
    "but it would have been hard to interpret those and many of them would not make sense. Hence, one of\n",
    "the caveats of using the K-means algorithm is to use it in the case where we have some idea about the total\n",
    "number of clusters that may exist in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering Example\n",
    "\n",
    "We can use the same data to perform a hierarchical clustering and see if the results change much as\n",
    "compared to K-means clustering and the actual labels. \n",
    "\n",
    "agglomerative clustering is hierarchical clustering using a\n",
    "bottom up approach i.e. each observation starts in its own cluster and clusters are successively merged\n",
    "together. The merging criteria can be used from a candidate set of linkages; the selection of linkage governs\n",
    "the merge strategy. Some examples of linkage criteria are Ward, Complete linkage, Average linkage and so\n",
    "on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the linkage matrix using Ward’s minimum variance criterion.\n",
    "\n",
    "# Perform hierarchical/agglomerative clustering.\n",
    "# Returns hierarchical clustering encoded as a linkage matrix.\n",
    "Z = linkage(X, \"ward\")\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a dendrogram to visualize the distance-based merges\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Data point\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "# Plot the hierarchical clustering as a dendrogram.\n",
    "dendrogram(Z)\n",
    "\n",
    "# y: position in data coordinates of the horizontal line.\n",
    "# c: line color, 'k' as black\n",
    "# ls: line style\n",
    "# lw: line weight\n",
    "plt.axhline(y=10000, c=\"k\", ls=\"--\", lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster labels\n",
    "\n",
    "max_dist = 10000\n",
    "\n",
    "# Form flat clusters from the hierarchical clustering defined by\n",
    "# the given linkage matrix\n",
    "hc_labels = fcluster(Z, max_dist, criterion=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s compare how the cluster outputs look based on the PCA reduced dimensions as\n",
    "# compared to the original label distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "fig.suptitle(\"Visualizing breast cancer clusters\")\n",
    "fig.subplots_adjust(top=0.85, wspace=0.5)\n",
    "ax1.set_title(\"Actual Labels\")\n",
    "ax2.set_title(\"Hierarchical Clustered Labels\")\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 0:\n",
    "        c1 = ax1.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"g\", marker=\".\")\n",
    "    if y[i] == 1:\n",
    "        c2 = ax1.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"r\", marker=\".\")\n",
    "\n",
    "    if hc_labels[i] == 1:\n",
    "        c3 = ax2.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"g\", marker=\".\")\n",
    "    if hc_labels[i] == 2:\n",
    "        c4 = ax2.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"r\", marker=\".\")\n",
    "\n",
    "l1 = ax1.legend([c1, c2], [\"0\", \"1\"])\n",
    "l2 = ax2.legend([c3, c4], [\"1\", \"2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We definitely see two distinct clusters but there is more overlap as compared to the K-means method\n",
    "between the two clusters and we have more mislabeled instances. However, do take a note of the label\n",
    "numbers; here we have 1 and 2 as the label values. This is just to reinforce the fact that the label values are\n",
    "just to distinguish the clusters and don’t mean anything. The advantage of this method is that you do not\n",
    "need to input the number of clusters beforehand and the model tries to find it from the underlying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s first prepare train and test datasets to build our classification models.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s build a logistic regression model on our breast cancer dataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix for the model predictions on the test dataset.\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    total_classes = len(classes)\n",
    "    level_labels = [total_classes * [0], list(range(total_classes))]\n",
    "    cm = metrics.confusion_matrix(\n",
    "        y_true=true_labels, y_pred=predicted_labels, labels=classes\n",
    "    )\n",
    "    cm_frame = pd.DataFrame(\n",
    "        data=cm,\n",
    "        columns=pd.MultiIndex(levels=[[\"Predicted:\"], classes], codes=level_labels),\n",
    "        index=pd.MultiIndex(levels=[[\"Actual:\"], classes], codes=level_labels),\n",
    "    )\n",
    "    print(cm_frame)\n",
    "\n",
    "\n",
    "# predict on test data and view confusion matrix\n",
    "y_pred = logistic.predict(X_test)\n",
    "display_confusion_matrix(true_labels=y_test, predicted_labels=y_pred, classes=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that out of\n",
    "60 observations with label 0 (malignant), our model has correctly predicted 59 observations. Similarly out of\n",
    "111 observations with label 1 (benign), our model has correctly predicted 107 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive, False Positive, True Negative and False Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set positive class label\n",
    "positive_class = 1\n",
    "\n",
    "# True Positive (TP): This is the count of the total number of instances from the\n",
    "# positive class where the true class label was equal to the predicted class label.\n",
    "TP = 107\n",
    "\n",
    "# False Positive (FP): This is the count of the total number of instances from the\n",
    "# negative class where our model misclassified them by predicting them as positive.\n",
    "FP = 4\n",
    "\n",
    "# True Negative (FN): This is the count of the total number of instances from the\n",
    "# negative class where the true class label was equal to the predicted class label.\n",
    "TN = 59\n",
    "\n",
    "# False Negative (FN): This is the count of the total number of instances from the\n",
    "# positive class where our model misclassified them by predicting them as negative.\n",
    "FN = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "This is one of the most popular measures of classifier performance. It is defined as the overall\n",
    "accuracy or proportion of correct predictions of the model. The formula for computing accuracy from the\n",
    "confusion matrix is:\n",
    "\n",
    "$Accurcy=\\frac{TP+TN}{TP+FP+TN+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_acc = round(metrics.accuracy_score(y_true=y_test, y_pred=y_pred), 5)\n",
    "mc_acc = round((TP + TN) / (TP + TN + FP + FN), 5)\n",
    "\n",
    "print(\"Framework Accuracy:\", fw_acc)\n",
    "print(\"Manually Computed Accuracy:\", mc_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision, also known as positive predictive value, is another metric that can be derived from\n",
    "the confusion matrix. It is defined as the number of predictions made that are actually correct or relevant out\n",
    "of all the predictions based on the positive class. The formula for precision is as follows:\n",
    "\n",
    "$Precision=\\frac{TP}{TP+FP}$\n",
    "\n",
    "A model with high precision will identify a higher fraction of positive class as compared to a model\n",
    "with a lower precision. Precision becomes important in cases where we are more concerned about finding\n",
    "the maximum number of positive class even if the total accuracy reduces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_prec = round(metrics.precision_score(y_true=y_test, y_pred=y_pred), 5)\n",
    "mc_prec = round((TP) / (TP + FP), 5)\n",
    "\n",
    "print(\"Framework Precision:\", fw_prec)\n",
    "print(\"Manually Computed Precision:\", mc_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall, also known as sensitivity, is a measure of a model to identify the percentage of relevant\n",
    "data points. It is defined as the number of instances of the positive class that were correctly predicted. This is\n",
    "also known as hit rate, coverage, or sensitivity. The formula for recall is:\n",
    "\n",
    "$Recall=\\frac{TP}{TP+FN}$\n",
    "\n",
    "Recall becomes an important measure of classifier performance in scenarios where we want to catch\n",
    "the most number of instances of a particular class even when it increases our false positives. For example,\n",
    "consider the case of bank fraud, a model with high recall will give us higher number of potential fraud cases.\n",
    "But it will also help us raise alarm for most of the suspicious cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_rec = round(metrics.recall_score(y_true=y_test, y_pred=y_pred), 5)\n",
    "mc_rec = round((TP) / (TP + FN), 5)\n",
    "\n",
    "print(\"Framework Recall:\", fw_rec)\n",
    "print(\"Manually Computed Recall:\", mc_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score\n",
    "\n",
    "There are some cases in which we want a balanced optimization of both precision and recall.\n",
    "F1 score is a metric that is the harmonic mean of precision and recall and helps us optimize a classifier for\n",
    "balanced precision and recall performance.\n",
    "The formula for the F1 score is:\n",
    "\n",
    "$F1 Score = \\frac{2 x Precision x Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_f1 = round(metrics.f1_score(y_true=y_test, y_pred=y_pred), 5)\n",
    "mc_f1 = round((2 * mc_prec * mc_rec) / (mc_prec + mc_rec), 5)\n",
    "\n",
    "print(\"Framework F1-Score:\", fw_f1)\n",
    "print(\"Manually Computed F1-Score:\", mc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "The ROC curve can be created by plotting the fraction of true positives versus the fraction of false\n",
    "positives, i.e. it is a plot of True Positive Rate (TPR) versus the False Positive Rate (FPR). It is applicable\n",
    "mostly for scoring classifiers. Scoring classifiers are the type of classifiers which will return a probability\n",
    "value or score for each class label, from which a class label can be deduced (based on maximum probability\n",
    "value).\n",
    "\n",
    "This curve can be plotted using the true positive rate (TPR) and the false positive rate (FPR) of a\n",
    "classifier. TPR is known as sensitivity or recall, which is the total number of correct positive results, predicted\n",
    "among all the positive samples the dataset. FPR is known as false alarms or (1 - specificity), determining the\n",
    "total number of incorrect positive predictions among all negative samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_roc_curve(\n",
    "    clf, features, true_labels, label_encoder=None, class_names=None\n",
    "):\n",
    "    ## Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    if hasattr(clf, \"classes_\"):\n",
    "        class_labels = clf.classes_\n",
    "    elif label_encoder:\n",
    "        class_labels = label_encoder.classes_\n",
    "    elif class_names:\n",
    "        class_labels = class_names\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unable to derive prediction classes, please specify class_names!\"\n",
    "        )\n",
    "    n_classes = len(class_labels)\n",
    "    y_test = label_binarize(true_labels, classes=class_labels)\n",
    "    if n_classes == 2:\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            prob = clf.predict_proba(features)\n",
    "            y_score = prob[:, prob.shape[1] - 1]\n",
    "        elif hasattr(clf, \"decision_function\"):\n",
    "            prob = clf.decision_function(features)\n",
    "            y_score = prob[:, prob.shape[1] - 1]\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"Estimator doesn't have a probability or confidence scoring system!\"\n",
    "            )\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(\n",
    "            fpr,\n",
    "            tpr,\n",
    "            label=\"ROC curve (area = {0:0.2f})\" \"\".format(roc_auc),\n",
    "            linewidth=2.5,\n",
    "        )\n",
    "\n",
    "    elif n_classes > 2:\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            y_score = clf.predict_proba(features)\n",
    "        elif hasattr(clf, \"decision_function\"):\n",
    "            y_score = clf.decision_function(features)\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"Estimator doesn't have a probability or confidence scoring system!\"\n",
    "            )\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        ## Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        ## Compute macro-average ROC curve and ROC area\n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        ## Plot ROC curves\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(\n",
    "            fpr[\"micro\"],\n",
    "            tpr[\"micro\"],\n",
    "            label=\"micro-average ROC curve (area = {0:0.2f})\" \"\".format(\n",
    "                roc_auc[\"micro\"]\n",
    "            ),\n",
    "            linewidth=3,\n",
    "        )\n",
    "\n",
    "        plt.plot(\n",
    "            fpr[\"macro\"],\n",
    "            tpr[\"macro\"],\n",
    "            label=\"macro-average ROC curve (area = {0:0.2f})\" \"\".format(\n",
    "                roc_auc[\"macro\"]\n",
    "            ),\n",
    "            linewidth=3,\n",
    "        )\n",
    "\n",
    "        for i, label in enumerate(class_labels):\n",
    "            plt.plot(\n",
    "                fpr[i],\n",
    "                tpr[i],\n",
    "                label=\"ROC curve of class {0} (area = {1:0.2f})\" \"\".format(\n",
    "                    label, roc_auc[i]\n",
    "                ),\n",
    "                linewidth=2,\n",
    "                linestyle=\":\",\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\"Number of classes should be atleast 2 or more\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_model_roc_curve(clf=logistic, features=X_test, true_labels=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, the best prediction model would give a point\n",
    "on the top left corner (0, 1) indicating perfect classification (100% sensitivity & specificity). A diagonal line\n",
    "depicts a classifier that does a random guess. Ideally if your ROC curve occurs in the top half of the graph,\n",
    "you have a decent classifier which is better than average. The plot above shows a near perfect ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Model Evaluation Metrics\n",
    "\n",
    "The lack of a validated ground truth, i.e. the absence of true labels in the data makes the  evaluation of clustering (or unsupervised models in general) very difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build two clustering models on the breast cancer dataset\n",
    "\n",
    "We will leverage the breast cancer\n",
    "dataset available in the variables X for the data and y for the observation labels. We will also use the K-means\n",
    "algorithm to fit two models on this data—one with two clusters and the second one with five clusters—and\n",
    "then evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2 = KMeans(n_clusters=2, random_state=42).fit(X)\n",
    "km2_labels = km2.labels_\n",
    "\n",
    "km5 = KMeans(n_clusters=5, random_state=42).fit(X)\n",
    "km5_labels = km5.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External validation\n",
    "\n",
    "External validation means validating the clustering model when we have some ground truth available\n",
    "as labeled data. The presence of external labels reduces most of the complexity of model evaluation as\n",
    "the clustering (unsupervised) model can be validated in similar fashion to classification models.\n",
    "\n",
    "Three popular metrics can be used in this scenario:\n",
    "\n",
    "* **Homogeneity**: A clustering model prediction result satisfies homogeneity if all of\n",
    "its clusters contain only data points that are members of a single class (based on the\n",
    "true class labels).\n",
    "* **Completeness**: A clustering model prediction result satisfies completeness if\n",
    "all the data points of a specific ground truth class label are also elements of the\n",
    "same cluster.\n",
    "* **V-measure**: The harmonic mean of homogeneity and completeness scores gives us\n",
    "the V-measure value.\n",
    "\n",
    "Values are typically bounded between 0 and 1 and usually higher values are better. Let’s compute these\n",
    "metric on our two K-means clustering models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2_hcv = np.round(metrics.homogeneity_completeness_v_measure(y, km2_labels), 3)\n",
    "km5_hcv = np.round(metrics.homogeneity_completeness_v_measure(y, km5_labels), 3)\n",
    "\n",
    "print(\"Homogeneity, Completeness, V-measure metrics for num clusters=2: \", km2_hcv)\n",
    "print(\"Homogeneity, Completeness, V-measure metrics for num clusters=5: \", km5_hcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the V-measure for the first model with two clusters is better than the one with five\n",
    "clusters and the reason is because of higher completeness score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal validation\n",
    "\n",
    "Internal validation means validating a clustering model by defining metrics that capture the expected\n",
    "behavior of a good clustering model. A good clustering model can be identified by two very desirable traits:\n",
    "\n",
    "* Compact groups, i.e. the data points in one cluster occur close to each other.\n",
    "* Well separated groups, i.e. two groups\\clusters have as large distance among\n",
    "them as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Silhouette Coefficient\n",
    "\n",
    "Silhouette coefficient is a metric that tries to combine the two requirements of a good clustering model. The\n",
    "silhouette coefficient is defined for each sample and is a combination of its similarity to the data points in its\n",
    "own cluster and its dissimilarity to the data points not in its cluster.\n",
    "\n",
    "The silhouette coefficient is usually bounded between -1 (incorrect clustering) and +1 (excellent quality\n",
    "dense clusters). A higher value of silhouette coefficient generally means that the clustering model is leading\n",
    "to clusters that are dense and well separated and distinguishable from each other. Lower scores indicate\n",
    "overlapping clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2_silc = metrics.silhouette_score(X, km2_labels, metric=\"euclidean\")\n",
    "km5_silc = metrics.silhouette_score(X, km5_labels, metric=\"euclidean\")\n",
    "\n",
    "print(\"Silhouette Coefficient for num clusters=2: \", km2_silc)\n",
    "print(\"Silhouette Coefficient for num clusters=5: \", km5_silc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that from the metric results it seems like we have better\n",
    "cluster quality with two clusters as compared to five clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calinski-Harabaz Index\n",
    "\n",
    "The Calinski-Harabaz index is another metric that we can use to evaluate clustering models when the\n",
    "ground truth is not known. The Calinski-Harabaz score is given as the ratio of the between-clusters\n",
    "dispersion and the within-cluster dispersion.\n",
    "\n",
    "A higher score normally indicates that the clusters are dense and well separated, which\n",
    "relates to the general principles of clustering models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2_chi = metrics.calinski_harabasz_score(X, km2_labels)\n",
    "km5_chi = metrics.calinski_harabasz_score(X, km5_labels)\n",
    "\n",
    "print(\"Calinski-Harabaz Index for num clusters=2: \", km2_chi)\n",
    "print(\"Calinski-Harabaz Index for num clusters=5: \", km5_chi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both the scores are pretty high with the results for five clusters being even higher. This\n",
    "goes to show that just relying on metric number alone is not sufficient and you must try multiple evaluation\n",
    "methods coupled with feedback from data scientists as well as domain experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this tutorial, we've learned essential skills in model building and evaluation, including:\n",
    "\n",
    "- How to properly evaluate classification models using multiple metrics\n",
    "- Understanding the differences between internal and external clustering validation\n",
    "- Practical application of evaluation metrics on real biomedical data\n",
    "- Interpreting model performance through various visualization techniques\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NIGMS Sandbox UD submodule_4)",
   "language": "python",
   "name": "nigms_sandbox_ud__submodule_4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
