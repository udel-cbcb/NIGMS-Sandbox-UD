{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation\n",
    "\n",
    "Adapted from Wafiq Syed 2020 [How to use Scikit-Learn Datasets for Machine Learning](https://towardsdatascience.com/how-to-use-scikit-learn-datasets-for-machine-learning-d6493b38eca3) and Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers core concepts in model building and evaluation, focusing on classification and clustering approaches using the Breast Cancer Wisconsin dataset. We'll explore various algorithms and metrics for evaluating their performance.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn to build and evaluate classification models using metrics like accuracy, precision, recall, and F1-score\n",
    "- Understand how to interpret confusion matrices and ROC curves\n",
    "- Apply clustering model evaluation using both external and internal validation methods\n",
    "- Apply practical model evaluation techniques using scikit-learn's metrics module\n",
    "- Gain hands-on experience with real biomedical data analysis\n",
    "\n",
    "### Tasks to complete\n",
    "\n",
    "- Build classification models\n",
    "- Evaluate model performance using various metrics\n",
    "- Create and analyze clustering models\n",
    "- Generate performance visualizations\n",
    "- Compare different evaluation approaches\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A working Python environment and familiarity with Python\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with numpy and sklearn libraries\n",
    "- Knowledge of basic statistical concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "- Please select kernel \"conda_tensorflow2_p310\" from SageMaker notebook instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the warnings module to handle and control warning messages during code execution.\n",
    "import warnings\n",
    "\n",
    "# Import the pyplot module from matplotlib for creating plots and visualizations.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the numpy library for numerical computations, especially for handling arrays and matrices.\n",
    "import numpy as np\n",
    "\n",
    "# Import the pandas library for data manipulation and analysis, particularly using DataFrames.\n",
    "import pandas as pd\n",
    "\n",
    "# Import the metrics module from sklearn (scikit-learn) for evaluating model performance. # TODO this should probably be removed - as it's imported again later\n",
    "import sklearn.metrics  # TODO this should probably be removed - as it's imported again later\n",
    "\n",
    "# Import the interp function from numpy for interpolation, likely used in ROC curve calculations.\n",
    "from numpy import interp\n",
    "\n",
    "# Import dendrogram, fcluster, and linkage functions from scipy.cluster.hierarchy for hierarchical clustering and dendrogram plotting.\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster, linkage\n",
    "\n",
    "# Import datasets, linear_model, and metrics modules from sklearn for various machine learning tasks like loading datasets, linear models, and evaluation metrics.\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "\n",
    "# Import the clone function from sklearn.base for creating copies of estimators.\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Import the KMeans class from sklearn.cluster for K-Means clustering algorithm.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Import the load_breast_cancer function from sklearn.datasets to load the breast cancer dataset.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Import the PCA class from sklearn.decomposition for Principal Component Analysis (dimensionality reduction).\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Import auc and roc_curve functions from sklearn.metrics for calculating Area Under the ROC Curve and plotting ROC curves.\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "# Import the train_test_split function from sklearn.model_selection to split datasets into training and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import the KNeighborsClassifier class from sklearn.neighbors for K-Nearest Neighbors classification algorithm.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import LabelEncoder and label_binarize from sklearn.preprocessing for encoding categorical labels and binarizing labels in a one-vs-all fashion.\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "\n",
    "# Sets the backend of matplotlib to the 'inline' backend: With this backend,\n",
    "# the output of plotting commands is displayed inline within frontends like\n",
    "# the Jupyter notebook, directly below the code cell that produced it.\n",
    "# The resulting plots will then also be stored in the notebook document.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Example\n",
    "\n",
    "In this example, we’ll be working with the “Breast Cancer Wisconsin” dataset. We will import the data and understand how to read it. We will also build a simple ML model that is able to classify cancer scans either as malignant or benign.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import “Breast Cancer Wisconsin” dataaset\n",
    "\n",
    "The dataset can be found in _sklearn.datasets_. Each dataset has a corresponding function used to load the dataset. These functions follow the same format: \"load_DATASET()\", where DATASET refers to the name of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the breast cancer dataset from scikit-learn's datasets module.\n",
    "bc = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These load functions (such as _load_breast_cancer()_) don't return data in the tabular format, they return a **Bunch** object, a Scikit-Learn's fancy name for a Dictionary.\n",
    "\n",
    "Let's looking into its keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the keys of the 'bc' dictionary.\n",
    "print(bc.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the following keys:\n",
    "\n",
    "- **data** is all the feature data (the attributes of the scan that help us identify if the tumor is malignant or benign, such as radius, area, etc.) in a NumPy array\n",
    "- **target** is the target data (the variable you want to predict, in this case whether the tumor is malignant or benign) in a NumPy array,\n",
    "- **feature_names** are the names of the feature variables, in other words names of the columns in data\n",
    "- **target_names** is the name(s) of the target variable(s), in other words name(s) of the target column(s)\n",
    "- **DESCR**, short for DESCRIPTION, is a description of the dataset\n",
    "- **filename** is the path to the actual file of the data in CSV format.\n",
    "- **data_module** is the name of the data module from where the data is being loaded.\n",
    "\n",
    "It’s important to note that all of Scikit-Learn datasets are divided into data and target. data represents the features, which are the variables that help the model learn how to predict. target includes the actual labels. In our case, the target data is one column classifies the tumor as either 0 indicating malignant or 1 for benign.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look the description of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the description of the dataset object 'bc' to the console.\n",
    "print(bc.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Dataset\n",
    "\n",
    "We can use _pandas_ to explore the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the DataFrame using pandas, initializing it with breast cancer feature data and column names from bc.feature_names\n",
    "df = pd.DataFrame(bc.data, columns=bc.feature_names)\n",
    "\n",
    "# Add a new column named \"target\" to the DataFrame\n",
    "df[\"target\"] = bc.target\n",
    "\n",
    "# Display the first five rows of the DataFrame to show the initial data structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the value of this dataset, run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays a concise summary of the DataFrame 'df', including column names, data types, non-null values, and memory usage.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to observe:\n",
    "\n",
    "- There aren’t any missing values, all the columns have 569 values. This saves us time from having to account for missing values.\n",
    "- All the data types are numerical. This is important because Scikit-Learn models do not accept categorical variables. In the real world, when we get categorical variables, we transform them into numerical variables. Scikit-Learn’s datasets are free of categorical variables.\n",
    "\n",
    "Hence, Scikit-Learn takes care of the data cleansing work. Their datasets are extremely valuable. You will benefit from learning ML by using them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do some AI\n",
    "\n",
    "Let’s build a model that classifies cancer tumors as malignant (spreading) or benign (non-spreading). This will show you how to use the data for your own models. We’ll build a simple K-Nearest Neighbors model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s split the dataset into two, one for training the model — giving it data to learn from, and the second for testing the model — seeing how well the model performs on data (scans) it hasn’t seen before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the feature data from the 'bc' dataset into the variable 'X'.\n",
    "X = bc.data\n",
    "\n",
    "# Store the target data (labels) from the 'bc' dataset into the variable 'y'.\n",
    "y = bc.target\n",
    "\n",
    "# Import the 'train_test_split' function from Scikit-Learn's model_selection module.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the feature data (X) and target data (y) into training and testing sets using the train_test_split function.\n",
    "# X_train: Feature data for the training set.\n",
    "# X_test: Feature data for the testing set.\n",
    "# y_train: Target data (labels) for the training set.\n",
    "# y_test: Target data (labels) for the testing set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us two datasets —one for training and one for testing. Let’s get onto training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier implementing the k-nearest neighbors vote. Initializes a KNeighborsClassifier object with 6 neighbors.\n",
    "logreg = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the k-nearest neighbors classifier from the training dataset. Trains the KNeighborsClassifier model using the training features (X_train) and training labels (y_train).\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Return the mean accuracy on the given test data and labels. Calculates and prints the accuracy score of the trained KNeighborsClassifier model on the test features (X_test) and test labels (y_test).\n",
    "print(\"Model accurcy: \", logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n_neighbors parameter in KNN determines the number of neighboring data points considered when making a classification decision. Choosing this value involves a trade-off:\n",
    "\n",
    "- **Smaller values**: Result in more intricate decision boundaries that closely fit the training data but may lead to overfitting and increased sensitivity to noise.\n",
    "- **Larger values**: Produce smoother decision boundaries that enhance generalization but risk overlooking important patterns in the data.\n",
    "\n",
    "Selecting an appropriate n_neighbors value requires balancing these factors to capture local patterns without excessive noise influence. This choice should be guided by the dataset's characteristics (e.g., larger datasets or datasets with more noise often benefit from larger values of `n_neighbors`) and your domain knowledge. Additionally, cross-validation is commonly used to test different values and identify the optimal setting for a given dataset.\n",
    "\n",
    "For the Breast Cancer Wisconsin dataset, a value of 6 was chosen based on prior experimentation showing good performance with this dataset, the relatively small size of the dataset, and the desire to balance capturing local patterns while still avoiding noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Example\n",
    "\n",
    "In this example, we will learn how we can fit a clustering model on “Breast Cancer Wisconsin” dataset. We will use a labeled dataset to help us see the results of the clustering model and compare it with actual labels. A point to remember here is that, usually labeled data is not available in the real world,\n",
    "which is why we choose to go for unsupervised methods like clustering. We will try to cover two different\n",
    "algorithms, one each from partitioning based clustering and hierarchical clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wisconsin Breast Cancer Dataset\n",
    "\n",
    "# Import the load_breast_cancer function from the sklearn.datasets module to load the dataset.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# load data\n",
    "# Load the Wisconsin Breast Cancer dataset into the variable 'bc'.\n",
    "bc = load_breast_cancer()\n",
    "\n",
    "# Store the feature data\n",
    "# Assign the feature data from the loaded dataset 'bc' to the variable 'X'.\n",
    "X = bc.data\n",
    "\n",
    "# store the target data\n",
    "# Assign the target data (labels) from the loaded dataset 'bc' to the variable 'y'.\n",
    "y = bc.target\n",
    "# Print the shape of the feature data 'X' and the names of the features from 'bc.feature_names'.\n",
    "print(X.shape, bc.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that we have a total of 569 observations and 30 attributes or features for each observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition based Clustering Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose the simplest yet most popular partition based clustering model for our example, which\n",
    "is **K-means** algorithm. This algorithm is a centroid based clustering algorithm, which starts with some\n",
    "assumption about the total clusters in the data and with random centers assigned to each of the clusters.\n",
    "It then reassigns each data point to the center closest to it, using Euclidean distance as the distance metric.\n",
    "After each reassignment, it recalculates the center of that cluster. The whole process is repeated iteratively\n",
    "and stopped when reassignment of data points doesn’t change the cluster centers. Variants include\n",
    "algorithms like **K-medoids**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans class from scikit-learn library for K-means clustering algorithm.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans clustering object, specifying the number of clusters and random state.\n",
    "km = KMeans(n_clusters=2, random_state=2)\n",
    "# Fit the K-means model to the data X, which performs the clustering.\n",
    "km.fit(X)\n",
    "\n",
    "# Get the cluster labels assigned to each data point by the fitted K-means model.\n",
    "labels = km.labels_\n",
    "# Get the coordinates of the cluster centers calculated by K-means.\n",
    "centers = km.cluster_centers_\n",
    "\n",
    "# Print the cluster labels for the first 10 data points to see cluster assignments.\n",
    "print(labels[:10])\n",
    "\n",
    "# Print the cluster centers, representing the mean feature vector for each cluster.\n",
    "# These centers are numerical values in the feature space (30 dimensions in this case) and indicate the central point of each cluster.\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will leverage PCA to reduce the input dimensions (30) to two principal components\n",
    "# and visualize the clusters on top of the same.\n",
    "\n",
    "# Instantiate PCA object from scikit-learn, specifying that we want to reduce the data to 2 principal components.\n",
    "pca = PCA(n_components=2)\n",
    "# Fit PCA to the data X and then transform X to its first 2 principal components, storing the result in bc_pca.\n",
    "bc_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters on the reduced 2D feature space for the actual labels as well as the clustered output labels.\n",
    "# Create a figure and a set of subplots (1 row, 2 columns) for visualization, setting the figure size to 8x4 inches.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "# Set the suptitle for the entire figure to \"Visualizing breast cancer clusters\".\n",
    "fig.suptitle(\"Visualizing breast cancer clusters\")\n",
    "# Adjust the spacing between subplots and the top of the figure to prevent overlap with the suptitle.\n",
    "fig.subplots_adjust(top=0.85, wspace=0.5)\n",
    "# Set the title for the first subplot (ax1) to \"Actual Labels\", representing the ground truth labels.\n",
    "ax1.set_title(\"Actual Labels\")\n",
    "# Set the title for the second subplot (ax2) to \"Clustered Labels\", representing the labels from the clustering algorithm.\n",
    "ax2.set_title(\"Clustered Labels\")\n",
    "\n",
    "# Iterate through each data point in the dataset using its index 'i'.\n",
    "for i in range(len(y)):\n",
    "    # Check if the actual label 'y[i]' for the i-th data point is 0.\n",
    "    if y[i] == 0:\n",
    "        # If actual label is 0, scatter plot the i-th data point on the first subplot (ax1)\n",
    "        # using the first two PCA components (bc_pca[i, 0], bc_pca[i, 1]), color it green ('g'), and use a dot marker ('.').\n",
    "        c1 = ax1.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"g\", marker=\".\")\n",
    "    # Check if the actual label 'y[i]' for the i-th data point is 1.\n",
    "    if y[i] == 1:\n",
    "        # If actual label is 1, scatter plot the i-th data point on the first subplot (ax1)\n",
    "        # using the first two PCA components, color it red ('r'), and use a dot marker ('.').\n",
    "        c2 = ax1.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"r\", marker=\".\")\n",
    "\n",
    "    # Check if the clustered label 'labels[i]' for the i-th data point is 0.\n",
    "    if labels[i] == 0:\n",
    "        # If clustered label is 0, scatter plot the i-th data point on the second subplot (ax2)\n",
    "        # using the first two PCA components, color it green ('g'), and use a dot marker ('.').\n",
    "        c3 = ax2.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"g\", marker=\".\")\n",
    "    # Check if the clustered label 'labels[i]' for the i-th data point is 1.\n",
    "    if labels[i] == 1:\n",
    "        # If clustered label is 1, scatter plot the i-th data point on the second subplot (ax2)\n",
    "        # using the first two PCA components, color it red ('r'), and use a dot marker ('.').\n",
    "        c4 = ax2.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"r\", marker=\".\")\n",
    "\n",
    "# Create a legend 'l1' for the first subplot (ax1) using the scatter plot handles 'c1' and 'c2', and label the classes as \"0\" and \"1\".\n",
    "l1 = ax1.legend([c1, c2], [\"0\", \"1\"])\n",
    "# Create a legend 'l2' for the second subplot (ax2) using the scatter plot handles 'c3' and 'c4', and label the clusters as \"0\" and \"1\".\n",
    "l2 = ax2.legend([c3, c4], [\"0\", \"1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the clustering has worked quite well and it shows distinct\n",
    "separation between clusters with labels 0 and 1 and is quite similar to the actual labels. However we do\n",
    "have some overlap where we have mislabeled some instances.\n",
    "\n",
    "Remember in an actual real-world scenario, you will not have the actual labels to compare with and the\n",
    "main idea is to find structures or patterns in your data in the form of these clusters.\n",
    "Hence even when dealing with labeled data and running clustering do not\n",
    "compare clustered label values with actual labels and try to measure accuracy.\n",
    "\n",
    "Another very important\n",
    "point to remember is that cluster label values have no significance. The labels 0 and 1 are just values to\n",
    "distinguish cluster data points from each other.\n",
    "\n",
    "Also another important note\n",
    "is that if we had asked for more than two clusters, the algorithm would have readily supplied more clusters\n",
    "but it would have been hard to interpret those and many of them would not make sense. Hence, one of\n",
    "the caveats of using the K-means algorithm is to use it in the case where we have some idea about the total\n",
    "number of clusters that may exist in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering Example\n",
    "\n",
    "We can use the same data to perform a hierarchical clustering and see if the results change much as\n",
    "compared to K-means clustering and the actual labels.\n",
    "\n",
    "agglomerative clustering is hierarchical clustering using a\n",
    "bottom up approach i.e. each observation starts in its own cluster and clusters are successively merged\n",
    "together. The merging criteria can be used from a candidate set of linkages; the selection of linkage governs\n",
    "the merge strategy. Some examples of linkage criteria are Ward, Complete linkage, Average linkage and so\n",
    "on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the linkage matrix using Ward’s minimum variance criterion.\n",
    "# Perform hierarchical/agglomerative clustering.\n",
    "# Returns hierarchical clustering encoded as a linkage matrix.\n",
    "# Apply hierarchical clustering to the data 'X' using Ward's minimum variance method.\n",
    "Z = linkage(X, \"ward\")\n",
    "# Print the resulting linkage matrix 'Z' to the console.\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dendrogram to visualize the hierarchical clustering distance-based merges.\n",
    "# Create a new figure for the dendrogram plot with a specified size (width=8 inches, height=3 inches).\n",
    "plt.figure(figsize=(8, 3))\n",
    "# Set the title of the dendrogram plot.\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "# Set the label for the x-axis, representing the data points.\n",
    "plt.xlabel(\"Data point\")\n",
    "# Set the label for the y-axis, representing the distance between clusters.\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "# Plot the hierarchical clustering as a dendrogram using the linkage matrix 'Z'.\n",
    "dendrogram(Z)\n",
    "\n",
    "# Add a horizontal line to the dendrogram to help in visually determining a cutoff for cluster formation.\n",
    "# y: position of the horizontal line on the y-axis (distance), set to 10000 in data coordinates.\n",
    "# c: color of the line, set to 'k' for black.\n",
    "# ls: line style, set to '--' for dashed line.\n",
    "# lw: line weight, set to 0.5 for a thin line.\n",
    "plt.axhline(y=10000, c=\"k\", ls=\"--\", lw=0.5)\n",
    "# Display the dendrogram plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster labels. This line is a general comment indicating the purpose of the following code block, which is to obtain cluster labels from hierarchical clustering.\n",
    "max_dist = 10000  # Define a threshold distance 'max_dist' and set it to 10000. This value will be used as the maximum distance to form clusters.\n",
    "\n",
    "# Form flat clusters from the hierarchical clustering defined by\n",
    "# the given linkage matrix. This multi-line comment explains that the following line uses the 'fcluster' function to generate flat clusters from a pre-computed hierarchical clustering.\n",
    "#   - Z: This parameter is expected to be the linkage matrix, which is the output of hierarchical clustering algorithms like 'linkage' in scipy. It encodes the hierarchical relationships between data points.\n",
    "#   - max_dist: This parameter specifies the threshold distance for forming clusters. Clusters will be formed such that the maximum cophenetic distance between any two original observations in the same cluster is no more than 'max_dist'.\n",
    "#   - criterion=\"distance\": This parameter defines the criterion to use for forming flat clusters. Here, \"distance\" means that clusters are formed based on the 'max_dist' distance threshold.\n",
    "# The resulting flat cluster labels are assigned to the variable 'hc_labels'. Each data point will be assigned a cluster label based on the distance criterion.\n",
    "hc_labels = fcluster(\n",
    "    Z, max_dist, criterion=\"distance\"\n",
    ")  # Use the fcluster function to derive flat clusters from the hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block aims to visualize and compare cluster outputs based on PCA-reduced dimensions against the original label distribution of a breast cancer dataset.\n",
    "# Create a figure and a set of subplots (1 row, 2 columns) for visualization, setting the figure size to 8x4 inches.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "# Set the main title for the entire figure, which encompasses both subplots.\n",
    "fig.suptitle(\"Visualizing breast cancer clusters\")\n",
    "# Adjust subplot parameters to provide more space at the top for the suptitle and adjust horizontal spacing between subplots.\n",
    "fig.subplots_adjust(top=0.85, wspace=0.5)\n",
    "# Set the title for the first subplot (ax1) to \"Actual Labels\", representing the ground truth.\n",
    "ax1.set_title(\"Actual Labels\")\n",
    "# Set the title for the second subplot (ax2) to \"Hierarchical Clustered Labels\", showing the clusters found by hierarchical clustering.\n",
    "ax2.set_title(\"Hierarchical Clustered Labels\")\n",
    "\n",
    "# Iterate through each data point in the dataset using an index 'i' ranging from 0 to the length of 'y' (actual labels).\n",
    "for i in range(len(y)):\n",
    "    # Check if the actual label 'y[i]' for the i-th data point is 0.\n",
    "    if y[i] == 0:\n",
    "        # If the label is 0, create a scatter plot point on the first subplot (ax1) using the PCA-reduced data 'bc_pca[i, 0]' (first principal component) and 'bc_pca[i, 1]' (second principal component).\n",
    "        # Set the color of the point to green ('g') and the marker style to a dot ('.'). Assign the scatter plot object to 'c1'.\n",
    "        c1 = ax1.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"g\", marker=\".\")\n",
    "    # Check if the actual label 'y[i]' for the i-th data point is 1.\n",
    "    if y[i] == 1:\n",
    "        # If the label is 1, create a scatter plot point on the first subplot (ax1) using the PCA-reduced data 'bc_pca[i, 0]' and 'bc_pca[i, 1]'.\n",
    "        # Set the color of the point to red ('r') and the marker style to a dot ('.'). Assign the scatter plot object to 'c2'.\n",
    "        c2 = ax1.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"r\", marker=\".\")\n",
    "\n",
    "    # Check if the hierarchical clustering label 'hc_labels[i]' for the i-th data point is 1.\n",
    "    if hc_labels[i] == 1:\n",
    "        # If the cluster label is 1, create a scatter plot point on the second subplot (ax2) using the PCA-reduced data 'bc_pca[i, 0]' and 'bc_pca[i, 1]'.\n",
    "        # Set the color of the point to green ('g') and the marker style to a dot ('.'). Assign the scatter plot object to 'c3'.\n",
    "        c3 = ax2.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"g\", marker=\".\")\n",
    "    # Check if the hierarchical clustering label 'hc_labels[i]' for the i-th data point is 2.\n",
    "    if hc_labels[i] == 2:\n",
    "        # If the cluster label is 2, create a scatter plot point on the second subplot (ax2) using the PCA-reduced data 'bc_pca[i, 0]' and 'bc_pca[i, 1]'.\n",
    "        # Set the color of the point to red ('r') and the marker style to a dot ('.'). Assign the scatter plot object to 'c4'.\n",
    "        c4 = ax2.scatter(bc_pca[i, 0], bc_pca[i, 1], c=\"r\", marker=\".\")\n",
    "\n",
    "# Create a legend for the first subplot (ax1) using the scatter plot objects 'c1' and 'c2', labeling them as \"0\" and \"1\" respectively, and assign the legend object to 'l1'.\n",
    "l1 = ax1.legend([c1, c2], [\"0\", \"1\"])\n",
    "# Create a legend for the second subplot (ax2) using the scatter plot objects 'c3' and 'c4', labeling them as \"1\" and \"2\" respectively, and assign the legend object to 'l2'.\n",
    "l2 = ax2.legend([c3, c4], [\"1\", \"2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We definitely see two distinct clusters but there is more overlap as compared to the K-means method\n",
    "between the two clusters and we have more mislabeled instances. However, do take a note of the label\n",
    "numbers; here we have 1 and 2 as the label values. This is just to reinforce the fact that the label values are\n",
    "just to distinguish the clusters and don’t mean anything. The advantage of this method is that you do not\n",
    "need to input the number of clusters beforehand and the model tries to find it from the underlying data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first prepare train and test datasets to build our classification models.\n",
    "# Split the feature matrix X and target vector y into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # X is the feature matrix containing the independent variables.\n",
    "    X,\n",
    "    # y is the target vector containing the dependent variable (labels).\n",
    "    y,\n",
    "    # test_size=0.3 specifies that 30% of the data will be used for testing, and 70% for training.\n",
    "    test_size=0.3,\n",
    "    # random_state=42 ensures that the data split is reproducible. Using the same random_state will result in the same split each time the code is run.\n",
    "    random_state=42,\n",
    ")\n",
    "# Print the shapes of the training and testing feature matrices to verify the split.\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the warnings module to handle warning messages.\n",
    "import warnings\n",
    "\n",
    "# Filter all warnings to be ignored to suppress them from output.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize a Logistic Regression model from scikit-learn's linear_model module.\n",
    "logistic = linear_model.LogisticRegression()\n",
    "# Train the Logistic Regression model using the training data (X_train features and y_train labels).\n",
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display a formatted confusion matrix.\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1, 0]):\n",
    "    # Determine the total number of classes from the input 'classes' list.\n",
    "    total_classes = len(classes)\n",
    "    # Define levels and codes for creating a MultiIndex for pandas DataFrame columns and rows.\n",
    "    level_labels = [total_classes * [0], list(range(total_classes))]\n",
    "    # Calculate the confusion matrix using scikit-learn's metrics.confusion_matrix function.\n",
    "    cm = metrics.confusion_matrix(\n",
    "        # Pass the true labels to the confusion_matrix function.\n",
    "        y_true=true_labels,\n",
    "        # Pass the predicted labels to the confusion_matrix function.\n",
    "        y_pred=predicted_labels,\n",
    "        # Specify the classes to be considered in the confusion matrix.\n",
    "        labels=classes,\n",
    "    )\n",
    "    # Create a pandas DataFrame to present the confusion matrix in a structured format.\n",
    "    cm_frame = pd.DataFrame(\n",
    "        # Use the calculated confusion matrix 'cm' as the data for the DataFrame.\n",
    "        data=cm,\n",
    "        # Define the columns of the DataFrame using pandas MultiIndex for hierarchical column labels ('Predicted:' and class names).\n",
    "        columns=pd.MultiIndex(levels=[[\"Predicted:\"], classes], codes=level_labels),\n",
    "        # Define the index (rows) of the DataFrame using pandas MultiIndex for hierarchical row labels ('Actual:' and class names).\n",
    "        index=pd.MultiIndex(levels=[[\"Actual:\"], classes], codes=level_labels),\n",
    "    )\n",
    "    # Print the formatted confusion matrix DataFrame.\n",
    "    print(cm_frame)\n",
    "\n",
    "\n",
    "# Use the trained logistic regression model to predict class labels for the test features (X_test).\n",
    "y_pred = logistic.predict(X_test)\n",
    "# Display the confusion matrix to evaluate the model's performance.\n",
    "# Pass the true labels (y_test), predicted labels (y_pred), and class labels [0, 1] to the function.\n",
    "display_confusion_matrix(true_labels=y_test, predicted_labels=y_pred, classes=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that out of\n",
    "60 observations with label 0 (malignant), our model has correctly predicted 59 observations. Similarly out of\n",
    "111 observations with label 1 (benign), our model has correctly predicted 107 observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive, False Positive, True Negative and False Negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set positive class label\n",
    "positive_class = 1\n",
    "\n",
    "# True Positive (TP): This is the count of the total number of instances from the\n",
    "# positive class where the true class label was equal to the predicted class label.\n",
    "TP = 107\n",
    "\n",
    "# False Positive (FP): This is the count of the total number of instances from the\n",
    "# negative class where our model misclassified them by predicting them as positive.\n",
    "FP = 4\n",
    "\n",
    "# True Negative (FN): This is the count of the total number of instances from the\n",
    "# negative class where the true class label was equal to the predicted class label.\n",
    "TN = 59\n",
    "\n",
    "# False Negative (FN): This is the count of the total number of instances from the\n",
    "# positive class where our model misclassified them by predicting them as negative.\n",
    "FN = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "This is one of the most popular measures of classifier performance. It is defined as the overall\n",
    "accuracy or proportion of correct predictions of the model. The formula for computing accuracy from the\n",
    "confusion matrix is:\n",
    "\n",
    "$Accurcy=\\frac{TP+TN}{TP+FP+TN+FN}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the framework accuracy using scikit-learn's accuracy_score function and round to 5 decimal places.\n",
    "fw_acc = round(metrics.accuracy_score(y_true=y_test, y_pred=y_pred), 5)\n",
    "# Manually compute accuracy using the confusion matrix components (TP, TN, FP, FN) and round to 5 decimal places.\n",
    "mc_acc = round((TP + TN) / (TP + TN + FP + FN), 5)\n",
    "\n",
    "# Print the framework-calculated accuracy.\n",
    "print(\"Framework Accuracy:\", fw_acc)\n",
    "# Print the manually computed accuracy.\n",
    "print(\"Manually Computed Accuracy:\", mc_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision, also known as positive predictive value, is another metric that can be derived from\n",
    "the confusion matrix. It is defined as the number of predictions made that are actually correct or relevant out\n",
    "of all the predictions based on the positive class. The formula for precision is as follows:\n",
    "\n",
    "$Precision=\\frac{TP}{TP+FP}$\n",
    "\n",
    "A model with high precision will identify a higher fraction of positive class as compared to a model\n",
    "with a lower precision. Precision becomes important in cases where we are more concerned about finding\n",
    "the maximum number of positive class even if the total accuracy reduces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates precision using scikit-learn's precision_score function and rounds it to 5 decimal places.\n",
    "fw_prec = round(metrics.precision_score(y_true=y_test, y_pred=y_pred), 5)\n",
    "# Manually computes precision using the formula: True Positives (TP) / (True Positives (TP) + False Positives (FP)), and rounds it to 5 decimal places.\n",
    "mc_prec = round((TP) / (TP + FP), 5)\n",
    "\n",
    "# Prints the precision calculated using scikit-learn's function.\n",
    "print(\"Framework Precision:\", fw_prec)\n",
    "# Prints the precision calculated manually.\n",
    "print(\"Manually Computed Precision:\", mc_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall, also known as sensitivity, is a measure of a model to identify the percentage of relevant\n",
    "data points. It is defined as the number of instances of the positive class that were correctly predicted. This is\n",
    "also known as hit rate, coverage, or sensitivity. The formula for recall is:\n",
    "\n",
    "$Recall=\\frac{TP}{TP+FN}$\n",
    "\n",
    "Recall becomes an important measure of classifier performance in scenarios where we want to catch\n",
    "the most number of instances of a particular class even when it increases our false positives. For example,\n",
    "consider the case of bank fraud, a model with high recall will give us higher number of potential fraud cases.\n",
    "But it will also help us raise alarm for most of the suspicious cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the recall score using scikit-learn's metrics.recall_score function, rounding to 5 decimal places.\n",
    "fw_rec = round(metrics.recall_score(y_true=y_test, y_pred=y_pred), 5)\n",
    "# Manually compute recall using the formula: True Positives (TP) / (True Positives (TP) + False Negatives (FN)), rounding to 5 decimal places.\n",
    "mc_rec = round((TP) / (TP + FN), 5)\n",
    "\n",
    "# Print the recall score calculated using scikit-learn's framework.\n",
    "print(\"Framework Recall:\", fw_rec)\n",
    "# Print the recall score manually computed.\n",
    "print(\"Manually Computed Recall:\", mc_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score\n",
    "\n",
    "There are some cases in which we want a balanced optimization of both precision and recall.\n",
    "F1 score is a metric that is the harmonic mean of precision and recall and helps us optimize a classifier for\n",
    "balanced precision and recall performance.\n",
    "The formula for the F1 score is:\n",
    "\n",
    "$F1 Score = \\frac{2 x Precision x Recall}{Precision + Recall}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the F1-score using scikit-learn's metrics.f1_score function, rounding to 5 decimal places.\n",
    "fw_f1 = round(metrics.f1_score(y_true=y_test, y_pred=y_pred), 5)\n",
    "# Manually computes the F1-score using the formula: 2 * (precision * recall) / (precision + recall), rounding to 5 decimal places.\n",
    "mc_f1 = round((2 * mc_prec * mc_rec) / (mc_prec + mc_rec), 5)\n",
    "\n",
    "# Prints the F1-score calculated using scikit-learn's framework.\n",
    "print(\"Framework F1-Score:\", fw_f1)\n",
    "# Prints the F1-score computed manually.\n",
    "print(\"Manually Computed F1-Score:\", mc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "The ROC curve can be created by plotting the fraction of true positives versus the fraction of false\n",
    "positives, i.e. it is a plot of True Positive Rate (TPR) versus the False Positive Rate (FPR). It is applicable\n",
    "mostly for scoring classifiers. Scoring classifiers are the type of classifiers which will return a probability\n",
    "value or score for each class label, from which a class label can be deduced (based on maximum probability\n",
    "value).\n",
    "\n",
    "This curve can be plotted using the true positive rate (TPR) and the false positive rate (FPR) of a\n",
    "classifier. TPR is known as sensitivity or recall, which is the total number of correct positive results, predicted\n",
    "among all the positive samples the dataset. FPR is known as false alarms or (1 - specificity), determining the\n",
    "total number of incorrect positive predictions among all negative samples in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_labels(clf, label_encoder=None, class_names=None):\n",
    "    \"\"\"\n",
    "    Retrieves class labels from a classifier, label encoder, or directly from\n",
    "    provided class names.\n",
    "\n",
    "    Args:\n",
    "        clf: A trained classifier object.\n",
    "        label_encoder: Optional. A fitted LabelEncoder object.\n",
    "        class_names: Optional. A list of class name strings.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of class labels.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If class labels cannot be determined from any of the inputs.\n",
    "    \"\"\"\n",
    "    # Check if the classifier object has class labels defined\n",
    "    if hasattr(clf, \"classes_\"):\n",
    "        # If yes, get class labels from the classifier\n",
    "        class_labels = clf.classes_\n",
    "    # Else if a label encoder is provided\n",
    "    elif label_encoder:\n",
    "        # If yes, get class labels from the label encoder\n",
    "        class_labels = label_encoder.classes_\n",
    "    # Else if class names are directly provided\n",
    "    elif class_names:\n",
    "        # If yes, use the provided class names\n",
    "        class_labels = class_names\n",
    "    # Else if no class labels can be derived\n",
    "    else:\n",
    "        # Raise a ValueError indicating inability to determine prediction classes\n",
    "        raise ValueError(\n",
    "            \"Unable to derive prediction classes, please specify class_names!\"\n",
    "        )\n",
    "    return class_labels\n",
    "\n",
    "\n",
    "def get_prediction_scores(clf, features):\n",
    "    \"\"\"\n",
    "    Gets prediction scores (probabilities or decision function values) from a\n",
    "    classifier.\n",
    "\n",
    "    Args:\n",
    "        clf: A trained classifier object.\n",
    "        features: Feature matrix to get predictions for.\n",
    "\n",
    "    Returns:\n",
    "        An array of prediction scores or probabilities.\n",
    "\n",
    "    Raises:\n",
    "        AttributeError: If the classifier doesn't have predict_proba or decision_function methods.\n",
    "    \"\"\"\n",
    "    # Check if the classifier has a predict_proba method (for probability estimates)\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        # Get probability predictions from the classifier\n",
    "        return clf.predict_proba(features)\n",
    "    # Else if the classifier has a decision_function method (for decision values)\n",
    "    elif hasattr(clf, \"decision_function\"):\n",
    "        # Get decision function values from the classifier\n",
    "        return clf.decision_function(features)\n",
    "    # Else if the classifier has neither predict_proba nor decision_function\n",
    "    else:\n",
    "        # Raise an AttributeError indicating the estimator lacks probability or confidence scoring\n",
    "        raise AttributeError(\n",
    "            \"Estimator doesn't have a probability or confidence scoring system!\"\n",
    "        )\n",
    "\n",
    "\n",
    "def calculate_binary_roc_data(y_test, y_score):\n",
    "    \"\"\"\n",
    "    Calculates ROC curve data for binary classification.\n",
    "\n",
    "    Args:\n",
    "        y_test: True binary labels.\n",
    "        y_score: Target scores (probabilities or decision function outputs).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (fpr, tpr, roc_auc) where:\n",
    "            - fpr: array of false positive rates\n",
    "            - tpr: array of true positive rates\n",
    "            - roc_auc: area under the ROC curve\n",
    "    \"\"\"\n",
    "    # Compute False Positive Rate, True Positive Rate, and thresholds for ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    # Compute Area Under the ROC Curve (ROC AUC)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "\n",
    "def plot_binary_roc_curve(fpr, tpr, roc_auc):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve for binary classification.\n",
    "\n",
    "    Args:\n",
    "        fpr: Array of false positive rates.\n",
    "        tpr: Array of true positive rates.\n",
    "        roc_auc: Area under the ROC curve.\n",
    "    \"\"\"\n",
    "    # Create a new figure for plotting ROC curves\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    # Plot the ROC curve for binary classification\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        label=\"ROC curve (area = {0:0.2f})\".format(roc_auc),\n",
    "        linewidth=2.5,\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_multiclass_roc_data(y_test, y_score, n_classes):\n",
    "    \"\"\"\n",
    "    Calculates ROC curve data for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        y_test: Binarized true labels.\n",
    "        y_score: Target scores (probabilities or decision function outputs).\n",
    "        n_classes: Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (fpr, tpr, roc_auc) where:\n",
    "            - fpr: dictionary of false positive rates for each class, plus 'micro' and 'macro' averages\n",
    "            - tpr: dictionary of true positive rates for each class, plus 'micro' and 'macro' averages\n",
    "            - roc_auc: dictionary of ROC AUC values for each class, plus 'micro' and 'macro' averages\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionaries to store false positive rates, true positive\n",
    "    # rates, and ROC AUC values for each class.\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    # Iterate through each class to compute ROC curve and AUC\n",
    "    for i in range(n_classes):\n",
    "        # Compute FPR, TPR, and thresholds for each class's ROC curve\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        # Compute ROC AUC for each class\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # ## Compute micro-average ROC curve and ROC area\n",
    "    # Compute micro-average ROC curve (considering all classes together)\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    # Compute ROC AUC for micro-average ROC curve\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # ## Compute macro-average ROC curve and ROC area\n",
    "    # Initialize an array to store all false positive rates for macro-average calculation\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    # Initialize an array to store interpolated true positive rates\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    # Interpolate ROC curves at each point for macro-average\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    # Average true positive rates to get macro-average TPR\n",
    "    mean_tpr /= n_classes\n",
    "    # Assign macro-average FPR and TPR to dictionaries\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    # Compute ROC AUC for macro-average ROC curve\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "\n",
    "def plot_multiclass_roc_curves(fpr, tpr, roc_auc, class_labels):\n",
    "    \"\"\"\n",
    "    Plots ROC curves for multi-class classification, including micro and macro averages.\n",
    "\n",
    "    Args:\n",
    "        fpr: Dictionary of false positive rates for each class, plus 'micro' and 'macro' averages.\n",
    "        tpr: Dictionary of true positive rates for each class, plus 'micro' and 'macro' averages.\n",
    "        roc_auc: Dictionary of ROC AUC values for each class, plus 'micro' and 'macro' averages.\n",
    "        class_labels: List of class labels corresponding to the classes.\n",
    "    \"\"\"\n",
    "    # Create a new figure for plotting ROC curves\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Plot micro-average ROC curve\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "        linewidth=3,\n",
    "    )\n",
    "\n",
    "    # Plot macro-average ROC curve\n",
    "    plt.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "        linewidth=3,\n",
    "    )\n",
    "\n",
    "    # Plot ROC curve for each class\n",
    "    for i, label in enumerate(class_labels):\n",
    "        plt.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            label=\"ROC curve of class {0} (area = {1:0.2f})\".format(label, roc_auc[i]),\n",
    "            linewidth=2,\n",
    "            linestyle=\":\",\n",
    "        )\n",
    "\n",
    "\n",
    "def finalize_plot():\n",
    "    \"\"\"\n",
    "    Finalizes and displays the ROC curve plot with appropriate labels and formatting.\n",
    "    \"\"\"\n",
    "    # Plot the diagonal line representing chance level performance\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    # Set x-axis limits from 0 to 1\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    # Set y-axis limits from 0 to 1.05 for better visualization\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    # Set x-axis label\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    # Set y-axis label\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    # Set plot title\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    # Display legend to identify each ROC curve\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_roc_curve(\n",
    "    clf, features, true_labels, label_encoder=None, class_names=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots ROC curve(s) for a classifier's predictions.\n",
    "\n",
    "    Args:\n",
    "        clf: A trained classifier object.\n",
    "        features: Feature matrix to make predictions on.\n",
    "        true_labels: True labels corresponding to the features.\n",
    "        label_encoder: Optional. A fitted LabelEncoder object.\n",
    "        class_names: Optional. A list of class name strings.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the number of classes is less than 2.\n",
    "    \"\"\"\n",
    "    # Get class labels for the classifier\n",
    "    class_labels = get_class_labels(clf, label_encoder, class_names)\n",
    "\n",
    "    # Get the number of classes from the class labels\n",
    "    n_classes = len(class_labels)\n",
    "\n",
    "    # Binarize the true labels for ROC curve calculation, handling multi-class scenarios\n",
    "    y_test = label_binarize(true_labels, classes=class_labels)\n",
    "\n",
    "    # Check if it's a binary classification problem (2 classes)\n",
    "    if n_classes == 2:\n",
    "        # Get probability predictions from the classifier\n",
    "        prob = get_prediction_scores(clf, features)\n",
    "        # Extract probabilities for the positive class (assuming binary case, last column)\n",
    "        y_score = prob[:, prob.shape[1] - 1]\n",
    "        # Compute Area Under the ROC Curve (ROC AUC)\n",
    "        fpr, tpr, roc_auc = calculate_binary_roc_data(y_test, y_score)\n",
    "        plot_binary_roc_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "    # Else if it's a multi-class classification problem (more than 2 classes)\n",
    "    elif n_classes > 2:\n",
    "        # Get probability predictions from the classifier\n",
    "        y_score = get_prediction_scores(clf, features)\n",
    "        fpr, tpr, roc_auc = calculate_multiclass_roc_data(y_test, y_score, n_classes)\n",
    "        plot_multiclass_roc_curves(fpr, tpr, roc_auc, class_labels)\n",
    "\n",
    "    # Else if the number of classes is less than 2 (invalid case)\n",
    "    else:\n",
    "        # Raise a ValueError for insufficient number of classes for ROC curve plotting\n",
    "        raise ValueError(\"Number of classes should be atleast 2 or more\")\n",
    "\n",
    "    finalize_plot()\n",
    "\n",
    "\n",
    "# Example function call to plot ROC curve for a logistic regression classifier 'logistic'\n",
    "plot_model_roc_curve(clf=logistic, features=X_test, true_labels=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, the best prediction model would give a point\n",
    "on the top left corner (0, 1) indicating perfect classification (100% sensitivity & specificity). A diagonal line\n",
    "depicts a classifier that does a random guess. Ideally if your ROC curve occurs in the top half of the graph,\n",
    "you have a decent classifier which is better than average. The plot above shows a near perfect ROC curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Model Evaluation Metrics\n",
    "\n",
    "The lack of a validated ground truth, i.e. the absence of true labels in the data makes the evaluation of clustering (or unsupervised models in general) very difficult.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build two clustering models on the breast cancer dataset\n",
    "\n",
    "We will leverage the breast cancer\n",
    "dataset available in the variables X for the data and y for the observation labels. We will also use the K-means\n",
    "algorithm to fit two models on this data—one with two clusters and the second one with five clusters—and\n",
    "then evaluate their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans class from scikit-learn library\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans clustering with 2 clusters and a fixed random state for reproducibility, then fit it to the data X.\n",
    "km2 = KMeans(n_clusters=2, random_state=42).fit(X)\n",
    "# Get the cluster labels assigned by the KMeans model (km2) after fitting.\n",
    "km2_labels = km2.labels_\n",
    "\n",
    "# Initialize KMeans clustering with 5 clusters and a fixed random state for reproducibility, then fit it to the data X.\n",
    "km5 = KMeans(n_clusters=5, random_state=42).fit(X)\n",
    "# Get the cluster labels assigned by the KMeans model (km5) after fitting.\n",
    "km5_labels = km5.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External validation\n",
    "\n",
    "External validation means validating the clustering model when we have some ground truth available\n",
    "as labeled data. The presence of external labels reduces most of the complexity of model evaluation as\n",
    "the clustering (unsupervised) model can be validated in similar fashion to classification models.\n",
    "\n",
    "Three popular metrics can be used in this scenario:\n",
    "\n",
    "- **Homogeneity**: A clustering model prediction result satisfies homogeneity if all of\n",
    "  its clusters contain only data points that are members of a single class (based on the\n",
    "  true class labels).\n",
    "- **Completeness**: A clustering model prediction result satisfies completeness if\n",
    "  all the data points of a specific ground truth class label are also elements of the\n",
    "  same cluster.\n",
    "- **V-measure**: The harmonic mean of homogeneity and completeness scores gives us\n",
    "  the V-measure value.\n",
    "\n",
    "Values are typically bounded between 0 and 1 and usually higher values are better. Let’s compute these\n",
    "metric on our two K-means clustering models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the Homogeneity, Completeness, and V-measure metrics for KMeans clustering with 2 clusters and rounds the results to 3 decimal places.\n",
    "km2_hcv = np.round(metrics.homogeneity_completeness_v_measure(y, km2_labels), 3)\n",
    "# Calculates the Homogeneity, Completeness, and V-measure metrics for KMeans clustering with 5 clusters and rounds the results to 3 decimal places.\n",
    "km5_hcv = np.round(metrics.homogeneity_completeness_v_measure(y, km5_labels), 3)\n",
    "\n",
    "# Prints the Homogeneity, Completeness, and V-measure metrics for the KMeans clustering with 2 clusters.\n",
    "print(\"Homogeneity, Completeness, V-measure metrics for num clusters=2: \", km2_hcv)\n",
    "# Prints the Homogeneity, Completeness, and V-measure metrics for the KMeans clustering with 5 clusters.\n",
    "print(\"Homogeneity, Completeness, V-measure metrics for num clusters=5: \", km5_hcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the V-measure for the first model with two clusters is better than the one with five\n",
    "clusters and the reason is because of higher completeness score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal validation\n",
    "\n",
    "Internal validation means validating a clustering model by defining metrics that capture the expected\n",
    "behavior of a good clustering model. A good clustering model can be identified by two very desirable traits:\n",
    "\n",
    "- Compact groups, i.e. the data points in one cluster occur close to each other.\n",
    "- Well separated groups, i.e. two groups\\clusters have as large distance among\n",
    "  them as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Coefficient\n",
    "\n",
    "Silhouette coefficient is a metric that tries to combine the two requirements of a good clustering model. The\n",
    "silhouette coefficient is defined for each sample and is a combination of its similarity to the data points in its\n",
    "own cluster and its dissimilarity to the data points not in its cluster.\n",
    "\n",
    "The silhouette coefficient is usually bounded between -1 (incorrect clustering) and +1 (excellent quality\n",
    "dense clusters). A higher value of silhouette coefficient generally means that the clustering model is leading\n",
    "to clusters that are dense and well separated and distinguishable from each other. Lower scores indicate\n",
    "overlapping clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the silhouette score for k-means clustering with 2 clusters using the 'euclidean' distance metric.\n",
    "km2_silc = metrics.silhouette_score(X, km2_labels, metric=\"euclidean\")\n",
    "# Calculates the silhouette score for k-means clustering with 5 clusters using the 'euclidean' distance metric.\n",
    "km5_silc = metrics.silhouette_score(X, km5_labels, metric=\"euclidean\")\n",
    "\n",
    "# Prints the silhouette coefficient calculated for the k-means model with 2 clusters.\n",
    "print(\"Silhouette Coefficient for num clusters=2: \", km2_silc)\n",
    "# Prints the silhouette coefficient calculated for the k-means model with 5 clusters.\n",
    "print(\"Silhouette Coefficient for num clusters=5: \", km5_silc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that from the metric results it seems like we have better\n",
    "cluster quality with two clusters as compared to five clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calinski-Harabaz Index\n",
    "\n",
    "The Calinski-Harabaz index is another metric that we can use to evaluate clustering models when the\n",
    "ground truth is not known. The Calinski-Harabaz score is given as the ratio of the between-clusters\n",
    "dispersion and the within-cluster dispersion.\n",
    "\n",
    "A higher score normally indicates that the clusters are dense and well separated, which\n",
    "relates to the general principles of clustering models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Calinski-Harabasz Index for k-means clustering with 2 clusters.\n",
    "km2_chi = metrics.calinski_harabasz_score(X, km2_labels)\n",
    "# Calculate the Calinski-Harabasz Index for k-means clustering with 5 clusters.\n",
    "km5_chi = metrics.calinski_harabasz_score(X, km5_labels)\n",
    "\n",
    "# Print the Calinski-Harabasz Index for the clustering with 2 clusters.\n",
    "print(\"Calinski-Harabaz Index for num clusters=2: \", km2_chi)\n",
    "# Print the Calinski-Harabasz Index for the clustering with 5 clusters.\n",
    "print(\"Calinski-Harabaz Index for num clusters=5: \", km5_chi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both the scores are pretty high with the results for five clusters being even higher. This\n",
    "goes to show that just relying on metric number alone is not sufficient and you must try multiple evaluation\n",
    "methods coupled with feedback from data scientists as well as domain experts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this tutorial, we've learned essential skills in model building and evaluation, including:\n",
    "\n",
    "- How to properly evaluate classification models using multiple metrics\n",
    "- Understanding the differences between internal and external clustering validation\n",
    "- Practical application of evaluation metrics on real biomedical data\n",
    "- Interpreting model performance through various visualization techniques\n",
    "\n",
    "## Clean up\n",
    "\n",
    "Remember to shut down your Jupyter Notebook environment and delete any unnecessary files or resources once you've completed the tutorial.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
