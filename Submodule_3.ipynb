{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submodule 3 - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "This submodule will cover practical data cleaning techniques, Dive into feature engineering, scaling, and selection techniques. Explore practical examples of handling\n",
    "categorical data, transforming numerical distributions, and choosing scaling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "At the end of this module, you should be able to:\n",
    "\n",
    "+ Learn practical techniques for handling common data cleaning tasks like removing duplicates,\n",
    "formatting inconsistencies, and correcting errors, identifying and handling missing data points in a responsible way.\n",
    "+ Master fundamental concepts of feature engineering, feature scaling, and feature\n",
    "selection for building robust AI/ML models and understand their impact on model performance and\n",
    "interpretability.\n",
    "+ Apply various techniques for encoding categorical data into numerical representations,\n",
    "effectively transform numerical data distributions to improve model performance, deriving new informative features from existing data.\n",
    "+ Implement appropriate scaling techniques for numerical features based on data\n",
    "characteristics and handle outliers and their impact on data scaling.\n",
    "+ Apply different methods for feature\n",
    "selection to improve model accuracy and reduce dimensionality in specific scenarios.\n",
    "+ Gain practical\n",
    "experience applying feature engineering, scaling, and selection techniques on real-world biomedical datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* An AWS account with access to Amazon SageMaker\n",
    "* Basic understanding of Python programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "- Watch the Lecture Videos.\n",
    "- Complete the Quizzes to solidify your understanding.\n",
    "- Enhance your programming skills with Tutorials.\n",
    "- Challenge yourself with the Exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection and Preparation\n",
    "\n",
    "This lecture delves deeply into the **Data Collection and Preparation** phases within the machine learning (ML) process. These stages are essential for any predictive modeling project, as they lay the groundwork for accurate and effective model development. The initial phase, defining the problem, establishes a clear understanding of the project's goals and helps guide the selection of appropriate models for prediction tasks. At this stage, it is crucial to gain insight into the domain and identify the objectives that the model needs to achieve.\n",
    "\n",
    "**Data collection** often begins concurrently with problem definition and may continue as the project evolves. Data can come from various sources and exist in multiple forms, including legacy systems, web sources, databases, flat files, sensors, and mobile devices. We highlight some of the most common data formats:\n",
    "+ CSV (Comma Separated Values) files, which are widely used for their simplicity and compatibility across different systems. \n",
    "+ XML (eXtensible Markup Language) offers a more structured format, typically used for data sharing over the Internet, and is well-suited for human readability and platform independence. \n",
    "+ JSON (JavaScript Object Notation), another popular format, is favored for data interchange due to its compatibility with various programming languages and its straightforward key-value structure. \n",
    "+ HTML (Hypertext Markup Language) is used primarily to render content for web applications but also serves as a valuable data source. \n",
    "+ SQL (Structured Query Language) databases store data in tables that can be accessed through SQL queries or Object Relational Mapping (ORM) methods, making them central to many data science projects. \n",
    "\n",
    "Each data format and source provides a unique set of challenges and advantages, affecting how data is gathered and processed.\n",
    "\n",
    "Once collected, data often needs extensive preparation to be useful for ML algorithms, as raw data may contain noise, inconsistencies, or other issues that can undermine model performance. \n",
    "\n",
    "**Data preparation**, an often time-intensive process, ensures that the raw data is transformed into a form ready for model training and evaluation. This stage includes several critical sub-processes: \n",
    "+ Data Cleaning is the first, addressing specific errors and inconsistencies within the dataset. Cleaning might involve fixing missing values, correcting outliers, or identifying any system-based biases. Effective data cleaning requires a deep understanding of the domain, as it is essential to know which data points are valid and which are errors. \n",
    "+ Next, Data Transformation adjusts the scale or distribution of input variables. This process is crucial because ML algorithms generally perform better with normalized or standardized data, helping to reduce the impact of data range differences across features.\n",
    "\n",
    "**Feature Selection** is the process of identifying the most relevant features for the task at hand. This step is necessary to eliminate redundant or irrelevant data that could confuse or mislead the learning algorithms, potentially leading to reduced predictive performance. The goal is to simplify the model by reducing complexity while maintaining high accuracy. \n",
    "\n",
    "**Feature Engineering**, closely related to feature selection, involves creating new features from existing data to better represent patterns and improve the model's predictive power. Techniques here may include creating Boolean flags, generating summary statistics, or decomposing compound variables into individual components. Thoughtful feature engineering can significantly enhance model performance by providing the algorithm with a more structured and informative dataset.\n",
    "\n",
    "In some cases, **Dimensionality Reduction** becomes necessary to address the \"curse of dimensionality,\" a problem that arises as the number of features in a dataset increases. With more dimensions, data points tend to become sparse, making it difficult for models to generalize effectively. Dimensionality reduction techniques, such as principal component analysis (PCA), help reduce the number of features while preserving the critical structure of the data, improving computational efficiency and enhancing model robustness.\n",
    "\n",
    "These data preparation steps collectively provide ML models with the highest-quality inputs, improving the likelihood of developing effective and reliable predictive models. The slides emphasize that ML success is largely dependent on rigorous data preparation rather than just algorithm selection, as clean, well-organized data allows models to learn meaningful patterns rather than being confounded by noise or irrelevant information. Thus, data preparation is seen as a fundamental and iterative process in the ML lifecycle, requiring continuous refinement and adjustment to fit the project's unique requirements and data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "# Youtube\n",
    "YouTubeVideo(id='data_collection_and_preparation', height=200, width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture Slides\n",
    "\n",
    "Download the lecture slides [Data Collection and Preparation](Submodule_3/Lectures/Submodule_3_Lecture_1_Dat_Collection_and_Preparation.pptx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jupyterquiz\n",
    "from jupyterquiz import display_quiz\n",
    "display_quiz(\"Submodule_3/Quizzes/Submodule_3_Quiz_1_Data_Collection_and_Preparation.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering, Feature Scaling and Feature Selection\n",
    "This lecture covers **Feature Engineering**, **Feature Scaling**, and **Feature Selection** in the machine learning (ML) pipeline, breaking down essential techniques that transform raw data into refined inputs for ML models.\n",
    "\n",
    "**Feature Engineering** involves transforming raw data into meaningful features that capture essential patterns, improving model accuracy on unseen data. This process includes crafting features from numeric, categorical, text, temporal, and image data, with various methods tailored to each type. For numeric data, feature engineering might involve statistical summaries, binning (grouping values into ranges), or mathematical transformations like log and Box-Cox transformations to normalize data distributions. Categorical data, both nominal (without order) and ordinal (with order), often require encoding methods, such as one-hot encoding or label encoding, to represent categories numerically. Text data is processed with techniques like tokenization, removal of stop words, and vectorization (e.g., TF-IDF) to make it digestible for algorithms. Temporal and image data have specialized feature extraction methods, including date-based feature extraction for time series and automated feature extraction using deep learning for image data.\n",
    "\n",
    "**Feature Scaling** is the next critical step, ensuring that features contribute equally to the model by adjusting values into a common range or scale. Standardization, also called Z-score scaling, centers data by making the mean zero and scaling the variance to one. Min-max scaling rescales data between 0 and 1 (or other specified ranges), while robust scaling reduces the impact of outliers by scaling based on the interquartile range (IQR).\n",
    "\n",
    "**Feature Selection** aims to optimize model performance by reducing feature complexity and mitigating the curse of dimensionality. Techniques include:\n",
    "\n",
    "+ **Filter Methods**: Independently assess each feature's relevance based on metrics like correlation or mutual information.\n",
    "+ **Wrapper Methods**: Iteratively test different feature subsets using models to find the best-performing combinations (e.g., forward selection, backward elimination).\n",
    "+ **Embedded Methods**: Use model-driven techniques, such as decision trees or random forests, to rank and retain the most important features directly within the model training process.\n",
    "\n",
    "We also introduce considerations like class imbalance in classification problems, where techniques like resampling, specialized metrics, and ensemble approaches can improve predictive power on rare classes. Additionally, managing uncertainty in ML, stemming from noisy data or incomplete information, is highlighted as a crucial skill for refining model predictions.\n",
    "\n",
    "These steps—feature engineering, scaling, and selection—are pivotal in building effective and generalizable ML models, enabling data scientists to craft inputs that allow ML algorithms to learn complex patterns reliably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "# Youtube\n",
    "YouTubeVideo(id='feature_engineering_scaling_selection', height=200, width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture Slides\n",
    "\n",
    "Download the lecture slides [Feature_Engineering_Scaling_Selection](Submodule_3/Lectures/Submodule_3_Lecture_2_Feature_Engineering_Scaling_Selection.pptx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jupyterquiz\n",
    "from jupyterquiz import display_quiz\n",
    "display_quiz(\"Submodule_3/Quizzes/Submodule_3_Quiz_2_Feature_Engineering_Scaling_Selection.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tutorials\n",
    "+ [Basic Data Cleaning](Submodule_3/Tutorials/Submodule_3_Tutorial_1_Basic_Data_Cleaning.ipynb)\n",
    "+ [Mark and Remove Missing Data](Submodule_3/Tutorials/Submodule_3_Tutorial_2_Mark_and_Remove_Missing_Data.ipynb)\n",
    "+ [Outline Identification and Removal](Submodule_3/Tutorials/Submodule_3_Tutorial_3_Outlier_Identification_and_Removal.ipynb)\n",
    "+ [Missing Data Imputation](Submodule_3/Tutorials/Submodule_3_Tutorial_4_Missing_Data_Imputation.ipynb)\n",
    "+ [Feature Engineering - Encode Categorical Data](Submodule_3/Tutorials/Submodule_3_Tutorial_5_Feature_Engineering_Encode_Categorical_Data.ipynb)\n",
    "+ [Feature Engineering - Change Numerical Data Distribution](Submodule_3/Tutorials/Submodule_3_Tutorial_6_Feature_Engineering_Change_Numerical_Data_Distributions.ipynb)\n",
    "+ [Feature Engineering - Derive New Imput Variables](Submodule_3/Tutorials/Submodule_3_Tutorial_7_Feature_Engineering_Derive_New_Input_Variables.ipynb)\n",
    "+ [Feature Scaling - Numerical Data](Submodule_3/Tutorials/Submodule_3_Tutorial_8_Feature_Scaling_Numerical_Data.ipynb)\n",
    "+ [Feature Scaling - Data with Outliers](Submodule_3/Tutorials/Submodule_3_Tutorial_9_Feature_Scaling_Data_with_Outliers.ipynb)\n",
    "+ [Feature Selection - Categorical Input Features](Submodule_3/Tutorials/Submodule_3_Tutorial_10_Feature_Selection_Categorical_Input_Features.ipynb)\n",
    "+ [Feature Selection - Numerical Input Features](Submodule_3/Tutorials/Submodule_3_Tutorial_11_Feature_Selection_Numerical_Input_Features.ipynb)\n",
    "+ [Feature Selection - Recursive Feature Elimination](Submodule_3/Tutorials/Submodule_3_Tutorial_12_Feature_Selection_Recursive_Feature_Elimination.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exercises\n",
    "+ [Data Wrangling Exercise](Submodule_3/Exercises/Submodule_3_Exercise_1_Data_Wrangling.ipynb) ([Solution](Submodule_3/Exercises/Submodule_3_Exercise_1_Data_Wrangling_Solution.ipynb))\n",
    "+ [Feature Enigneering Exercise](Submodule_3/Exercises/Submodule_3_Exercise_2_Feature_Engineering.ipynb) ([Solution](Submodule_3/Exercises/Submodule_3_Exercise_2_Feature_Engineering_Solution.ipynb))\n",
    "+ [Feature Scaling Exercise](Submodule_3/Exercises/Submodule_3_Exercise_3_Feature_Scaling.ipynb) ([Solution](Submodule_3/Exercises/Submodule_3_Exercise_3_Feature_Scaling_Solution.ipynb))\n",
    "+ [Feature Selection Exercise](Submodule_3/Exercises/Submodule_3_Exericse_4_Feature_Selection.ipynb) ([Solution](Submodule_3/Exercises/Submodule_3_Exericse_4_Feature_Selection_Solution.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "This submodule covers essential techniques for transforming raw data into a suitable format for machine learning models. Key topics include data cleaning, feature engineering, scaling, and selection. By mastering these techniques, learners can effectively handle missing values, outliers, and inconsistencies, create informative features, scale data appropriately, and select the most relevant features for optimal model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "A reminder to shutdown VM and delete any relevant resources. <br><br>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
